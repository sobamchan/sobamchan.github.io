---
title: LLMs and summarization
updated: 2023-05-31
---

## Background

I don't know how much are other summarization people are confused but I am almost panicing by seeing how well these LLMs can generate summaries without being exposed to summarization training objectives, and also the growing speed of developments within the research community, and also in a larger tech community.

To determine whether I should just stop working on summarization and find something else or if there is something that I can find interesting and meaningful, I created this page which I will keep updating until I go crazy.

This page contains the following things.

- A list of papers about LLMs + summarization
- My thoughts on the topic/direction at the time of updating

Both will change over time, and [this link](https://github.com/sobamchan/sobamchan.github.io/commits/master/_posts/2023-05-31-llms-summarization.md) to the git history page will show you how it has develop so far.


## Paper list

Most of them will be arxiv papers not published work so use this list at your own risk.
TLDRs are generated by a "traditionaly" fine-tuned BART-large model, [model](https://github.com/sobamchan/schnitsum/).
My comments are not extensive, they just focus on my question mentioned above in the background.
I give a number to each paper but the order has no meaning, it's just for referencing.

- 1. [News Summarization and Evaluation in the Era of GPT-3](http://arxiv.org/abs/2209.12356)
  - TLDR: We show that humans overwhelmingly prefer GPT-3 summaries, but these also do not suffer from common dataset-specific issues such as poor factuality.

- 2. [Benchmarking Large Language Models for News Summarization](http://arxiv.org/abs/2301.13848)
  - TLDR: We show that instruction tuning, not model size, is the key to the LLM's zero-shot summarization capability.
  - Comments
    - The number of evaluated dataset may be too small to draw their conclusion given these two datasets are known to have some critical problems.
    - But I agree to that at least on this relatively simple setup, i.e., single document news summarization, large models can generate human-level summaries.

- 3. [On Learning to Summarize with Large Language Models as References](http://arxiv.org/abs/2305.14239)
  - TLDR: We propose a novel reward-based learning paradigm for text summarization models that considers the LLMs as the reference or the gold-standard oracle on commonly used summarization datasets such as the CNN/DailyMail dataset.
  - Comments
    - I have no idea if it makes sense to train and evaluate models using the one big model.



## Thoughts

### Reference summary quality issues

As repeatedly mentioned in the paper 2, reference summaries in the existing datasets have quality issues and they show that zero-shot LLMs can easily generate better summaries. It makes sense because these summaries (especially ones in XSUM dataset) are not really summaries but "the first sentence in a news article". Therefore, it is natural that they are sometimes inconsistent to the rest of the article or having some information which do not appear in the rest of the article.

This "problem" results in two super critical issues which can mislead research outcomes,

- fine-tuned models generate "the first sentence in the article" not summaries,
- automatic metrics (e.g., ROUGE-x) doesn't correlate well with human evaluations.

It's a fact that LLMs can generate high-quality summaries but because of the first issue, fine-tuned models are underestimated specially if they are evaluated human annotators because they are asked to evaluate text quality as summaries which are not what models are trained to generate.

There have been much work on "better evaluation metrics" but there is a possibility that what we need is "real" summarization datasets with summaries.

I strongly believe that we need such datasets to re-evaluate the tools we have now.


### Ignoring instructions

Paper 2 mentions that LLMs often ignore instruction. Such as generating some texts which are not summaries and exceeding length limit. Even though smaller fine-tuned models sometimes low-quality texts but they are still "summaries" that the models are trained to generate.
This is unaccepted for many applications, for instance where users can't interact with models to actually make them to generate summaries, e.g., [my paper tldr pages](https://sotaro.io/tldrs).

So I at the moment in my opinion if there are datasets which fit the application needs, it is safer to take fine-tuned models.
