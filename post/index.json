[
    
        
            {
                "ref": "https://sobamchan.github.io/post/goals-2019/",
                "title": "Goals 2019",
            "section": "post",
                "date" : "2019.01.01",
                "body": "目的 自分はいろんなことに対してすぐに脱線しがちなので、年始にまとめておくことで そのリスクをできるだけ軽減できることを期待している。 今年は大学院終了後のキャリアについても、多くの選択をしなければならないので アドホックな作業が多くなり予想通りに行かないと思うが、終了したときの満足感ができるだけ大きくしたい。\nGoals in 2019 キャリア 進路の決定 これが何よりも大事な目標。\n博士課程へ進学したいこと自体は、自分の中では昨年に心ぎめしてぼちぼち研究室探しとかをしてはいた。 今年は、とにかくプロセスを早いペースで進めていきたい。 先生方にアポをもらい話を聞いて、年の前半に何かしら安心できる結果がほしい。\n研究 国際会議に出せる論文を書く これは、結局去年は達成できなかった。\n今年はもっと効率良く手を動かして、なんとしてもこぎつけたい。\nライブラリ開発 昨年末から少しずつ、pytorchベースでseq2seqを統一されたインターフェイスで色々実験できるような ものを実装してきたので、今年はこれをある程度自信を持って人に見せれるくらいのクオリティーにしていきたい。\nその他  本\n先月から開始しているが、月2,3冊ペースで小説以外の本を読んでいきたい。 料理\nいろんなものに挑戦して行きたい。凝ったものまで ビール\nいろんな新しい人と、飲んだことないビールを飲んでいきたい。 競プロ\nというよりも、データ構造とアルゴリズムをちゃんと学習し直したい。（そして競プロを趣味にしたい。） Apple製品 無しの生活にしていきたい。（macbook, iphone高すぎるのでもう買いたくない） "
            }
        
    ,
        
            {
                "ref": "https://sobamchan.github.io/post/sklearn-tfidf-ja/",
                "title": "scikit-learnのTfidfVectorizerを日本語に使うときの注意",
            "section": "post",
                "date" : "2018.01.01",
                "body": "scikit-learnのTfidfVectorizer\nはものすごく便利で、分類とか類似度計算とかを数行で実装することができる。\nインターフェイスもシンプルなので、これまでもいくつかのプロジェクトで使用してきたが、結構大きな罠が日本語に対して適応するときにあるのにこの前気づいた。\n何も考えずに使用するとこんな感じで書ける。\nfrom sklearn.feature_extraction.text import TfidfVectorizer corpus = [\u0026#39;I like a dog .\u0026#39;, \u0026#39;I am a dog .\u0026#39;, \u0026#39;She was a cat before .\u0026#39;] vectorizer = TfidfVectorizer() vectorizer.fit(corpus) ただここで、文章をベクトル化する際に変換の対象になっているvectorizerのattributeであるvocabulary_を確認すると、\n{\u0026#34;am\u0026#34;: 0, \u0026#34;cat\u0026#34;: 1, \u0026#34;dog\u0026#34;: 2, \u0026#34;like\u0026#34;: 3, \u0026#34;she\u0026#34;: 4, \u0026#34;was\u0026#34;: 5} がとなっていて、\u0026lsquo;I'や\u0026rsquo;a'は特徴として使用されないことがわかる。\n最初これに遭遇したときに、TfidfVectorizerの引数であるmax_df等のせいかと思ったがdefaultで1.0なのでそんなこともなくて、 何でだろうかと思っていた。\nするとTfidfVectorizerにはtoken_patternという引数もあり、それにはdefaultで’(?u)\\b\\w\\w+\\b’が渡されている。 Vectorizerがtokenとして認識する文字列の正規表現を指定しているのだが、このままだと文字数が2以上じゃないと認識されない。\nなのでこれを'(?u)\\\\b\\\\w+\\\\b'にしてあげる必要がある。\nつまりこう\nfrom sklearn.feature_extraction.text import TfidfVectorizer corpus = [\u0026#39;I like a dog .\u0026#39;, \u0026#39;I am a dog .\u0026#39;, \u0026#39;She was a cat before .\u0026#39;] vectorizer = TfidfVectorizer(token_pattern=\u0026#39;(?u)\\\\b\\\\w+\\\\b\u0026#39;) vectorizer.fit(corpus) こうするとvocabulary_が\n{\u0026#34;a\u0026#34;: 0, \u0026#34;am\u0026#34;: 1, \u0026#34;cat\u0026#34;: 2, \u0026#34;dog\u0026#34;: 3, \u0026#34;I\u0026#34;: 4, \u0026#34;like\u0026#34;: 5, \u0026#34;she\u0026#34;: 6, \u0026#34;was\u0026#34;: 7} になる。\n英語を扱っている場合は、一文字の単語は意味のあまりないものが多いが、日本語の場合は漢字があるので結構危ない。 (象、 猿とか)\n"
            }
        
    
]