---
title: 2020-10 に読んで面白かった論文 best 3
updated: 2020-11-03
---

今月読んだ論文の中から 3 つ，自分的に面白かったものを簡単に紹介しつつあげていく．
注意なのだが，2020/10 に投稿された論文ではなくて，2020/10 に自分が読んだ論文，であるので投稿自体はそれよりも前であるものもある．

まずタイトルだけあげる．

- [Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision](https://arxiv.org/abs/2010.06775), 20, Oct
- [What Happens To BERT Embeddings During Fine-tuning?](https://arxiv.org/abs/2004.14448), 20, Apr
- [Towards Understanding Sample Variance in Visually Grounded Language Generation: Evaluations and Observations](https://arxiv.org/abs/2010.03644), Oct 20

一つずつ雑に紹介してみる．

---

### [Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision](https://arxiv.org/abs/2010.06775), 20, Oct
visually-grounded language datasets と language only datasets に含まれているデータの分布が違うことを解決しようとした．
テキストのみのデータセットの文中の token に対応する image を見つけてあげる (vokenize) ことで，visual supervision を作成．
それを使って LM を再学習することで glue, squad, swag で精度向上．

### [What Happens To BERT Embeddings During Fine-tuning?](https://arxiv.org/abs/2004.14448), 20, Apr
BERT が finetune を経ることでどんな変化をするのかを調べた研究．
linguistic features は finetuning で忘れられず，最終推論時に利用されるかは不明だが representation に保持はされている．
finetuning は保守的にパラメータを変化させているとのこと．

### [Towards Understanding Sample Variance in Visually Grounded Language Generation: Evaluations and Observations](https://arxiv.org/abs/2010.03644), Oct 20
Image Captioning とか Translation とかの reference を複数持つタイプのデータセットは，データインスタンスを増やすのではなく，各サンプルの reference を増やした方が精度の向上により効率的に貢献することがわかった．

---

いくつかの学会の proceedings がそろそろ公開されるであろうのでそれがきたら効率的にたくさん論文が読めるので楽しみ．
