<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Posts on Tofu Blog</title>
		<link>https://sobamchan.github.io/posts/</link>
		<description>Recent content in Posts on Tofu Blog</description>
		<generator>Hugo -- gohugo.io</generator>
		<language>en-us</language>
		<copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright>
		<lastBuildDate>Sat, 21 Mar 2020 08:20:15 +0900</lastBuildDate>
		<atom:link href="https://sobamchan.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
		
		<item>
			<title>文ベクトル作成ライブラリsisterを使った，簡単な日本語テキスト分類</title>
			<link>https://sobamchan.github.io/posts/sister-ja-text-classification/</link>
			<pubDate>Sat, 21 Mar 2020 08:20:15 +0900</pubDate>
			
			<guid>https://sobamchan.github.io/posts/sister-ja-text-classification/</guid>
			<description>tl;dr 文ベクトルを使って簡単に日本語テキスト分類 Web記事のタイトルを2値分類 -&amp;gt; 正答率90% 概要 前回の記事では，文を入力したら，単語ベクトルを利</description>
			<content type="html"><![CDATA[<figure>
    <img src="/imgs/sister-ja-text-classification-header.jpg"
         alt="image"/> 
</figure>

<h2 id="tldr">tl;dr</h2>
<ul>
<li>文ベクトルを使って簡単に日本語テキスト分類</li>
<li>Web記事のタイトルを2値分類 -&gt; 正答率90%</li>
</ul>
<h2 id="概要">概要</h2>
<p><a href="https://sobamchan.github.io/posts/sister-intro/">前回の記事</a>では，文を入力したら，単語ベクトルを利用して文の数値表現を作成してくれるライブラリの紹介をしました．
実質4行くらいのpythonで文ベクトルを得ることができました．</p>
<p>文ベクトルを使ってできることはたくさんあるのですが，本記事ではその中でも特に一般的な，テキスト分類を行ってみようと思います．</p>
<h2 id="データセットの用意">データセットの用意</h2>
<p>今回は，<a href="https://www.rondhuit.com/download.html">livedoor ニュースコーパス</a>を利用させてもらいます．
このデータセットは，複数メディアから収集された記事がたくさん格納されているのですが，今回は，その中でも二つ (「ITライフハック」，「独女通信」) に絞り，
さらに記事全体ではなく，タイトルだけを利用しようと思います．</p>
<p>つまり，今回行いたいのは，あるネット記事のタイトルが入力された時に，それが「ITライフハック」と「独女通信」のどちらに属するか判定するモデルを構築します．</p>
<p><a href="https://gist.github.com/sobamchan/1bb50c23de0e9b72eadfe60a9ae520a8#file-sister-ja-text-classification-build-dataset-py">GitHub Gist</a>にデータセットのダウンロードから，
今回のフォーマットに変換してcsv形式で保存するpython scriptをおいておきましたので，実際に実行したい方は参照してください．
これを実行すると，<code>text</code>と<code>label</code>をカラムとして持つcsvファイルが作成されます．</p>
<pre><code class="language-csv" data-lang="csv">text,label
Ultrabookをパワーアップ！　mSATA対応の小型SSDがマイクロンより登場,0
コレでたばこを辞めました。独女の禁煙ヒストリー,1
</code></pre><p>データセットの一部を見てみると，こんな感じになっています．(label 0 -&gt; ITライフハック，label 1 -&gt; 独女通信)</p>
<h2 id="分類器の作成">分類器の作成</h2>
<p>ここから，実際の分類器の作成に入っていきます．まず全体を下に貼り付けました．改行含んでも30行以下に収まります．</p>
<script type="application/javascript" src="https://gist.github.com/sobamchan/1bb50c23de0e9b72eadfe60a9ae520a8.js?file=sister-ja-text-classification-main.py"></script>

<p>各ブロックごとに解説していきます．</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>

<span class="kn">import</span> <span class="nn">sister</span>
</code></pre></div><p>まず，今回使うライブラリをインポートします．<code>numpy</code>と<code>pandas</code>はデータ読み出し関連で使っています．
モデルには，今回，<a href="https://scikit-learn.org/">scikit-learn</a>に実装されている<a href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC">SVC</a>を利用します．
ここではモデルの詳細は飛ばしますが，分類タスクに強いモデルです．
最後に，文をベクトルに変換してくれる<a href="https://github.com/tofunlp/sister"><code>sister</code></a>を読み込みます．</p>
<p>次に，<code>main</code>関数の中を見ていきます．</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">dataset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&#34;./dataset.csv&#34;</span><span class="p">)</span>
<span class="n">texts</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">dataset</span><span class="p">[[</span><span class="s2">&#34;text&#34;</span><span class="p">,</span> <span class="s2">&#34;label&#34;</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>

<span class="n">train_texts</span><span class="p">,</span> <span class="n">test_texts</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">test_labels</span> <span class="o">=</span>\
    <span class="n">train_test_split</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
</code></pre></div><p>関数内，最初のブロックは，先ほど作成したデータセットを読み込んで， 訓練用と評価用に分割しています．</p>
<p>で，非常に短いですが，本丸です．</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">train_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">sentence_embedding</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">train_texts</span><span class="p">])</span>
<span class="n">test_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">sentence_embedding</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">test_texts</span><span class="p">])</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s2">&#34;linear&#34;</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">)</span>
</code></pre></div><p>最初の二行で，sisterを利用して文を一つずつベクトルに変換しています．
ここの詳しい説明は，<a href="https://sobamchan.github.io/posts/sister-intro/">前回の投稿</a>を参照してください．
そして，訓練用，評価用の文らのベクトルを獲得してからは，scikit-learnのSVCモデルを訓練しています ．</p>
<p>で，最後に評価用データで正答率を求めています，</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_x</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">))</span>
</code></pre></div><p>データの分割の際に乱数の固定をしていないので，結果は実行のたびに変わりますが，90%は超えてくれると思います．</p>
<h3 id="参考文献">参考文献</h3>
<ul>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC">sklearn.svm.SVC</a></li>
</ul>
<h3 id="関連記事">関連記事</h3>
]]></content>
		</item>
		
		<item>
			<title>自然言語処理ライブラリ sister を使った，簡単な文ベクトルの作成方法 </title>
			<link>https://sobamchan.github.io/posts/sister-intro/</link>
			<pubDate>Fri, 20 Mar 2020 09:03:34 +0900</pubDate>
			
			<guid>https://sobamchan.github.io/posts/sister-intro/</guid>
			<description>tl;dr 文埋め込みを簡単に作れるpythonライブラリの紹介 概要 推薦システムをはじめとして，アプリケーションへのデータ利用が盛んになってきた今，自</description>
			<content type="html"><![CDATA[<figure>
    <img src="/imgs/sister-intro-header.jpg"
         alt="image"/> 
</figure>

<h2 id="tldr">tl;dr</h2>
<ul>
<li>文埋め込みを簡単に作れる<a href="https://github.com/tofunlp/sister">pythonライブラリ</a>の紹介</li>
</ul>
<h2 id="概要">概要</h2>
<p>推薦システムをはじめとして，アプリケーションへのデータ利用が盛んになってきた今，自然言語処理技術をアプリケーションに組み込みたいことは多々あります．</p>
<ul>
<li>Webメディアに「この記事に類似した他の記事」推薦機能</li>
<li>基本の文字列検索よりも，少し「賢い」検索機能</li>
<li>ユーザーの投稿を見て，類似しているユーザーの列挙</li>
</ul>
<p>少し考えただけでたくさん出てきます．</p>
<p>一般に，このような仕組みを構築するには，自然言語データを計算機で扱いやすくするために数値表現に変換する必要があります．</p>
<p>この自然言語の数値表現方法は，現在たくさんの方法が提案されています．
その中でもMikolov大先生が2013年に提案したのを皮切りに激しく発展した， <strong>単語ベクトル</strong> (or 単語埋め込み) をベースにした方法は，
多くのアプリケーションで稼働していることだと思います．</p>
<p>僕自身も多くの場面で，単語ベクトルを使って自然言語文の数値表現を獲得したいことが多々あるのですが，
公開されているモデルをダウンロードしてきて，さらにそのモデルをpythonに読み込むライブラリをインストールして，
文中の単語をそれぞれベクトルかして，最後に文の表現を作るためにさらに変換する．<br>
と小さなことですが，多くのステップを文の表現を獲得するまでに踏まなくてはいけません．</p>
<p>これを簡潔にするために，今回<code>pip</code>でインストールできて，数行のpythonコードで上記のステップを行ってくれるライブラリを書きましたので，それの紹介をしようと思います．</p>
<h2 id="sisterhttpsgithubcomtofunlpsister-使い方"><a href="https://github.com/tofunlp/sister">sister</a> 使い方</h2>
<p>まずコードを載せちゃいます．
<code>pip install sister</code>でインストールしてから</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">sister</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">sentence_embedding</span> <span class="o">=</span> <span class="n">sister</span><span class="o">.</span><span class="n">MeanEmbedding</span><span class="p">(</span><span class="n">lang</span><span class="o">=</span><span class="s2">&#34;ja&#34;</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">sentence</span> <span class="o">=</span> <span class="s2">&#34;私はパリコレに出たことはありません．&#34;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">vector</span> <span class="o">=</span> <span class="n">sentence_embedding</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">vector</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="o">...</span> <span class="p">(</span><span class="mi">300</span><span class="p">,)</span>
</code></pre></div><p>実際のコードとしては4行で済んでいますね．</p>
<p>一行ずつ説明していきます．</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="o">&gt;&gt;&gt;</span> <span class="n">senten_embedding</span> <span class="o">=</span> <span class="n">sister</span><span class="o">.</span><span class="n">MeanEmbedding</span><span class="p">(</span><span class="n">lang</span><span class="o">=</span><span class="s2">&#34;ja&#34;</span><span class="p">)</span>
</code></pre></div><p>この行では，</p>
<ol>
<li>初回実行時の場合にはモデルのダウンロード (デフォルトでは<a href="https://github.com/facebookresearch/fastText/">fastText</a>です)</li>
<li>バイナリファイルをロード
をしています．</li>
</ol>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="o">&gt;&gt;&gt;</span> <span class="n">sentence</span> <span class="o">=</span> <span class="s2">&#34;私はパリコレに出たことはありません．&#34;</span>  <span class="c1"># サンプルの文を定義しているだけ</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">vector</span> <span class="o">=</span> <span class="n">sentence_embedding</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
</code></pre></div><p>ここでは，</p>
<ol>
<li>文を形態素解析して，単語ごとに分割 (<a href="https://mocobeta.github.io/janome/">Janome</a>)</li>
<li>各単語の単語ベクトルを取得</li>
<li>単語ベクトルのリストを使って，文のベクトルを作成する．(単語ベクトルの各要素の平均をとる．)</li>
</ol>
<figure class="small">
    <img src="/imgs/sister-intro-swem-mean.png"
         alt="image"/> <figcaption>
            <p>単語ベクトルから文ベクトルへ変換</p>
        </figcaption>
</figure>

<p>3.の単語ベクトルの羅列から文ベクトルを構成する方法は，
<a href="https://arxiv.org/abs/1805.09843">Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms</a>を参考にした非常にシンプルな方法が実装されています．</p>
<p>本ライブラリ, <a href="https://github.com/tofunlp/sister">sister</a>を使えば，単語ベクトルの取得，日本語の場合は分かち書きから，文への変換まで，
数行のコードで書くことができます．
次の記事では，このライブラリを利用して，実際に何かしらの機械学習を行ってみたいとおもっています．(本当にやるかはわからない．)</p>
<hr>
<h3 id="参考文献">参考文献</h3>
<ul>
<li><a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Distributed Representations of Words and Phrases and their Compositionality</a></li>
<li><a href="https://arxiv.org/pdf/1301.3781.pdf">Efficient Estimation of Word Representations in Vector Space</a></li>
</ul>
<hr>
<h3 id="関連記事">関連記事</h3>
]]></content>
		</item>
		
		<item>
			<title>gorm.Modelを自分好みの定義にする．</title>
			<link>https://sobamchan.github.io/posts/gorm-base-model-original/</link>
			<pubDate>Wed, 18 Mar 2020 08:02:05 +0900</pubDate>
			
			<guid>https://sobamchan.github.io/posts/gorm-base-model-original/</guid>
			<description>tl;dr gorm殻提供されている基本モデルを自分の好みに書き直す． 概要 最近，golangを使って簡単なWebアプリを作ろうとしており，横着して実装</description>
			<content type="html"><![CDATA[<figure>
    <img src="/imgs/gorm-base-header.jpg"
         alt="image"/> 
</figure>

<h2 id="tldr">tl;dr</h2>
<ul>
<li>gorm殻提供されている基本モデルを自分の好みに書き直す．</li>
</ul>
<h2 id="概要">概要</h2>
<p>最近，golangを使って簡単なWebアプリを作ろうとしており，横着して実装を進めながら勉強しています．<br>
Webアプリを作ろうとした時，いくつかあるベースとなるコンポーネントのうち特に重要なのがデータベースだと思います．</p>
<p>Webアプリを作る際には，<a href="https://ja.wikipedia.org/wiki/%E3%82%AA%E3%83%96%E3%82%B8%E3%82%A7%E3%82%AF%E3%83%88%E9%96%A2%E4%BF%82%E3%83%9E%E3%83%83%E3%83%94%E3%83%B3%E3%82%B0">ORM</a>
を使ってデータハンドリングをしたいですよね．
そして，golangでORMを検索すると，まず<a href="http://gorm.io/">gorm</a>に行き着くと思います． 今回自分もこれを採用しました．</p>
<p><a href="http://gorm.io/docs/#Quick-Start">Quick Start</a>を見てみると，以下のようにモデルの定義をしています．</p>
<div class="highlight"><pre class="chroma"><code class="language-go" data-lang="go"><span class="kn">package</span> <span class="nx">main</span>

<span class="kn">import</span> <span class="p">(</span>
  <span class="s">&#34;github.com/jinzhu/gorm&#34;</span>
  <span class="nx">_</span> <span class="s">&#34;github.com/jinzhu/gorm/dialects/sqlite&#34;</span>
<span class="p">)</span>

<span class="kd">type</span> <span class="nx">Product</span> <span class="kd">struct</span> <span class="p">{</span>
  <span class="nx">gorm</span><span class="p">.</span><span class="nx">Model</span>
  <span class="nx">Code</span> <span class="kt">string</span>
  <span class="nx">Price</span> <span class="kt">uint</span>
<span class="p">}</span>
</code></pre></div><p><code>Product</code>というモデルを定義する際に<code>gorm.Model</code>という構造体を埋め込んでいます，このAPIを<a href="https://pkg.go.dev/github.com/jinzhu/gorm?tab=doc#Model">ドキュメント</a>で見てみると．</p>
<div class="highlight"><pre class="chroma"><code class="language-go" data-lang="go"><span class="kd">type</span> <span class="nx">Model</span> <span class="kd">struct</span> <span class="p">{</span>
	<span class="nx">ID</span>        <span class="kt">uint</span> <span class="s">`gorm:&#34;primary_key&#34;`</span>
	<span class="nx">CreatedAt</span> <span class="nx">time</span><span class="p">.</span><span class="nx">Time</span>
	<span class="nx">UpdatedAt</span> <span class="nx">time</span><span class="p">.</span><span class="nx">Time</span>
	<span class="nx">DeletedAt</span> <span class="o">*</span><span class="nx">time</span><span class="p">.</span><span class="nx">Time</span> <span class="s">`sql:&#34;index&#34;`</span>
<span class="p">}</span>
</code></pre></div><p>Web開発でまず必要な情報を提供しているみたいです．primary keyとしてidってのはほとんどの場合で必要なので便利ですね．
しかし，ここで小さな問題にぶつかります．</p>
<p>gorm内でこの構造体が定義されているため，例えばDBから検索してきた結果を，JSONに変換した際に
<code>ID</code>フィールドがJSONでも<code>ID</code>という名前になってしまいます．が，一般的に小文字で<code>id</code>としたいことの方が多いです．</p>
<div class="highlight"><pre class="chroma"><code class="language-go" data-lang="go"><span class="nx">json</span><span class="p">.</span><span class="nf">Marshal</span><span class="p">(</span><span class="nx">one_product</span><span class="p">)</span>  <span class="c1">// -&gt; {&#34;ID&#34;: 1, &#34;CreatedAt&#34;: ...}
</span></code></pre></div><p><code>CreatedAt</code>も<code>created_at</code>になって欲しいですよね．</p>
<h2 id="解決">解決</h2>
<p>これを解決するために，<code>gorm.Model</code>を上書きする用法を調べていたのですが，<a href="https://github.com/jinzhu/gorm/issues/2681#issuecomment-536118464">gorm GitHub Issueのコメント</a>で
「嫌なら使わないで，自分で定義すればいいじゃん」ってのがあってそれで解決しました．</p>
<div class="highlight"><pre class="chroma"><code class="language-go" data-lang="go"><span class="kd">type</span> <span class="nx">MyBaseModel</span> <span class="kd">struct</span> <span class="p">{</span>
  <span class="nx">ID</span>        <span class="kt">uint</span>       <span class="s">`gorm:&#34;primary_key&#34; json:&#34;id&#34;`</span>
  <span class="nx">CreatedAt</span> <span class="nx">time</span><span class="p">.</span><span class="nx">Time</span>  <span class="s">`json:&#34;created_at&#34;`</span>
  <span class="nx">UpdatedAt</span> <span class="nx">time</span><span class="p">.</span><span class="nx">Time</span>  <span class="s">`json:&#34;updated_at&#34;`</span>
  <span class="nx">DeletedAt</span> <span class="o">*</span><span class="nx">time</span><span class="p">.</span><span class="nx">Time</span> <span class="s">`json:&#34;deleted_at&#34;`</span>
<span class="p">}</span>

<span class="kd">type</span> <span class="nx">Product</span> <span class="kd">struct</span> <span class="p">{</span>
  <span class="nx">MyBaseModel</span>
  <span class="nx">Code</span> <span class="kt">string</span>
  <span class="nx">Price</span> <span class="kt">uint</span>
<span class="p">}</span>
</code></pre></div><p>こうすることで，<code>json.Marshal(one_product)</code>したときに，JSON内の<code>ID</code>だったところが<code>id</code>になってくれます．
<code>CreatedAt</code>，<code>UpdatedAt</code>，<code>DeletedAt</code>も<code>gorm.Model</code>を使用していた時と同様に，自動でgormが更新してくれます．</p>
<hr>
<h3 id="参考">参考</h3>
<ul>
<li><a href="http://gorm.io/">gorm.io</a></li>
<li><a href="https://gobyexample.com/json">Go by Example: JSON</a></li>
<li><a href="https://github.com/jinzhu/gorm/issues/2681#issuecomment-536118464">can you change the gorm.Model definition？</a></li>
</ul>
]]></content>
		</item>
		
		<item>
			<title>kewpie: KEyWord PIckEr with tf-idf</title>
			<link>https://sobamchan.github.io/posts/kewpie-intro/</link>
			<pubDate>Tue, 17 Mar 2020 09:08:29 +0900</pubDate>
			
			<guid>https://sobamchan.github.io/posts/kewpie-intro/</guid>
			<description>&lt;figure&gt;
    &lt;img src=&#34;https://sobamchan.github.io/imgs/kewpie-intro-header.jpg&#34;
         alt=&#34;image&#34;/&gt; 
&lt;/figure&gt;

&lt;h2 id=&#34;tldr&#34;&gt;tl;dr&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;tf-idfをscikit-learnから利用&lt;/li&gt;
&lt;li&gt;それをさらに抽象化しまくった&lt;a href=&#34;https://github.com/tofunlp/kewpie&#34;&gt;kewpie&lt;/a&gt;の紹介&lt;/li&gt;
&lt;/ul&gt;</description>
			<content type="html"><![CDATA[<figure>
    <img src="/imgs/kewpie-intro-header.jpg"
         alt="image"/> 
</figure>

<h2 id="tldr">tl;dr</h2>
<ul>
<li>tf-idfをscikit-learnから利用</li>
<li>それをさらに抽象化しまくった<a href="https://github.com/tofunlp/kewpie">kewpie</a>の紹介</li>
</ul>
<h2 id="概要">概要</h2>
<p>自然言語を含んだデータの分析を行っていると，対象としている文書群，それぞれの特徴的な単語を抽出したいことが多々あります．
ここでの特徴的な単語とは，今見ているある文書をそれ以外の文書らと比較した際に，&ldquo;際立つ&rdquo; 単語のことを今回は指すことにします．</p>
<p>この目的を達成するために，自然言語処理分野では最も基本的なテクニックの一つに，<a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">tf-idf</a>がある．
これは，文書中のそれぞれの単語に対して単語の&quot;特徴的度&quot;を計算してくれます．
本記事は実装に関してのものなので，細かい説明は省きますが，ざっくりというと次の2点を満たす単語には高いスコアが付与されます．</p>
<ol>
<li>対象の文書中にたくさん出現しているか？</li>
<li>対象の文書以外ではあまり出現していないか？</li>
</ol>
<p>要約すると，tf-idfは「対象とする文書で特別多く出現する単語」を発見することのできる手法となっています．</p>
<h2 id="tf-idfの利用">tf-idfの利用</h2>
<p>tf-idf自体は，先に述べた要点からも分かるように，数え上げを基本とするので実装を一から自分でするのも簡単に行うことができます．
が，pythonの機械学習ライブラリのボスである<a href="https://scikit-learn.org/stable/">scikit-learn</a>にすでに<a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn-feature-extraction-text-tfidfvectorizer">実装されたもの</a>がありますので，
それを使うと，何も考えずに文書だけ用意すれば簡単にスコアを計算することができます．</p>
<h3 id="scikit-learnのインストール">scikit-learnのインストール</h3>
<div class="highlight"><pre class="chroma"><code class="language-sh" data-lang="sh">pip install scikit-learn
</code></pre></div><h3 id="tfidfvectorizerの利用">TfidfVectorizerの利用</h3>
<div class="highlight"><pre class="chroma"><code class="language-py" data-lang="py"><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span>
<span class="o">...</span>    <span class="s2">&#34;我々 は 宇宙人 だ ．&#34;</span><span class="p">,</span>
<span class="o">...</span>    <span class="s2">&#34;我々 は 野蛮人 だ ．&#34;</span><span class="p">,</span>
<span class="o">...</span>    <span class="s2">&#34;我々 は 海人 だ ．&#34;</span><span class="p">,</span>
<span class="o">...</span> <span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">vectorizer</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">token_pattern</span><span class="o">=</span><span class="s1">&#39;(?u)</span><span class="se">\\</span><span class="s1">b</span><span class="se">\\</span><span class="s1">w+</span><span class="se">\\</span><span class="s1">b&#39;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">X</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">vectorizer</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span>
<span class="o">...</span> <span class="p">[</span><span class="s1">&#39;だ&#39;</span><span class="p">,</span> <span class="s1">&#39;は&#39;</span><span class="p">,</span> <span class="s1">&#39;宇宙人&#39;</span><span class="p">,</span> <span class="s1">&#39;我々&#39;</span><span class="p">,</span> <span class="s1">&#39;海人&#39;</span><span class="p">,</span> <span class="s1">&#39;野蛮人&#39;</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="o">...</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
</code></pre></div><p>注意) <code>TfidfVectorizer</code>を利用する際に<code>token_pattern</code>を指定していないと，1文字からなるトークン (eg, は, だ) が消えます．<br>
これだけのコードで得られた<code>X</code>のなかに，<code>(corpus n, feature n)</code>分のtf-idfスコアが格納されています．
よって，これを<code>numpy</code>なりで大きい順にソートしてあげると，各corpusにおける特徴的な単語がわかります．</p>
<div class="highlight"><pre class="chroma"><code class="language-py" data-lang="py"><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">X</span><span class="o">.</span><span class="n">todense</span><span class="p">(),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="o">...</span> <span class="n">matrix</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>  <span class="c1"># -&gt; (&#39;宇宙人&#39;, &#39;だ&#39;, &#39;は&#39;, &#39;我々&#39;, &#39;海人&#39;, &#39;野蛮人&#39;)</span>
<span class="o">...</span>         <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>  <span class="c1"># -&gt; ( &#39;野蛮人&#39;, &#39;だ&#39;, &#39;は&#39;, &#39;我々&#39;, &#39;宇宙人&#39;, &#39;海人&#39;)</span>
<span class="o">...</span>         <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">]])</span>  <span class="c1"># -&gt; (&#39;海人&#39;, &#39;だ&#39;, &#39;は&#39;, &#39;我々&#39;, &#39;宇宙人&#39;, &#39;野蛮人&#39;)</span>
</code></pre></div><p>この<code>X.todense()</code>は<code>TfidfVectorizer</code>の出力はそのままではnumpyでソートできないので，できる形に変換していて，
最初に<code>-1</code>をかけているのは，そのままだと昇順なので，降順にしています．</p>
<p>この例では<code>corpus</code>が小さく単語の種類も限られていますが，ちゃんと各文書のみで出現する単語が，リストの最初に来ていることがわかります．</p>
<h2 id="kewpie-keyword-picker-with-tf-idf">kewpie: KEyWord PIckEr with tf-idf</h2>
<p>このように<code>TfidfVectorizer</code>を使えば，簡単に文書の特徴的な単語を発見することができるのですが，
僕自身，個人プロジェクト，研究やアルバイトであまりに多用するため，少し前から<a href="https://github.com/tofunlp/kewpie">pip installできる形でライブラリ化</a>して使っています．<br>
ここでは，その使い方を少しだけ紹介します．</p>
<h3 id="インストール">インストール</h3>
<div class="highlight"><pre class="chroma"><code class="language-sh" data-lang="sh">&gt;&gt;&gt; pip install kewpie
</code></pre></div><h3 id="kewpieの利用">kewpieの利用</h3>
<div class="highlight"><pre class="chroma"><code class="language-py" data-lang="py"><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">kewpie</span> <span class="kn">import</span> <span class="n">KwPicker</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span>
<span class="o">...</span>    <span class="s2">&#34;我々 は 宇宙人 だ ．&#34;</span><span class="p">,</span>
<span class="o">...</span>    <span class="s2">&#34;我々 は 野蛮人 だ ．&#34;</span><span class="p">,</span>
<span class="o">...</span>    <span class="s2">&#34;我々 は 海人 だ ．&#34;</span><span class="p">,</span>
<span class="o">...</span> <span class="p">]</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">picker</span> <span class="o">=</span> <span class="n">KwPicker</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">savedir</span><span class="o">=</span><span class="s1">&#39;/dir/to/save/model/&#39;</span><span class="p">)</span>  <span class="c1"># corpusを渡してBuild</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">sentence</span> <span class="o">=</span> <span class="s2">&#34;我々 は 宇宙人 だ ．&#34;</span>  <span class="c1"># 今回特徴単語を発見したい文</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">span</span><span class="p">,</span> <span class="n">keyword</span> <span class="o">=</span> <span class="n">picker</span><span class="o">.</span><span class="n">get_keyword</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>  <span class="c1"># を渡すと，，，</span>

<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">keyword</span><span class="p">)</span>  <span class="c1"># -&gt; &#39;宇宙人&#39;  # 抽出される</span>
</code></pre></div><p>コード量はそんなに変わりませんが，実装を間違えそうなソートの処理が隠蔽されて，文をそのまま渡せるAPIを生やしています．
また，<code>KwPicker.bulid</code>の際に，<code>savedir</code>を渡すと，tf-idf行列の計算をファイルに書き出して保存しておいてくれます．
小さな文書群の時は良いですが，大きくなると計算時間が少しかかるので，少し時間の節約になります．</p>
<p>以下のコードでロードしてまた使えます．</p>
<div class="highlight"><pre class="chroma"><code class="language-py" data-lang="py"><span class="o">&gt;&gt;&gt;</span> <span class="n">loaded_picker</span> <span class="o">=</span> <span class="n">KwPicker</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">savedir</span><span class="o">=</span><span class="s1">&#39;/dir/you/saved/model&#39;</span><span class="p">)</span>
</code></pre></div>]]></content>
		</item>
		
		<item>
			<title>Naacl 2018</title>
			<link>https://sobamchan.github.io/posts/naacl-2018/</link>
			<pubDate>Mon, 16 Mar 2020 23:31:58 +0900</pubDate>
			
			<guid>https://sobamchan.github.io/posts/naacl-2018/</guid>
			<description>&lt;h1 id=&#34;目的&#34;&gt;目的&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://aclanthology.coli.uni-saarland.de/events/naacl-2018&#34;&gt;NAACL 2018&lt;/a&gt;の論文で自分の興味沿っているやつリストを消化したので、 面白かったものを10本選択して、ここにまとめる。&lt;br&gt;
基本的には&lt;a href=&#34;https://sobamchan.github.io/summaries/&#34;&gt;ここ&lt;/a&gt;に投稿しているものを選択して、 内容をコピーしただけ。&lt;/p&gt;</description>
			<content type="html"><![CDATA[<h1 id="目的">目的</h1>
<p><a href="https://aclanthology.coli.uni-saarland.de/events/naacl-2018">NAACL 2018</a>の論文で自分の興味沿っているやつリストを消化したので、 面白かったものを10本選択して、ここにまとめる。<br>
基本的には<a href="https://sobamchan.github.io/summaries/">ここ</a>に投稿しているものを選択して、 内容をコピーしただけ。</p>
<h1 id="背景">背景</h1>
<ul>
<li>Totalで40本くらい読んだ。</li>
<li>対話、QA、翻訳、生成全般に興味があるのでそれに偏っている。</li>
<li>急いで読んだものも多いので、理解に間違いがある可能性あり。</li>
</ul>
<h1 id="中身">中身</h1>
<h2 id="multi-reward-reinforced-summarization-with-saliency-and-entailmenthttpssobamchangithubiopaper_summariesmulti-reward20reinforced20summarization20with20saliency20and20entailment"><a href="https://sobamchan.github.io/paper_summaries/Multi-Reward%20Reinforced%20Summarization%20with%20Saliency%20and%20Entailment/">Multi-Reward Reinforced Summarization with Saliency and Entailment</a></h2>
<p>Abstractive summarizationにおいてpolicy gradientを使うことが増えてきた。<br>
この論文では、報酬関数にはROUGEが使用されることがほとんどだが、 本研究では新しい二つの報酬関数を作成提案した。 1. ROUGESal<br>
Keyword detecteを使用して、salientな情報に対して重みをつけたROUGE 2. Entail<br>
Entailment classifierを使用して、包含しているものに対して重みづけする</p>
<h2 id="attentive-interaction-model-modeling-changes-in-view-in-argumentationhttpssobamchangithubiopaper_summariesattentive20interaction20model20modeling20changes20in20view20in20argumentation"><a href="https://sobamchan.github.io/paper_summaries/Attentive%20Interaction%20Model:%20Modeling%20Changes%20in%20View%20in%20Argumentation/">Attentive Interaction Model: Modeling Changes in View in Argumentation</a></h2>
<p>redditの”change my view” forumっていう、スレッドの開始者が最初に述べた 意見に対して後続の参加者たちが、その意見を変えるためにいろんな意見を投稿するforumがある。<br>
この研究では、後続の意見がviewを変化させる事が出来たかを推定するモデルを、 最初の意見に対してのattentionや最初の意見と、後続の意見のinteractionをmodelingするネットワークで学習している。</p>
<h2 id="improving-lexical-choice-in-neural-machine-translationhttpssobamchangithubiopaper_summariesimproving20lexical20choice20in20neural20machine20translation"><a href="https://sobamchan.github.io/paper_summaries/Improving%20Lexical%20Choice%20in%20Neural%20Machine%20Translation/">Improving Lexical Choice in Neural Machine Translation</a></h2>
<p>従来のenc-decでのNMTは出現頻度の少ないrare wordsの翻訳をうまくできていない。<br>
本研究では、従来の翻訳フレームワークの最後のoperationである、 hidden stateから単語idに対してmappingする式でのベクトル達のnormをconstant valueに固定してあげることで、row-resourceな言語間での翻訳も精度を保つことに成功した。</p>
<h2 id="deep-generative-model-for-joint-alignment-and-word-representationhttpssobamchangithubiopaper_summariesdeep20generative20model20for20joint20alignment20and20word20representation"><a href="https://sobamchan.github.io/paper_summaries/Deep%20Generative%20Model%20for%20Joint%20Alignment%20and%20Word%20Representation/">Deep Generative Model for Joint Alignment and Word Representation</a></h2>
<p>word embeddingをtranslation datasetを使用して学習した。 その際に、</p>
<p>を仮定して、language 1のword embeddingをlanguage 2を使用して学習した</p>
<h2 id="self-training-for-jointly-learning-to-ask-and-answer-questionshttpssobamchangithubiopaper_summariesself-training20for20jointly20learning20to20ask20and20answer20questions"><a href="https://sobamchan.github.io/paper_summaries/Self-Training%20for%20Jointly%20Learning%20to%20Ask%20and%20Answer%20Questions/">Self-Training for Jointly Learning to Ask and Answer Questions</a></h2>
<p>QAタスクでの課題としてデータセットの作成がexpensiveであるというものがあり、これをunlabeled corpusを使用して改善する手法を提案している。<br>
labeled datasetでQA (Question answer) model, QG (Question generation) modelをまず学習する。 そのmodelsを使用して、unlabeled corpusからさらに、questionとanswerを生成し学習に役立てている。</p>
<h2 id="combining-character-and-word-information-in-neural-machine-translation-using-a-multi-level-attentionehttpssobamchangithubiopaper_summariescombining20character20and20word20information20in20neural20machine20translation20using20a20multi-level20attention"><a href="https://sobamchan.github.io/paper_summaries/Combining%20Character%20and%20Word%20Information%20in%20Neural%20Machine%20Translation%20Using%20a%20Multi-Level%20Attention/">Combining Character and Word Information in Neural Machine Translation Using a Multi-Level Attentione</a></h2>
<p>従来の機械翻訳はword / sub-word / char levelから粒度を選択して使用する必要があった。<br>
本研究ではchar levelの情報をattentionを通して使用して(sub)word levelのencodingとdecodingに役立てるframeworkを提案している。</p>
<h2 id="improving-neural-machine-translation-with-conditional-sequence-generative-adversarial-netshttpssobamchangithubiopaper_summariesimproving20neural20machine20translation20with20conditional20sequence20generative20adversarial20nets"><a href="https://sobamchan.github.io/paper_summaries/Improving%20Neural%20Machine%20Translation%20with%20Conditional%20Sequence%20Generative%20Adversarial%20Nets/">Improving Neural Machine Translation with Conditional Sequence Generative Adversarial Nets</a></h2>
<p>二つの手法の合わせ技でNMTの精度向上を狙っている。<br>
GAN frameworkによる、教師データと生成されたデータをdiscriminatorに 判別されないように学習。 RLを使用して評価指標であるBLEUを直接最大化するように学習する。 最近はやりの両方のダブルパンチ。</p>
<h2 id="dialog-generation-using-multi-turn-reasoning-neural-networkshttpssobamchangithubiopaper_summariesdialog20generation20using20multi-turn20reasoning20neural20networks"><a href="https://sobamchan.github.io/paper_summaries/Dialog%20Generation%20Using%20Multi-turn%20Reasoning%20Neural%20Networks/">Dialog Generation Using Multi-turn Reasoning Neural Networks</a></h2>
<p>QA taskで最近よく使用されるような、memoryが拡張されているmodelを使用してqaでのdocumentを対話のcontextをして、<br>
queryをuser utteranceとして、memoryを参照して発話を生成することで、よりそれっぽくて、多様性のある発話を作ることができた。</p>
<h2 id="knowledge-enriched-two-layered-attention-network-for-sentiment-analysishttpssobamchangithubiopaper_summariesknowledge-enriched20two-layered20attention20network20for20sentiment20analysis"><a href="https://sobamchan.github.io/paper_summaries/Knowledge-enriched%20Two-layered%20Attention%20Network%20for%20Sentiment%20Analysis/">Knowledge-enriched Two-layered Attention Network for Sentiment Analysis</a></h2>
<p>sentiment analysis taskにおいて二つのモデルのensembleで精度向上を狙った。 二つのモデルは、文章を入力として受け取るRNNと特徴量を入力として受け取るSVRを使用している。<br>
単語に対する情報をrichにするために、word netから作成したKnowledge Graph Embeddingを 特徴量としてSVRへの入力の一つとして使用している。</p>
<h1 id="感想">感想</h1>
<p>QAはまあまあ追ってきたけど、なんかあんまり面白くないところに着地しつつあるかなって感じ。 QAやりたいって最初の方思ってたけど、なんか微妙。<br>
要約はここにのせたのも面白いし、乗せてなくても面白いのあった。</p>]]></content>
		</item>
		
		<item>
			<title>Pytorch S2s Projects</title>
			<link>https://sobamchan.github.io/posts/pytorch-s2s-projects/</link>
			<pubDate>Tue, 22 Jan 2019 23:39:57 +0900</pubDate>
			
			<guid>https://sobamchan.github.io/posts/pytorch-s2s-projects/</guid>
			<description>&lt;h1 id=&#34;目的&#34;&gt;目的&lt;/h1&gt;
&lt;p&gt;少し前からPytorchをベースとしたSeq2Seqのライブラリ？(or boilerplate的なもの)を作っているんだけど、 それの全体的な構成の参考のために類似プロジェクトを探しているのでリストとしてここにまとめておく。&lt;br&gt;
OpenNMTとか、allennlpは完成度は非常に高くていいのだが、ここではもう少し小さくて簡単に全体の見通しがたてれるものに絞る。 構造の参考にしたいだけなので、動作確認まではしていない。&lt;br&gt;
(随時更新)&lt;/p&gt;</description>
			<content type="html"><![CDATA[<h1 id="目的">目的</h1>
<p>少し前からPytorchをベースとしたSeq2Seqのライブラリ？(or boilerplate的なもの)を作っているんだけど、 それの全体的な構成の参考のために類似プロジェクトを探しているのでリストとしてここにまとめておく。<br>
OpenNMTとか、allennlpは完成度は非常に高くていいのだが、ここではもう少し小さくて簡単に全体の見通しがたてれるものに絞る。 構造の参考にしたいだけなので、動作確認まではしていない。<br>
(随時更新)</p>
<h1 id="探し方">探し方</h1>
<p>githubで&quot;pytorch&rdquo;, &ldquo;seq2seq&quot;で検索して、スター件数上位である程度コードが構造化されているものたち。</p>
<h1 id="上位からいくつか">上位からいくつか</h1>
<ul>
<li><a href="https://github.com/IBM/pytorch-seq2seq">IBM/pytorch-seq2seq</a><br>
半年前くらいから開発が止まってしまっているっぽいが、シンプルな実装で全体の見通しは良い。</li>
<li><a href="https://github.com/MaximumEntropy/Seq2Seq-PyTorch">MaximumEntropy/Seq2Seq-PyTorch</a><br>
これも二年前から開発が止まっているぽい。要約とかも実装してあるが、training loopとかがベタッとしている。</li>
<li><a href="https://github.com/eladhoffer/seq2seq.pytorch">eladhoffer/seq2seq.pytorch</a><br>
かなりいい感じ。同作者の他のutil repoを使用していたり、広大になっているが構造も実装もすごく参考になる。Transformerも実装されている。</li>
<li><a href="https://github.com/lium-lst/nmtpytorch">lium-lst/nmtpytorch</a><br>
pipでインストールしてコマンドラインから使用できる。</li>
<li><a href="https://github.com/keon/seq2seq">keon/seq2se</a><br>
train, model, utilsからなるかなりシンプルな実装</li>
<li><a href="https://github.com/outcastofmusic/quick-nlp">outcastofmusic/quick-nlp</a><br>
fast.ai inspiredな実装になっているらしい。Transformerもある。Dockerfileも置いてあっていきなり使えるようになっている。</li>
</ul>]]></content>
		</item>
		
		<item>
			<title>Goals 2019</title>
			<link>https://sobamchan.github.io/posts/goals-2019/</link>
			<pubDate>Tue, 01 Jan 2019 23:37:29 +0900</pubDate>
			
			<guid>https://sobamchan.github.io/posts/goals-2019/</guid>
			<description>&lt;h1 id=&#34;目的&#34;&gt;目的&lt;/h1&gt;
&lt;p&gt;自分はいろんなことに対してすぐに脱線しがちなので、年始にまとめておくことで そのリスクをできるだけ軽減できることを期待している。 今年は大学院終了後のキャリアについても、多くの選択をしなければならないので アドホックな作業が多くなり予想通りに行かないと思うが、終了したときの満足感ができるだけ大きくしたい。&lt;/p&gt;</description>
			<content type="html"><![CDATA[<h1 id="目的">目的</h1>
<p>自分はいろんなことに対してすぐに脱線しがちなので、年始にまとめておくことで そのリスクをできるだけ軽減できることを期待している。 今年は大学院終了後のキャリアについても、多くの選択をしなければならないので アドホックな作業が多くなり予想通りに行かないと思うが、終了したときの満足感ができるだけ大きくしたい。</p>
<h1 id="goals-in-2019">Goals in 2019</h1>
<h2 id="キャリア">キャリア</h2>
<h3 id="進路の決定">進路の決定</h3>
<p>これが何よりも大事な目標。<br>
博士課程へ進学したいこと自体は、自分の中では昨年に心ぎめしてぼちぼち研究室探しとかをしてはいた。 今年は、とにかくプロセスを早いペースで進めていきたい。 先生方にアポをもらい話を聞いて、年の前半に何かしら安心できる結果がほしい。</p>
<h2 id="研究">研究</h2>
<h3 id="国際会議に出せる論文を書く">国際会議に出せる論文を書く</h3>
<p>これは、結局去年は達成できなかった。<br>
今年はもっと効率良く手を動かして、なんとしてもこぎつけたい。</p>
<h3 id="ライブラリ開発">ライブラリ開発</h3>
<p>昨年末から少しずつ、pytorchベースでseq2seqを統一されたインターフェイスで色々実験できるような ものを実装してきたので、今年はこれをある程度自信を持って人に見せれるくらいのクオリティーにしていきたい。</p>
<h3 id="その他">その他</h3>
<ul>
<li>本<br>
先月から開始しているが、月2,3冊ペースで小説以外の本を読んでいきたい。</li>
<li>料理<br>
いろんなものに挑戦して行きたい。凝ったものまで</li>
<li>ビール<br>
いろんな新しい人と、飲んだことないビールを飲んでいきたい。</li>
<li>競プロ<br>
というよりも、データ構造とアルゴリズムをちゃんと学習し直したい。（そして競プロを趣味にしたい。）</li>
<li>Apple製品 無しの生活にしていきたい。（macbook, iphone高すぎるのでもう買いたくない）</li>
</ul>]]></content>
		</item>
		
		<item>
			<title>Review 2018</title>
			<link>https://sobamchan.github.io/posts/review-2018/</link>
			<pubDate>Mon, 31 Dec 2018 23:36:13 +0900</pubDate>
			
			<guid>https://sobamchan.github.io/posts/review-2018/</guid>
			<description>&lt;h1 id=&#34;目的&#34;&gt;目的&lt;/h1&gt;
&lt;p&gt;本当は月単位とかで自分をレビューして、それを年単位でまとめ振り返るってのが 本当の粒度としては正しい気がするんだけど、なかなかできなかった。 ここでは、数十年後自分で見て楽しむために一年間なにをしてきたか、を書いておこうと思う。&lt;/p&gt;</description>
			<content type="html"><![CDATA[<h1 id="目的">目的</h1>
<p>本当は月単位とかで自分をレビューして、それを年単位でまとめ振り返るってのが 本当の粒度としては正しい気がするんだけど、なかなかできなかった。 ここでは、数十年後自分で見て楽しむために一年間なにをしてきたか、を書いておこうと思う。</p>
<h1 id="review-of-2018">Review of 2018</h1>
<h2 id="さきに全体感">さきに全体感</h2>
<p>この年は、一ヶ月単位で実行される大きめのイベントが2つあり（NTT研究所インターンとブラジル研究留学）、 年の最初に考えていたほど、自分の研究や学習をすすめることができなかったのが反省としてある。 ただ、どちらのイベントからも大きく自分の考え方を変える経験を得ることができたので公開は全くしていない。</p>
<h2 id="ntt研究所インターンシップ">NTT研究所インターンシップ</h2>
<h3 id="なぜいったか">なぜいったか</h3>
<p>これのことを知った直後は、自分の研究の時間が減ることを恐れて興味はあまりなかったが、 少ししてから研究テーマの選定で行き詰まり、参加することで新しい視点が得られることを期待したのが 一番大きい理由。 それに加えて、参加した際にメンターとしてついてくれる人が分野で知られている人だったのでそれも大きかった。</p>
<h3 id="何をしたのか">何をしたのか</h3>
<p>大きな枠組みとしては、大学での研究生活とあまり変わりなく、研究分野での問題があり、関連論文を読んで アプローチを考え実装・評価をしていた。 具体的なテーマとしては対話システム関連で、結構抽象度が高く問題設定レベルから難しいテーマだった。</p>
<h3 id="何を学んだか">何を学んだか</h3>
<p>インターンとして一ヶ月で問題の理解から成果の発表までする必要があったので、 スケジュールをある程度組んで、テンポ良く進めていく必要があったというのが一番普段の研究生活との違いだった。<br>
最初は自然言語処理という分野での知識はあったが、今回の対話ってことに関しては 知識があまりなかったので無理かと思ったが、やってみると 各フェーズでの目的をちゃんと設定してから行動してみると、文献の収集から実装の優先順位付けから、なんだかんだで やりきることができたので体験として今後の参考になった。</p>
<h3 id="経験後の自分の変化">経験後の自分の変化</h3>
<p>研究においての自分が現在何をしているのかを明確に理解しながら作業することを以前より強く意識するようになった。 これはまだまだ上手く路線に乗っていないので継続して意識していこうと思う。</p>
<h2 id="ブラジル留学">ブラジル留学</h2>
<h3 id="なぜいったか-1">なぜいったか</h3>
<p>大学に以前ブラジルから留学生が来ていて、彼らと仲良くしていたので もともとブラジルに機会があれば迷わず行こうと思っていたのが一番の理由だった。 サブの理由としては、長いスパンで抱えている自分の課題として、自分の住むところをガンガン探索する ってのがあるので時間が許す限り海外へはどんどん行く必要があったから。</p>
<h3 id="何をしたのか-1">何をしたのか</h3>
<p>形式としては、研究インターンシップみたいな形で行ったので、平日の日中はロボティクス系の研究室にてコンピュータービジョンのコードを書いてた。 週末とか夜は、20人くらい住んでいるシェアハウスに住んでいたこともあって彼らと遊んでいた。（破壊的に楽しかった。）</p>
<h3 id="何を学んだか-1">何を学んだか</h3>
<p>一番大きい学び（再確認）は、結局どれだけ良い人間関係を人生を通して築けるかってのが、 満足感を高める一番の方法ってことだった。 どれだけ信頼できて、楽しく一緒に時間を過ごせる人を家族を含めて作っていくことの大事を再実感した。 研究とか仕事も自分は今の所楽しみ過ぎで、忘れがちになるが気をつけたい。</p>
<h3 id="経験後の自分の変化-1">経験後の自分の変化</h3>
<p>自分から人を誘って飲みに行ったり、時間を取って料理を自分でしたりするようになった。 あと、新しい人と合う機会を逃さないようにしたいって気持ちが大きくなった。</p>
<h2 id="その他起きたこととかやったこと">その他起きたこととか、やったこと</h2>
<ul>
<li>YANS<br>
結果として微妙な成果を持っていくことになってしまったが、多くの有用な意見とモチベーションをもらえた。</li>
<li>ジム<br>
体力が低下しているのを体感するようになってきてしまったので、週２，３で短時間でもできるだけ行くようになった。</li>
<li>大学院<br>
まあ学部と特に変化なし。</li>
</ul>]]></content>
		</item>
		
		<item>
			<title>Backtranslation Papers</title>
			<link>https://sobamchan.github.io/posts/backtranslation-papers/</link>
			<pubDate>Tue, 02 Jan 2018 23:38:56 +0900</pubDate>
			
			<guid>https://sobamchan.github.io/posts/backtranslation-papers/</guid>
			<description>&lt;h1 id=&#34;目的&#34;&gt;目的&lt;/h1&gt;
&lt;p&gt;最近、教師なし翻訳とかでよく使用されているback-translationだが、 直接翻訳に使用する以外ではどんな感じの研究があるかを知りたかったので、 あまり見つからなかったが論文いくつか読んだのでここにまとめておく。（随時更新）&lt;/p&gt;</description>
			<content type="html"><![CDATA[<h1 id="目的">目的</h1>
<p>最近、教師なし翻訳とかでよく使用されているback-translationだが、 直接翻訳に使用する以外ではどんな感じの研究があるかを知りたかったので、 あまり見つからなかったが論文いくつか読んだのでここにまとめておく。（随時更新）</p>
<h1 id="全体感">全体感</h1>
<p>back-translateした結果をsource sentenceのparaphraseとして、paraphrase detectionとかgeneration に使用されるのが多かった。</p>
<h1 id="論文s">論文s</h1>
<h2 id="iterative-back-translation-for-neural-machine-translationhttpsaclanthologycoliuni-saarlanddepapersw18-2703w18-2703"><a href="https://aclanthology.coli.uni-saarland.de/papers/W18-2703/w18-2703">Iterative Back-Translation for Neural Machine Translation</a></h2>
<p>source to targetの機械翻訳機を学習するのだけど、まず先に手持ちのparallel corpusでtarget to sourceを学習し それを用いて、monolingual corpus (in source language)からparallel corpusを生成してparallel corpusを カサ増ししてデータセットに加えてしまおうというもの。</p>
<h2 id="style-transfer-through-back-translationhttpsaclanthologycoliuni-saarlanddepapersp18-1080p18-1080"><a href="https://aclanthology.coli.uni-saarland.de/papers/P18-1080/p18-1080">Style Transfer Through Back-Translation</a></h2>
<p>Style Transferを行う文生成器を学習するんだけど、生成器 (decoder)への入力をいきなり、文をそのまま入れるのではなくて 別学習のNMT modelを使用してsource -&gt; target -&gt; sourceと一度back-translateし、そのsourceをencodeしたものを 入れることで、意味情報を濃く、style情報が削られた状態にしている。 こうすることでstyleを変換しやすくしているらしい。</p>
<h2 id="learning-paraphrastic-sentence-embeddings-from-back-translated-bitexthttpsaclanthologycoliuni-saarlanddepapersd17-1026d17-1026"><a href="https://aclanthology.coli.uni-saarland.de/papers/D17-1026/d17-1026">Learning Paraphrastic Sentence Embeddings from Back-Translated Bitext</a></h2>
<p>paraphrase sentenceの生成をback-translationすることで生成する。</p>
<h2 id="paraphrasing-revisited-with-neural-machine-translationhttpaclweborganthologye17-1083"><a href="http://aclweb.org/anthology/E17-1083">Paraphrasing Revisited with Neural Machine Translation</a></h2>
<p>paraphraseを生成するモデルをNMTのframeworkでやっているのだが、 back-translationで生成すると翻訳例がone-to-oneでmappingすることはできない理由で、必ずしも意味を完全に捉えたparaphraseを生成することはできない。 なので、target sentenceを生成する際に一つではなく複数生成したり、target languageを複数用意することで、source sentenceのaspectをできるだけ捉えてから生成するようにしている。</p>]]></content>
		</item>
		
		<item>
			<title>How I Read Papers 2018</title>
			<link>https://sobamchan.github.io/posts/how-i-read-papers-2018/</link>
			<pubDate>Mon, 01 Jan 2018 23:25:54 +0900</pubDate>
			
			<guid>https://sobamchan.github.io/posts/how-i-read-papers-2018/</guid>
			<description>&lt;p&gt;研究室に所属してから一年経って、自分の論文の「速読」スタイルが少し固まったので記録しておく。（多くの人が似たことをしていると思うが）
随時、読み方に変更が出たら更新していく。&lt;/p&gt;</description>
			<content type="html"><![CDATA[<p>研究室に所属してから一年経って、自分の論文の「速読」スタイルが少し固まったので記録しておく。（多くの人が似たことをしていると思うが）
随時、読み方に変更が出たら更新していく。</p>
<h1 id="背景">背景</h1>
<ul>
<li>自然言語処理系の研究をしているM1で、深層強化学習をSeq2Seqに応用する系の研究をしたいと思っている。</li>
<li>分野に関する知見は少なく手探り感が強いため、とにかくたくさん論文を読む必要がある。</li>
</ul>
<h1 id="要件">要件</h1>
<ul>
<li>具体的な研究テーマも決まっていないので、手広く見てテーマ決めに対する知識が欲しい。</li>
</ul>
<h1 id="読み方">読み方</h1>
<h2 id="論文収集リソース">論文収集リソース</h2>
<ul>
<li><a href="https://qiita.com/mhangyo/items/4eb5add038d6d4e76c79">この辺の会議のaccepted paper</a></li>
<li><a href="https://aclanthology.coli.uni-saarland.de/">自分の興味のある単語をacl anthologyで検索して引っかかったものたち</a></li>
</ul>
<h2 id="論文選択">論文選択</h2>
<p>上のソースで得られた論文のリストを以下の順番でフィルタリングしていていく。 1. titleだけ見て少しでも面白そうと思うもの 2. abstractだけ読んでまあ面白そうと思うもの</p>
<h2 id="論文管理">論文管理</h2>
<p>上のフィルタリングで残ったのみをabstractのメモとともにTrelloにソースごとにListにしていく。
結果として会議とか検索ワードごとに積読される。</p>
<h2 id="読み">読み</h2>
<p>読む論文をTrelloから選択したら、論文をMendeleyにインポートしてからIpad miniで読む。
速読する際には基本的に論文中の以下の点のみ読む。</p>
<ul>
<li>Abstract</li>
<li>Introductionの最後</li>
<li>Related worksの最後</li>
<li>手法の概要（methodみたいな章の最初に書いてあることが多い？）</li>
<li>Conclusion</li>
</ul>
<p>自分が知っていることが多い分野の論文だと、これだけで概要はつかめる。 けど、知らない手法とかをベースに成り立っている論文とかだと、これだけじゃわからないことがあるので、 method等の章をもう少しちゃんと読む必要が出てくる。</p>
<h2 id="読んだ後">読んだ後</h2>
<p>このテンプレートに従って簡単にまとめを書き、Trelloのカードをdid readに移動し、 まとめサイトを更新し、終える。
（このテンプレートは、なんか変える必要がある気がしている）</p>]]></content>
		</item>
		
		<item>
			<title>scikit-learnのTfidfVectorizerを日本語に使うときの注意</title>
			<link>https://sobamchan.github.io/posts/sklearn-tfidf-ja/</link>
			<pubDate>Mon, 01 Jan 2018 21:51:46 +0900</pubDate>
			
			<guid>https://sobamchan.github.io/posts/sklearn-tfidf-ja/</guid>
			<description>&lt;p&gt;&lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html&#34;&gt;scikit-learnのTfidfVectorizer&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;はものすごく便利で、分類とか類似度計算とかを数行で実装することができる。&lt;br&gt;
インターフェイスもシンプルなので、これまでもいくつかのプロジェクトで使用してきたが、結構大きな罠が日本語に対して適応するときにあるのにこの前気づいた。&lt;br&gt;
何も考えずに使用するとこんな感じで書ける。&lt;/p&gt;</description>
			<content type="html"><![CDATA[<p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html">scikit-learnのTfidfVectorizer</a></p>
<p>はものすごく便利で、分類とか類似度計算とかを数行で実装することができる。<br>
インターフェイスもシンプルなので、これまでもいくつかのプロジェクトで使用してきたが、結構大きな罠が日本語に対して適応するときにあるのにこの前気づいた。<br>
何も考えずに使用するとこんな感じで書ける。</p>
<div class="highlight"><pre class="chroma"><code class="language-py" data-lang="py"><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>

<span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;I like a dog .&#39;</span><span class="p">,</span> <span class="s1">&#39;I am a dog .&#39;</span><span class="p">,</span> <span class="s1">&#39;She was a cat before .&#39;</span><span class="p">]</span>

<span class="n">vectorizer</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">()</span>
<span class="n">vectorizer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
</code></pre></div><p>ただここで、文章をベクトル化する際に変換の対象になっているvectorizerのattributeであるvocabulary_を確認すると、</p>
<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span><span class="nt">&#34;am&#34;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="nt">&#34;cat&#34;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="nt">&#34;dog&#34;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="nt">&#34;like&#34;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="nt">&#34;she&#34;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="nt">&#34;was&#34;</span><span class="p">:</span> <span class="mi">5</span><span class="p">}</span>
</code></pre></div><p>がとなっていて、&lsquo;I'や&rsquo;a'は特徴として使用されないことがわかる。</p>
<p>最初これに遭遇したときに、TfidfVectorizerの引数であるmax_df等のせいかと思ったがdefaultで1.0なのでそんなこともなくて、 何でだろうかと思っていた。</p>
<p>するとTfidfVectorizerにはtoken_patternという引数もあり、それにはdefaultで<code>’(?u)\b\w\w+\b’</code>が渡されている。 Vectorizerがtokenとして認識する文字列の正規表現を指定しているのだが、このままだと文字数が2以上じゃないと認識されない。</p>
<p>なのでこれを<code>'(?u)\\b\\w+\\b'</code>にしてあげる必要がある。</p>
<p>つまりこう</p>
<div class="highlight"><pre class="chroma"><code class="language-py" data-lang="py"><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>

<span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;I like a dog .&#39;</span><span class="p">,</span> <span class="s1">&#39;I am a dog .&#39;</span><span class="p">,</span> <span class="s1">&#39;She was a cat before .&#39;</span><span class="p">]</span>

<span class="n">vectorizer</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">token_pattern</span><span class="o">=</span><span class="s1">&#39;(?u)</span><span class="se">\\</span><span class="s1">b</span><span class="se">\\</span><span class="s1">w+</span><span class="se">\\</span><span class="s1">b&#39;</span><span class="p">)</span>
<span class="n">vectorizer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
</code></pre></div><p>こうするとvocabulary_が</p>
<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span><span class="nt">&#34;a&#34;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="nt">&#34;am&#34;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="nt">&#34;cat&#34;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="nt">&#34;dog&#34;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="nt">&#34;I&#34;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="nt">&#34;like&#34;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span> <span class="nt">&#34;she&#34;</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span> <span class="nt">&#34;was&#34;</span><span class="p">:</span> <span class="mi">7</span><span class="p">}</span>
</code></pre></div><p>になる。</p>
<p>英語を扱っている場合は、一文字の単語は意味のあまりないものが多いが、日本語の場合は漢字があるので結構危ない。 (象、 猿とか)</p>]]></content>
		</item>
		
		<item>
			<title>洋楽ラッパーの歌詞の分類問題をchainer使ってやってみる</title>
			<link>https://sobamchan.github.io/posts/eminem-snoop-chainer/</link>
			<pubDate>Sat, 04 Feb 2017 09:27:28 +0900</pubDate>
			
			<guid>https://sobamchan.github.io/posts/eminem-snoop-chainer/</guid>
			<description>[Qiitaのアカウント削除のために，移動してきました．] ソースコードはここにあります。 以前同様の分類をscikit-learnを使用してや</description>
			<content type="html"><![CDATA[<p>[Qiitaのアカウント削除のために，移動してきました．]</p>
<p>ソースコードは<a href="https://github.com/sobamchan/eminem_snoop_chainer">ここ</a>にあります。
<a href="http://qiita.com/sobamchan/items/24c3616aac81aa86f74d">以前</a>同様の分類をscikit-learnを使用してやってみたことがあったので、今回はディープラーニングのフレームワークであるchainerを使用して、簡単なプログラムを書いて分類してみました。</p>
<p>(*) 今回はプログラムの正確さにあまり自信がないため、もし間違い等がありましたら、是非コメント等で教えていただけたらと思います。</p>
<p>使用するデータは前回と同様、<a href="https://ja.wikipedia.org/wiki/%E3%82%A8%E3%83%9F%E3%83%8D%E3%83%A0">eminem</a>さんと<a href="https://ja.wikipedia.org/wiki/%E3%82%B9%E3%83%8C%E3%83%BC%E3%83%97%E3%83%BB%E3%83%89%E3%83%83%E3%82%B0">snoop dogg</a>さんの歌詞を使用させてもらいました。</p>
<h2 id="手順">手順</h2>
<ol>
<li>歌詞を数え上げにより作成された、出現回数のベクトルに変換する</li>
<li>データセットを訓練用と、テスト用に分割する</li>
<li>chainerで必要なものを初期化し</li>
<li>学習サイクルを回して</li>
</ol>
<h2 id="1-歌詞を数え上げにより作成された出現回数のベクトルに変換する">1. 歌詞を数え上げにより作成された、出現回数のベクトルに変換する</h2>
<p>この作業は前回にTfidfのベクトルをscikit-learnを使用して作成したのと同様に、scikit-learnに頼ってベクトル化します。</p>
<div class="highlight"><pre class="chroma"><code class="language-py3" data-lang="py3"><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="k">import</span> <span class="n">CountVectorizer</span>
<span class="n">vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">lyrics</span><span class="p">)</span>
</code></pre></div><p>これだけです！
lyricsには一つの歌詞を一つの要素としている配列が代入されています。
このvectorizerは後で使用します。</p>
<h2 id="2-データセットを訓練用とテスト用に分割する">2. データセットを訓練用と、テスト用に分割する</h2>
<p>ここでは以下の四つのデータを用意します</p>
<ul>
<li>x_train (学習に使用するデータ, 歌詞)</li>
<li>t_train (学習データに対応する正解ラベル, eminem(0) か snoop(1)か)</li>
<li>y_test (テスト時に使用するデータ, 歌詞)</li>
<li>t_test (テスト時に使用するデータに対応する正解ラベル)</li>
</ul>
<p>実際には以下のようにしています（効率が悪そうなやり方ですが、、、）</p>
<div class="highlight"><pre class="chroma"><code class="language-py3" data-lang="py3"><span class="n">x_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">eminem_lyrics</span><span class="p">[:</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">eminem_lyrics</span><span class="p">)</span><span class="o">*</span><span class="mf">0.8</span><span class="p">)]</span> <span class="o">+</span> <span class="n">snoop_lyrics</span><span class="p">[:</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">snoop_lyrics</span><span class="p">)</span><span class="o">*</span><span class="mf">0.8</span><span class="p">)])</span>
<span class="n">t_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">eminem_lyrics</span><span class="p">)</span><span class="o">*</span><span class="mf">0.8</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">snoop_lyrics</span><span class="p">)</span><span class="o">*</span><span class="mf">0.8</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">eminem_lyrics</span><span class="p">[</span><span class="o">-</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">eminem_lyrics</span><span class="p">)</span><span class="o">*</span><span class="mf">0.2</span><span class="p">):]</span> <span class="o">+</span> <span class="n">snoop_lyrics</span><span class="p">[</span><span class="o">-</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">snoop_lyrics</span><span class="p">)</span><span class="o">*</span><span class="mf">0.2</span><span class="p">):])</span>
<span class="n">t_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">eminem_lyrics</span><span class="p">)</span><span class="o">*</span><span class="mf">0.2</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">snoop_lyrics</span><span class="p">)</span><span class="o">*</span><span class="mf">0.2</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</code></pre></div><h2 id="3-chainerで必要なものを初期化">3. chainerで必要なものを初期化</h2>
<p>chainerで定義したモデルと、chainer提供の最適化関数を初期化します。</p>
<div class="highlight"><pre class="chroma"><code class="language-py3" data-lang="py3"><span class="n">model</span> <span class="o">=</span> <span class="n">MyChain</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">()</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</code></pre></div><h2 id="4-学習サイクルを回してみる">4. 学習サイクルを回してみる</h2>
<div class="highlight"><pre class="chroma"><code class="language-py3" data-lang="py3"><span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">epoch</span><span class="p">)):</span>
    <span class="n">sfindx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">train_N</span><span class="p">)</span>
    <span class="c1"># train</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">train_N</span><span class="p">,</span> <span class="n">bs</span><span class="p">):</span>
        <span class="n">x_text</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">sfindx</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">bs</span><span class="p">]]</span>
        <span class="n">x_vecs</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">x_text</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
        <span class="n">x_vecs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x_vecs</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">x_vecs</span><span class="p">)</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">t_train</span><span class="p">[</span><span class="n">sfindx</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">bs</span><span class="p">]])</span>
        <span class="n">model</span><span class="o">.</span><span class="n">cleargrads</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
</code></pre></div><p>やっていることはnumpyのrandom.permutation()で配列からランダムに要素を取り出すためのいんでっくすを作成し、
先ほど定義しておいたvectorizerで歌詞をベクトルに変換した上で、
chainer.Variable()でchainerが受け付けてくれる形に変換し、
modelに渡してあげているだけです。</p>
<p>そのモデルは以下のように定義されています。</p>
<div class="highlight"><pre class="chroma"><code class="language-py3" data-lang="py3"><span class="k">class</span> <span class="nc">MyChain</span><span class="p">(</span><span class="n">Chain</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MyChain</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">l1</span><span class="o">=</span><span class="n">L</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
        <span class="p">)</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax_cross_entropy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fwd</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="n">train</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">fwd</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div><p>学習サイクルに関しては以上になります。詳しくはリポジトリの方を意味ていただきたいです。</p>
<h2 id="結果">結果</h2>
<pre><code>epoch = 15
batch size = 10
</code></pre><p>という、何も考えずに何となく設定して学習を回してみてテストデータで正答率を出してみたところ91.6%ほどになりました！</p>
<h2 id="感想">感想</h2>
<p>chainerが本当に扱いやすく、深層学習ちょびっとだけしか勉強したことがない僕でも、一応何かしらの学習は行うことができる感じでした。
今回はとりあえずchainerで何かしらやってみたいってことで、何も考えずにとりあえずモデル等を定義しましたが、次回からはちゃんと目的を持ち、大きめのデータセットで学習をしてみたいなと思いました。</p>
]]></content>
		</item>
		
		<item>
			<title>julia nlp入門 [1]</title>
			<link>https://sobamchan.github.io/posts/julia-nlp-intro-1/</link>
			<pubDate>Sun, 29 Jan 2017 09:26:13 +0900</pubDate>
			
			<guid>https://sobamchan.github.io/posts/julia-nlp-intro-1/</guid>
			<description>[Qiitaのアカウント削除のために，移動してきました．] 経緯 数値計算や機械学習に強い言語として最近julia langがよく出てくるので入門</description>
			<content type="html"><![CDATA[<p>[Qiitaのアカウント削除のために，移動してきました．]</p>
<h2 id="経緯">経緯</h2>
<p>数値計算や機械学習に強い言語として最近julia langがよく出てくるので入門してみたい</p>
<h2 id="やること">やること</h2>
<p>テキストファイルを読み込んでテキストごとの単語出現回数をカウントする。</p>
<ul>
<li>テキストファイルの読み込み</li>
<li>辞書型を使ってみる</li>
<li>配列型を使ってみる</li>
</ul>
<h2 id="やらないこと">やらないこと</h2>
<ul>
<li>typeを使った型の定義</li>
</ul>
<h2 id="使うパッケージ">使うパッケージ</h2>
<p>今回は何も使いません</p>
<h2 id="ソースコード">ソースコード</h2>
<p><a href="https://github.com/oh-sore-takesou/julia_word_count">ここにあります</a></p>
<h2 id="実践">実践</h2>
<h3 id="ファイルの読み込み">ファイルの読み込み</h3>
<div class="highlight"><pre class="chroma"><code class="language-jl" data-lang="jl"><span class="k">function</span> <span class="n">readfile</span><span class="p">(</span><span class="n">filename</span><span class="o">::</span><span class="n">String</span><span class="p">)</span>
    <span class="n">docs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">open</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span> <span class="k">do</span> <span class="n">f</span>
        <span class="k">for</span> <span class="n">doc</span> <span class="kp">in</span> <span class="n">eachline</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
            <span class="n">push!</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">split</span><span class="p">(</span><span class="n">doc</span><span class="p">))</span>
        <span class="k">end</span>
    <span class="k">end</span>

    <span class="k">return</span> <span class="n">docs</span>
<span class="k">end</span>
</code></pre></div><p>ほとんどpythonと同じように読み出すことができました。<br>
関数定義内の引数の型指定はなくても大丈夫でした。(<code>filename::String</code>の所)</p>
<ul>
<li><code>split()</code>は文字列を引数にとって、スペースで区切って配列にしてくれます。</li>
<li><code>push!(array, hoge)</code>はarrayにhogeをpushします。juliaでは慣習的に破壊的メソッドには<code>!</code>を記述するようです</li>
</ul>
<h3 id="文章中の単語にidを振りカウントする">文章中の単語にidを振り、カウントする</h3>
<div class="highlight"><pre class="chroma"><code class="language-jl" data-lang="jl"><span class="k">function</span> <span class="n">get_corpus</span><span class="p">(</span><span class="n">docs</span><span class="o">::</span><span class="kt">Array</span><span class="p">)</span>
    <span class="n">corpus</span> <span class="o">=</span> <span class="kt">Dict</span><span class="p">()</span> <span class="c"># {id =&gt; word}</span>
    <span class="n">counts</span> <span class="o">=</span> <span class="kt">Dict</span><span class="p">()</span> <span class="c"># {id =&gt; count}</span>
    <span class="n">word2id</span> <span class="o">=</span> <span class="kt">Dict</span><span class="p">()</span> <span class="c"># {word =&gt; id}</span>
    <span class="k">for</span> <span class="n">doc</span> <span class="kp">in</span> <span class="n">docs</span>
        <span class="k">for</span> <span class="n">word</span> <span class="kp">in</span> <span class="n">doc</span>
            <span class="c"># exists</span>
            <span class="k">if</span> <span class="n">get</span><span class="p">(</span><span class="n">word2id</span><span class="p">,</span> <span class="n">word</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span>
                <span class="n">counts</span><span class="p">[</span><span class="n">word2id</span><span class="p">[</span><span class="n">word</span><span class="p">]]</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="c"># new</span>
            <span class="k">else</span>
                <span class="n">corpus</span><span class="p">[</span><span class="n">length</span><span class="p">(</span><span class="n">corpus</span><span class="p">)]</span> <span class="o">=</span> <span class="n">word</span>
                <span class="n">word2id</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">length</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
                <span class="n">counts</span><span class="p">[</span><span class="n">word2id</span><span class="p">[</span><span class="n">word</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="k">end</span>
        <span class="k">end</span>
    <span class="k">end</span>
    
    <span class="k">return</span> <span class="n">corpus</span><span class="p">,</span> <span class="n">word2id</span><span class="p">,</span> <span class="n">counts</span>
<span class="k">end</span>
</code></pre></div><p>のちにidに割り振られた単語が何か調べることができるようにword2idを用意しています。<br>
単語のidに当たるのもはcorpus追加時のcorpus内の単語数にしています(重複していない整数であれば何でも別にいい)</p>
<ul>
<li><code>get(dict, string, 0)</code>はdict内にkeyがstringのものがあるか調べています。第三引数に当たる<code>0</code>は該当がなかった際に返される値です。該当がある場合はvalueを返します。</li>
</ul>
<h3 id="文章をとってid--countを作成する">文章をとって、{id =&gt; count}を作成する</h3>
<div class="highlight"><pre class="chroma"><code class="language-jl" data-lang="jl"><span class="k">function</span> <span class="n">get_vector</span><span class="p">(</span><span class="n">docs</span><span class="o">::</span><span class="kt">Array</span><span class="p">,</span> <span class="n">corpus</span><span class="p">,</span> <span class="n">word2id</span><span class="p">,</span> <span class="n">counts</span><span class="p">)</span>
    <span class="n">bow_corpuses</span> <span class="o">=</span> <span class="p">[]</span> <span class="c"># {id =&gt; count}</span>
    <span class="k">for</span> <span class="n">doc</span> <span class="kp">in</span> <span class="n">docs</span>
        <span class="n">bow_corpus</span> <span class="o">=</span> <span class="kt">Dict</span><span class="p">()</span> <span class="c"># {id =&gt; count}</span>
        <span class="k">for</span> <span class="n">word</span> <span class="kp">in</span> <span class="n">doc</span>
            <span class="c"># exists</span>
            <span class="k">if</span> <span class="n">get</span><span class="p">(</span><span class="n">bow_corpus</span><span class="p">,</span> <span class="n">word2id</span><span class="p">[</span><span class="n">word</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span>
                <span class="n">bow_corpus</span><span class="p">[</span><span class="n">word2id</span><span class="p">[</span><span class="n">word</span><span class="p">]]</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="c"># new</span>
            <span class="k">else</span>
                <span class="n">bow_corpus</span><span class="p">[</span><span class="n">word2id</span><span class="p">[</span><span class="n">word</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="k">end</span>
        <span class="k">end</span>
        <span class="n">push!</span><span class="p">(</span><span class="n">bow_corpuses</span><span class="p">,</span> <span class="n">bow_corpus</span><span class="p">)</span>
    <span class="k">end</span>

    <span class="k">return</span> <span class="n">bow_corpuses</span>
<span class="k">end</span>
</code></pre></div><p>さっきの内容とほとんど同じです。</p>
<h3 id="上記で作成した関数を呼び出す">上記で作成した関数を呼び出す</h3>
<div class="highlight"><pre class="chroma"><code class="language-jl" data-lang="jl"><span class="n">docs</span> <span class="o">=</span> <span class="n">readfile</span><span class="p">(</span><span class="s">&#34;./docs.txt&#34;</span><span class="p">)</span>
<span class="n">corpus</span><span class="p">,</span> <span class="n">word2id</span><span class="p">,</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">get_corpus</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
<span class="n">bow_corpuses</span> <span class="o">=</span> <span class="n">get_vector</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">corpus</span><span class="p">,</span> <span class="n">word2id</span><span class="p">,</span> <span class="n">counts</span><span class="p">)</span>
</code></pre></div><h2 id="感想">感想</h2>
<p>今回はしっかりとコードを書くということはせず何となく書いてみたが、python(ruby)を知っている人なら馴染めるかなという感じだった。<br>
速度面の測定等はしなかったが早いというのは確かみたいなので、この雰囲気でPythonより高速で走らせることができるならばこれからの選択肢には全然入ってきそうな印象。<br>
ただクラスをpythonのように書けず少しトリッキーな表現方法になりそうなので調査が必要。</p>
<h2 id="今後">今後</h2>
<p>juliaの大きな特徴の一つである、行列計算とニューラルネットワーク(フレームワーク選定を含む)の記述をしてみたい。</p>
<p><a href="https://twitter.com/sobamchan">twitter始めました</a></p>
]]></content>
		</item>
		
		<item>
			<title>Djangoで簡単なフォロー機能を実装してみる</title>
			<link>https://sobamchan.github.io/posts/django-simple-following/</link>
			<pubDate>Sun, 29 Jan 2017 09:24:24 +0900</pubDate>
			
			<guid>https://sobamchan.github.io/posts/django-simple-following/</guid>
			<description>[Qiitaのアカウント削除のために，移動してきました．] 経緯 Djangoでユーザー間のフォロー機能を作成しそうなものがあるので、 どのように</description>
			<content type="html"><![CDATA[<p>[Qiitaのアカウント削除のために，移動してきました．]</p>
<h2 id="経緯">経緯</h2>
<p>Djangoでユーザー間のフォロー機能を作成しそうなものがあるので、
どのようにするのがシンプルか考えてみた。</p>
<h2 id="やること">やること</h2>
<ul>
<li>django.contrib.auth.modelsのUserモデルを拡張してインスタンスメソッドを追加する</li>
<li>RelationshipモデルをUserの中間テーブルとして作成し、フォロー管理をする</li>
</ul>
<h2 id="使うパッケージ">使うパッケージ</h2>
<ul>
<li>Django==1.9.7</li>
</ul>
<h2 id="ソースコード">ソースコード</h2>
<p>は<a href="https://github.com/oh-sore-takesou/django-simple-relationship">ここにあります</a></p>
<p>snsプロジェクトとuserアプリを作成しています。<br>
実際に編集しているのは<code>user/models.py</code>のみで、以下のようになっています。</p>
<div class="highlight"><pre class="chroma"><code class="language-py3" data-lang="py3"><span class="kn">from</span> <span class="nn">django.db</span> <span class="k">import</span> <span class="n">models</span>
<span class="kn">from</span> <span class="nn">django.contrib.auth.models</span> <span class="k">import</span> <span class="n">User</span> <span class="k">as</span> <span class="n">DjangoUser</span>

<span class="s1">&#39;&#39;&#39;
</span><span class="s1">DjangoのデフォルトUserモデルを継承して、インスタンスメソッドを追加している
</span><span class="s1">&#39;&#39;&#39;</span>
<span class="k">class</span> <span class="nc">User</span><span class="p">(</span><span class="n">DjangoUser</span><span class="p">):</span>

    <span class="s1">&#39;&#39;&#39;
</span><span class="s1">    新規にtableを作成せずに継承したmodelの拡張のみを行いたいので、
</span><span class="s1">    MetaクラスのproxyをTrueにしている。
</span><span class="s1">    &#39;&#39;&#39;</span>
    <span class="k">class</span> <span class="nc">Meta</span><span class="p">:</span>
        <span class="n">proxy</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="s1">&#39;&#39;&#39;
</span><span class="s1">    Userインスタンスがフォローしているuserを返す関数
</span><span class="s1">    &#39;&#39;&#39;</span>
    <span class="k">def</span> <span class="nf">get_followers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">relations</span> <span class="o">=</span> <span class="n">Relationship</span><span class="o">.</span><span class="n">objects</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">follow</span><span class="o">=</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">relation</span><span class="o">.</span><span class="n">follower</span> <span class="k">for</span> <span class="n">relation</span> <span class="ow">in</span> <span class="n">relations</span><span class="p">]</span>
        


<span class="s1">&#39;&#39;&#39;
</span><span class="s1">フォローしている人と、フォローされている人をつなぐ中間テーブル
</span><span class="s1">&#39;&#39;&#39;</span>
<span class="k">class</span> <span class="nc">Relationship</span><span class="p">(</span><span class="n">models</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="n">follow</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">ForeignKey</span><span class="p">(</span><span class="n">User</span><span class="p">,</span> <span class="n">related_name</span><span class="o">=</span><span class="s1">&#39;follows&#39;</span><span class="p">)</span>
    <span class="n">follower</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">ForeignKey</span><span class="p">(</span><span class="n">User</span><span class="p">,</span> <span class="n">related_name</span><span class="o">=</span><span class="s1">&#39;followers&#39;</span><span class="p">)</span>
</code></pre></div><p>DjangoのUserモデルを拡張したい場合にはMetaクラスをいじる必要があるのがミソな気がします。<a href="https://simpleisbetterthancomplex.com/tutorial/2016/07/22/how-to-extend-django-user-model.html">ここ</a>に詳しく書いてあります。
Relationshipモデル内で<code>related_name</code>を定義していますが現段階では必要ありません。</p>
<h2 id="感想">感想</h2>
<p>フォロー機能を追加するには様々な方法があると思うんですが、今回は自分が取れる一番シンプルそうな、方法を取ってみました。<br>
他にもっといい方法が必ずあると思うんですが、まあdjangoのuserモデルを拡張する方法を調べれたので満足しています。
これよりもいい感じの方法をご存知の方おられましたら是非教えていただきたいです。</p>
<p><a href="https://twitter.com/sobamchan">twitter始めました</a></p>
]]></content>
		</item>
		
		<item>
			<title>洋楽ラッパーの歌詞の分類問題をscrapyとscikit-learnを使ってやってみる</title>
			<link>https://sobamchan.github.io/posts/eminem-snoop-classification/</link>
			<pubDate>Sun, 29 Jan 2017 09:22:28 +0900</pubDate>
			
			<guid>https://sobamchan.github.io/posts/eminem-snoop-classification/</guid>
			<description>[Qiitaのアカウント削除のために，移動してきました．] ソースコードはここにあります。 先日機械学習ハンズオンでscikit-learnを用</description>
			<content type="html"><![CDATA[<p>[Qiitaのアカウント削除のために，移動してきました．]</p>
<p>ソースコードは<a href="https://bitbucket.org/Sotaro/emimem_vs_snoopdogg">ここ</a>にあります。</p>
<p>先日機械学習ハンズオンでscikit-learnを用いた簡単なテキスト分類の手法を勉強したので、自分の好きなものに使ってみました。</p>
<p>やりたいこと</p>
<hr>
<p>アメリカのラッパーで有名な<a href="https://ja.wikipedia.org/wiki/%E3%82%A8%E3%83%9F%E3%83%8D%E3%83%A0">EMINEM</a>と<a href="https://ja.wikipedia.org/wiki/%E3%82%B9%E3%83%8C%E3%83%BC%E3%83%97%E3%83%BB%E3%83%89%E3%83%83%E3%82%B0">SNOOP DOGG</a>の歌詞を判定するモデルをSVMを用いて作成したい。</p>
<p>手順</p>
<hr>
<p>scrapyで歌詞サイトから各アーティストの歌詞データを取得する
scikit-learnのTfidfVectorizerでベクトル化する
scikit-learnのsvmで学習
評価</p>
<hr>
<h3 id="1-scrapyで歌詞サイトから各アーティストの歌詞データを取得する">1) scrapyで歌詞サイトから各アーティストの歌詞データを取得する</h3>
<p>今回は元データを<a href="http://www.metrolyrics.com">MetroLyrics</a>の方から取得しました。</p>
<p>Qiitaの<a href="http://qiita.com/checkpoint/items/038b59b29df8e1e384a2">この投稿</a>を参考に<code>metro_spider.py</code>を作り、<code>scrapy runspider metro_spider.py -o snoop_dogg.json</code>としてSNOOP DOGGの歌詞を全て持ってきました。</p>
<p>spiderクラスの実装でscrapyが便利だなと感じたのが、</p>
<div class="highlight"><pre class="chroma"><code class="language-py" data-lang="py"><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;.songs-table tbody tr td &gt; a::attr(href)&#39;</span><span class="p">)</span>
</code></pre></div><p>とするだけで、html内のタグを指定でき、クロール対象となるリンクを取得できました。</p>
<p>クロール対象内では、</p>
<div class="highlight"><pre class="chroma"><code class="language-py3" data-lang="py3">
<span class="n">lyrics</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">lyric</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//p[@class=&#34;verse&#34;]/text()&#39;</span><span class="p">):</span>

<span class="n">lyrics</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lyric</span><span class="o">.</span><span class="n">extract</span><span class="p">())</span>

</code></pre></div><p>とすることで複数のpタグにまたがっている歌詞データをまとめてから保存することができました。</p>
<h3 id="2-scikit-learnのtfidfvectorizerでベクトル化する">2) scikit-learnのTfidfVectorizerでベクトル化する</h3>
<p>次に先ほど生成されたeminem.jsonとsnoop_dogg.jsonのデータをscikit-learnのTfidfVectorizerでベクトル化していきます。</p>
<div class="highlight"><pre class="chroma"><code class="language-py3" data-lang="py3">
<span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;eminem.json&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">json_data_eminem</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
<span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

</code></pre></div><p>でjsonを読み込んで、eminemの歌詞とsnoop doggの歌詞を一つのリスト(doc_list)に入れます。</p>
<div class="highlight"><pre class="chroma"><code class="language-py3" data-lang="py3">
<span class="n">vectorizer</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">min_df</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_df</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">doc_list</span><span class="p">)</span>

</code></pre></div><p>一行目でTfidVectorizerを用いたベクトル化のオブジェクトを作り、二行目で歌詞のリストをベクトル化してXに代入しています。</p>
<div class="highlight"><pre class="chroma"><code class="language-py3" data-lang="py3">
<span class="n">perm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">train_index</span> <span class="o">=</span> <span class="n">perm</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">80</span><span class="p">]</span>
<span class="n">test_index</span> <span class="o">=</span> <span class="n">perm</span><span class="p">[</span><span class="mi">80</span><span class="p">:]</span>

<span class="n">X_train</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train_index</span><span class="p">,:]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">train_index</span><span class="p">]</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">test_index</span><span class="p">,:]</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>

</code></pre></div><p><code>np.random.permutation(X.shape[0])</code>は先ほど作成したXデータは前半の90データがeminem、後半の90データがsnoop doggとデータの順序が偏っているので、シャッフルしています。
それを8:1の割合で学習データとテストデータに分割しています。</p>
<h3 id="3-scikit-learnのsvmで学習">3) scikit-learnのsvmで学習</h3>
<div class="highlight"><pre class="chroma"><code class="language-py3" data-lang="py3">
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">svm</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">LinearSVC</span><span class="p">()</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

</code></pre></div><p><a href="http://scikit-learn.org/stable/">scikit-learn</a>を使うと学習はこれだけみたいでした。便利。</p>
<h3 id="4-評価">4) 評価</h3>
<div class="highlight"><pre class="chroma"><code class="language-py" data-lang="py">
<span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

</code></pre></div><p>評価<code>score</code>関数を用いることで数値として見ることができました。</p>
<div class="highlight"><pre class="chroma"><code class="language-bash" data-lang="bash">
$ python scikit.py
0.8708

</code></pre></div><p>今回は9割程度の結果になりました。</p>
<p><a href="https://twitter.com/sobamchan">twitter始めました</a></p>
]]></content>
		</item>
		
	</channel>
</rss>
