---
title: 全次元活用されていませんよ
updated: 2025-01-14
---
![](https://i.imgur.com/Ui3iDA1.jpeg)


Downstream task を使わないで text encoder を評価する論文を何本か眺めてみた。

- [When is an Embedding Model More Promising than Another? \| OpenReview](https://openreview.net/forum?id=VqFz7iTGcl)
- [The Role of Embedding Complexity in Domain-invariant Representations](https://arxiv.org/abs/1910.05804)
- [Unsupervised Embedding Quality Evaluation](https://proceedings.mlr.press/v221/tsitsulin23a.html)
- [Understanding Dimensional Collapse in Contrastive Self-supervised Learning](https://arxiv.org/abs/2110.09348)

そして、dimensional collapse なる現象があることを知った。embedding representations で全ての次元が有効に活用されていないことを指す単語みたい。この現象自体は想像はつくし最近のプロジェクトで観測していたことなのだが、ちゃんと名前がついており、既存の研究が存在しているのは良かった。NLP ではなく、画像処理分野での研究が主な気がするが、モデルの構造とその学習方法は基本的には同じなので、テキスト処理分野でもこれを調べている研究があっても良さそうなのでもう少し調べてみる。

昨日催促のメールを送った結果、テレビは 2 - 4 営業日で届くだろう、とのことだが全く信頼ができない。他に信頼できるベンダーがあればそちらに切り替えることも可能なのだが、もはや何処も信用できないので諦めた。今週来なかったらどうしよう。