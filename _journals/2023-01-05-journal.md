---
title: できたかな事前学習自前にて
updated: 2023-01-05
---

そういえば去年も着ていた明け方の強風による音で目が5時ごろにさめた。
そこから寝入ることができなかったので一度起きて実験の経過や他人のブログを少し眺めて風が弱まってから再度ねた。
久しぶりにこの時間帯に起きた気がするな。冬に入ってから起きる時間がだいぶのんびりしていたので。

BART を pre-train するためのコードがひと段落し、一応実行できることは確認したので、README をかいた [link](https://github.com/sobamchan/engawa)。
学習したモデルがある程度ちゃんと動作することを確認したら、cli commands を実装しよう。

動作確認のために回していた小さめの事前学習がある程度進んだので、とりあえずいくつかの文を埋め込んで、それらの距離計算をし、何かしらの情報が学習されているか試してみたらそれっぽくなったのでよしとする。次は要約データに fine-tune して評価してみる。
これやっていて思ったのだが、表現学習にも生成にも使える BART は BERT の完全上位互換なのではないか？ BERT は基本的に生成には向かないので。
BART の方が少し動作が遅いのだろうが、一回の事前学習で獲得されたドメイン情報をより多くのタスクに適応できるのは大きなメリットな気がするのだが。
