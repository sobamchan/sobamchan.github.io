---
title: ACL 2022 - A Summarization of Some Papers.
updated: 2022-07-16
---



- [Generating Scientific Definitions with Controllable Complexity](https://aclanthology.org/2022.acl-long.569)
  - **Problem**: Scientific terms can be hard to understand for readers and definitions in general purpose dictionaries often don't contain them.
  - **Approach**: Collected pairs of a question about a scientifiv term and an answer from various sources and performed benchmark experiments with existing models.

- [When to Use Multi-Task Learning vs Intermediate Fine-Tuning for Pre-Trained Encoder Transfer Learning](https://aclanthology.org/2022.acl-short.30)
  - **Problem**: It's not clear yet which transfer learning strategy to take.
  - **Approach**: Compared multitask learning and intermediate fine-tuning on GLUE benchmark and found that multitask learning is better than intermediate fine-tuning when the target task has fewer samples than the supporting task (vice versa).

- [How does the pre-training objective affect what large language models learn about linguistic properties?](https://aclanthology.org/2022.acl-short.16)
  - **Problem**: There are no study investitages the relationship between pre-training objective and linguistic properties of models.
  - **Approach**: Compared MLM and rather random objective functions and performed linguistic knowledge probing, and show that there are no significant difference.

- [Problems with Cosine as a Measure of Embedding Similarity for High Frequency Words](https://aclanthology.org/2022.acl-short.45)
  - **Problem**: Relationship between word frequency and contextualized embebeddings is understudied.
  - **Approach**: Showed that contextualized embeddings are very sensitive to word frequency, underestimates similarity of frequent words by cosine similarity.

- []()
  - **Problem**: 
  - **Approach**:

- []()
  - **Problem**: 
  - **Approach**:
