---
title:
updated: 2022-05-19
---

- [Zero-Shot Aspect-Based Scientific Document Summarization using Self-Supervised Pre-training](https://aclanthology.org/2022.bionlp-1.5)
  - *Problem*: In existing datasets for research paper summarization, summaries focus on limited variety of aspects, limiting applications.
  - *Approach*: Using texts in each section from papers as signals to perform self-supervised leraning.

- [Attention Temperature Matters in Abstractive Summarization Distillation](https://aclanthology.org/2022.acl-long.11)
  - *Problem*: BART is large. In current distillation, teacher output distribution is "too shart" for students.
  - *Approach*: By relaxing attention temperature, make signal easier for students.

- [Read Top News First: A Document Reordering Approach for Multi-Document News Summarization](https://aclanthology.org/2022.findings-acl.51)
  - *Problem*: Simple documents concatenation ignores document importance. While summarization models tend to pay more attention to the beginneing of the inputs.
  - *Approach*: Before concatenating documents, order them by importance score obtained by supervised/unsupervised models.

- [GenCompareSum: a hybrid unsupervised summarization method using salience](https://aclanthology.org/2022.bionlp-1.22)
  - *Problem*: Transformer-based models have limination on sequence length and require a lot of training data.
  - *Approach*: 
- [HiStruct+: Improving Extractive Text Summarization with Hierarchical Structure Information](https://aclanthology.org/2022.findings-acl.102)
  - *Problem*: Structure of the document is not considered.
  - *Approach*: Propose to use section names and sentence position to explicitly encode document structure.
- [Faithful or Extractive? On Mitigating the Faithfulness-Abstractiveness Trade-off in Abstractive Summarization](https://aclanthology.org/2022.acl-long.100)
  - *Problem*: There is no evaluation how much we give up abstractiveness over faifulness.
  - *Approach*: Propose a way to compute faithfulness abstractiveness trade-off curve by separating training data samples by extractiveness.
- [Length Control in Abstractive Summarization by Pretraining Information Selection]()
  - *Problem*: Most length-controling happens in decoding time only. How to encode is not considered.
  - *Approach*: 
- []()
  - *Problem*: 
  - *Approach*: 
- []()
  - *Problem*: 
  - *Approach*: 
- []()
  - *Problem*: 
  - *Approach*: 
- []()
  - *Problem*: 
  - *Approach*: 
- []()
  - *Problem*: 
  - *Approach*: 

   

