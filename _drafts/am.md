---
title: Adapter Module
updated: 2020-11-13
---

- 背景
- メリット
- 技術詳細
- 応用例

多くの自然言語処理タスクで高性能を出している deep learning モデルだが，その成功には大規模な訓練用データセットが必要になる．
しかし多くのプロジェクトではコストの問題でそのようなデータセットを構築するのは困難である．
そこで近年 BERT をはじめとしたモデルで注目を集めている転移学習 (Transfer Learning) にたどり着く．
転移学習では入手可能な大規模なデータセットでモデルを学習させることで初期化し (pre-training) ，「温まった」状態で本来の目的であるがサイズは小さいデータセットで再学習 (fine-tuning) することで，
データが少量であったとしても高い性能を出すことを目的としたフレームワークである．
例として [BERT](https://arxiv.org/abs/1810.04805) では，[Transformer](https://arxiv.org/abs/1706.03762) をベースとした巨大モデルを，
まず教師データが不要な方法 (Masked Language Modeling & Next Sentence Prediction) で初期化し，その後で各種 NLP タスクに対して再学習している．
このように pre-training を経ることで，大規模な言語リソースを利用して自然言語の統計的情報を獲得し，それを興味のタスクに流用することで高い性能を達成できる．

この転移学習をさらに一歩進める機構を提案した手法を本記事では取り上げて，それを利用した他の研究もいくつか紹介する．
その手法とは，[Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/abs/1902.00751) で BERT への応用が提案された Adapter Modules である．
仕組み自体は[これより前に CV 分野で提案](https://arxiv.org/abs/1705.08045)されていたが，BERT での活用は上記で初めてだった．

[この論文](https://arxiv.org/abs/1902.00751) で設定されている問題は，前述の転移学習のものとは少し異なり，様々なタスクがシステムに流れてくる．
基本の転移学習システムでは，pre-training されたモデルをあるタスク (ex. テキスト感情分類) に対して fine-tuning したらそれ以降，別のタスク (ex. 質問応答) に応用されることはない．
つまり，2 つの異なるタスクに対応したかったら，pre-training したモデルを 2 つに複製して，それぞれ別のタスクに fine-tuning する必要がある．
本論文ではこの制約が，クラウドサービス等で機械学習機能を提供する際にメモリ等の圧迫につながる問題となるとしている．
確かに，最近よく使われる BERT 等のモデルは非常にサイズが大きいため，単一のサーバーでいくつもメモリに載せておくこと，さらにはストレージに保存しておくことさえも問題となるケースは考えられる．

この問題を解決するために，Adapter Modules が提案された．
どんなものかはあとで詳細に説明するが，概要としては，N 個のタスクに対応するために BERT 全体を N 個複製するのではなく，Adapter Modules と呼ばれる小さなパラメーターブロックのみを
タスクごとに用意することで，パラメーター効率よく転移学習ができるというもの．
結果として，通常通り BERT 全体を N 個複製した時と比較して，性能はほとんど変わらずに pre-training された BERT の 3 % ほどのパラメーターを足すだけで済んだとのこと．
