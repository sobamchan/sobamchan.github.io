---
title: COLING 2022
updated: 2022-10-12
layout: tldr
---

# COLING 2022 in Japanese

## TLDRs

- [Do Language Models Make Human-like Predictions about the Coreferents of Italian Anaphoric Zero Pronouns?](https://aclanthology.org/2022.coling-1.1)
  - James A. Michaelov, Benjamin K. Bergen
  - **TLDR**: 言語モデルで、文中の単語をゼロ文字に置き換える際に、人間がどう行動するかを検証した研究。入力文の単語をゼロ文字に置き換える際に、人間がどう行動するかを予測するモデルを提案。入力文の単語をゼロ文字に置き換える際に、人間がどう行動するかを予測するモデルを提案している。

- [Language Acquisition through Intention Reading and Pattern Finding](https://aclanthology.org/2022.coling-1.2)
  - Jens Nevens, Jonas Doumen, Paul Van Eecke, Katrien Beuls
  - **TLDR**: 自然言語処理のモデルで、自然言語処理のモデルを学習する研究。モデルは、自然言語処理のモデルと同等のモデルを構築している。モデルは、自然言語処理のモデルと同等のモデルを構築している。モデルは、自然言語処理のモデルと同等のモデルを構築している。モデルは、自然言語処理のモデルと同等のモデルを構築している。

- [Stability of Syntactic Dialect Classification over Space and Time](https://aclanthology.org/2022.coling-1.3)
  - Jonathan Dunn, Sidney Wong
  - **TLDR**: 自然言語処理のモデルが、変化する空間と時間でどう変化するかを検証した研究。自然言語処理は、単語の表現を予測するモデルで、単語の表現は、単語の表現を予測するモデルで作成する。単語の表現は、単語の表現を予測するモデルで作成する。単語の表現は、単語の表現を予測するモデルで作成する。

- [Subject Verb Agreement Error Patterns in Meaningless Sentences: Humans vs. BERT](https://aclanthology.org/2022.coling-1.4)
  - Karim Lasri, Olga Seminck, Alessandro Lenci, Thierry Poibeau
  - **TLDR**: 自然言語処理で、単語数のアクティベーションを学習する研究。単語数のアクティベーションは、単語の意味が異なる場合に起こりう現象を想定している。単語数のアクティベーションは、単語の意味が異なる場合に起こりう現象を想定している。単語数のアクティベーションは、単語の意味が異なる場合に起こりう現象を想定している。

- [Measuring Morphological Fusion Using Partial Information Decomposition](https://aclanthology.org/2022.coling-1.5)
  - Michaela Socolof, Jacob Louis Hoover, Richard Futrell, Alessandro Sordoni, Timothy J. O’Donnell
  - **TLDR**: 言語の構造を、単一の表現と組み合わせる手法の提案。単一の表現は単一の表現と同等の意味を持つが、複数の表現を持つ場合、その意味が異なることを前提として、表現の組み合わせを個別に行う。これにより、単一の表現と組み合わせる場合の意味の重みを計算する。

- [Smells like Teen Spirit: An Exploration of Sensorial Style in Literary Genres](https://aclanthology.org/2022.coling-1.6)
  - Osama Khalid, Padmini Srinivasan
  - **TLDR**: 自然言語処理で、文中の表現を「センチメント」と呼ぶ研究。センチメントは自然言語処理の分類器で、文中の単語は「センチメント」と「センチメント」の2つに分類される。センチメントは、文中の単語を「センチメント」と呼ぶ文に置き換える。センチメントは、文中の単語を「センチメント」と呼ぶ文に置き換える。

- [Metaphorical Polysemy Detection: Conventional Metaphor Meets Word Sense Disambiguation](https://aclanthology.org/2022.coling-1.7)
  - Rowan Hall Maudslay, Simone Teufel
  - **TLDR**: 自然言語処理で、自然言語処理の手法として、自然言語処理の手法を用いた研究。自然言語処理では、自然言語処理の手法を用いた研究が一般的だが、この研究では自然言語処理の手法を用いた研究。自然言語処理では、自然言語処理の手法を用いた研究が一般的だが、この研究では自然言語処理の手法を用いた研究が一般的だった。

- [Machine Reading, Fast and Slow: When Do Models “Understand” Language?](https://aclanthology.org/2022.coling-1.8)
  - Sagnik Ray Choudhury, Anna Rogers, Isabelle Augenstein
  - **TLDR**: 自然言語処理モデルの読解性能を検証した研究。読解モデルは、読解の順番を徐々に読むか、読解の順番を徐々に読むか、読解の順番を徐々に読むか、という2つのタスクを学習する。順番を読むと、読解の順番が徐々に読解の順番に変わることを確認。

- [Hierarchical Attention Network for Explainable Depression Detection on Twitter Aided by Metaphor Concept Mappings](https://aclanthology.org/2022.coling-1.9)
  - Sooji Han, Rui Mao, Erik Cambria
  - **TLDR**: ストレスや抑うつといった病気の診断を行う際に、モデルの判断根拠を説明できるか検証した研究。モデルは、Twitterのタイムラインから、ストレスや抑うつに関する投稿を抽出する。この結果、モデルの判断根拠を説明できるか検証した研究。

- [Multi-view and Cross-view Brain Decoding](https://aclanthology.org/2022.coling-1.10)
  - Subba Reddy Oota, Jashn Arora, Manish Gupta, Raju S. Bapi
  - **TLDR**: マルチタスクで、画像/文/単語の3つの視点から、マルチモーダルで学習する研究。画像/文/単語の3つの視点から学習するが、画像/文/単語の3つの視点で学習するモデルは、マルチモーダルで学習するモデルより精度が低い。

- [Visio-Linguistic Brain Encoding](https://aclanthology.org/2022.coling-1.11)
  - Subba Reddy Oota, Jashn Arora, Vijay Rowtula, Manish Gupta, Raju S. Bapi
  - **TLDR**: 画像とTransformerを組み合わせて、画像とTransformerを組み合わせた研究。Transformerは画像の表現が異なる場合、画像の表現が異なると認識する。Transformerは画像の表現が異なる場合、画像の表現が異なると認識する。画像の表現は、画像の表現とTransformerの表現が異なる場合に認識する。

- [Gestures Are Used Rationally: Information Theoretic Evidence from Neural Sequential Models](https://aclanthology.org/2022.coling-1.12)
  - Yang Xu, Yang Cheng, Riya Bhatia
  - **TLDR**: 自然言語処理で、自然言語処理のパフォーマンスを検証した研究。自然言語処理では、自然言語処理のパフォーマンスを上げるための手法として、自然言語処理の挙動を学習する手法を提案している。挙動は、自然言語処理の挙動と同等の挙動を示す。

- [Revisiting Statistical Laws of Semantic Shift in Romance Cognates](https://aclanthology.org/2022.coling-1.13)
  - Yoshifumi Kawasaki, Maëlys Salingre, Marzena Karpinska, Hiroya Takamura, Ryo Nagata
  - **TLDR**: 文法の変化と、自然言語処理のペアの関係を調べた研究。ペアは、自然言語処理のペアと同等の変化が起こるが、ペアは異なる語彙を持つ。ペアの語彙は、通常の文法変化に弱いが、ペアの語彙は変化する。ペアの語彙は、通常の文法変化に弱いが、ペアの語彙は変化する。

- [Character Jacobian: Modeling Chinese Character Meanings with Deep Learning Model](https://aclanthology.org/2022.coling-1.14)
  - Yu-Hsiang Tseng, Shu-Kai Hsieh
  - **TLDR**: 自然言語処理で、単語の生成を行う際に、単語の位置関係を学習する研究。単語の位置関係は、単語の位置関係と単語の位置関係を学習する形で学習する。位置関係は、単語の位置関係と単語の位置関係を学習する形で学習する。位置関係は、単語の位置関係と単語の位置関係を学習する形で学習する。

- [COMMA: Modeling Relationship among Motivations, Emotions and Actions in Language-based Human Activities](https://aclanthology.org/2022.coling-1.15)
  - Yuqiang Xie, Yue Hu, Wei Peng, Guanqun Bi, Luxi Xing
  - **TLDR**: 自然言語処理で、行動の根拠となる感情、行動の根拠となる行動をモデル化する研究。感情は行動の根拠となる要素で、行動は行動の根拠となる要素と見なせる。感情は行動の根拠となる要素で、行動は行動の根拠となる要素と見なせる。感情は行動の根拠となる要素で、行動は行動の根拠となる要素と見なせる。

- [Exploring Semantic Spaces for Detecting Clustering and Switching in Verbal Fluency](https://aclanthology.org/2022.coling-1.16)
  - Özge Alacam, Simeon Schüz, Martin Wegrzyn, Johanna Kißler, Sina Zarrieß
  - **TLDR**: 文表現の分散表現を、文表現の分散表現に適応する手法を検証した研究。分散表現は、単語の分散表現と同等/それ以上の分散表現を認識するが、分散表現は単語分散表現と同等/それ以上の分散表現を認識する。分散表現は、単語分散表現と同等/それ以上の分散表現を認識する。

- [Neuro-Symbolic Visual Dialog](https://aclanthology.org/2022.coling-1.17)
  - Adnen Abdessaied, Mihai Bâce, Andreas Bulling
  - **TLDR**: マルチモーメントの対話を行う際に、強化学習と潜在表現を組み合わせた研究。潜在表現は、質問に対する回答を予測する形で学習する。潜在表現は、質問に対する回答が予測されるかを予測する形で学習する。潜在表現は、質問に対する回答が予測されるかを予測する形で学習する。

- [LINGUIST: Language Model Instruction Tuning to Generate Annotated Utterances for Intent Classification and Slot Tagging](https://aclanthology.org/2022.coling-1.18)
  - Andy Rosenbaum, Saleh Soltan, Wael Hamza, Yannick Versley, Markus Boese
  - **TLDR**: マルチ言語の翻訳で、文分類器を強化する研究。文分類器は、文分類器の学習済みモデルと同等の精度を維持できるが、文分類器は文分類器の学習済みモデルと同等の精度を維持できない。そのため、文分類器の学習済みモデルを学習させる。文分類器は、文分類器の学習済みモデルと同等の精度を維持できる。

- [Adaptive Natural Language Generation for Task-oriented Dialogue via Reinforcement Learning](https://aclanthology.org/2022.coling-1.19)
  - Atsumoto Ohashi, Ryuichiro Higashinaka
  - **TLDR**: 自然言語処理で、タスクを解く際に自然言語を生成する手法の提案。タスクはタスクの学習済みモデルで行うが、生成はタスクの学習済みモデルで行う。タスクはタスクの学習済みモデルで行うが、タスクはタスクの学習済みモデルで行う。タスクはタスクの学習済みモデルで行う。

- [TAKE: Topic-shift Aware Knowledge sElection for Dialogue Generation](https://aclanthology.org/2022.coling-1.20)
  - Chenxu Yang, Zheng Lin, Jiangnan Li, Fandong Meng, Weiping Wang, Lanrui Wang, Jie Zhou
  - **TLDR**: マルチモーメントの対話で、質問に対する回答を自動生成するタスクを提案。質問に対する回答は、質問に対する質問に対する回答と同等の役割を果たすか、また質問に対する回答が同等になるかを判定する。質問に対する回答は、質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する質問に対する

- [Dynamic Dialogue Policy for Continual Reinforcement Learning](https://aclanthology.org/2022.coling-1.21)
  - Christian Geishauser, Carel van Niekerk, Hsien-chin Lin, Nurul Lubis, Michael Heck, Shutong Feng, Milica Gašić
  - **TLDR**: 対話システムの強化学習について、強化学習の仕組みをまとめた研究。強化学習は、強化学習の仕組みを学習する際の初期値と、強化学習の初期値を比較し、強化学習の初期値と比較した結果を比較し、強化学習の初期値と比較した結果を比較し、強化学習の初期値と比較した結果を比較し、強化学習の初期値と比較した結果を比較し、強化学習の初期値と比較した結果を比較し、強化学習の初期値と比較した結果を比較し、強化学習の初期値と比較した結果を比較し、強化学習の初期値と比較した結果を比較し、

- [GRAVL-BERT: Graphical Visual-Linguistic Representations for Multimodal Coreference Resolution](https://aclanthology.org/2022.coling-1.22)
  - Danfeng Guo, Arpit Gupta, Sanchit Agarwal, Jiun-Yu Kao, Shuyang Gao, Arijit Biswas, Chien-Wei Lin, Tagyoung Chung, Mohit Bansal
  - **TLDR**: マルチモーダルで学習するマルチタスクのモデルを提案。マルチモーダルで学習する際は、各モーダルで学習した情報を統合して学習する。マルチモーダルで学習する際は、各モーダルで学習した情報を統合して学習する。マルチモーダルで学習する際は、マルチタスクのモデルを用いることで学習する。

- [Learning to Improve Persona Consistency in Multi-party Dialogue Generation via Text Knowledge Enhancement](https://aclanthology.org/2022.coling-1.23)
  - Dongshi Ju, Shi Feng, Pengcheng Lv, Daling Wang, Yifei Zhang
  - **TLDR**: マルチペアの対話システムで、対話者の属性を入力するグラフを生成する研究。入力は、対話者とそのプロフィールを入力とする。入力は、対話者とプロフィールを入力とする。入力は、対話者とプロフィールを入力とする。入力は、対話者とプロフィールを入力とする。

- [Improving Top-K Decoding for Non-Autoregressive Semantic Parsing via Intent Conditioning](https://aclanthology.org/2022.coling-1.24)
  - Geunseob Oh, Rahul Goel, Chris Hidey, Shachi Paul, Aditya Gupta, Pararth Shah, Rushin Shah
  - **TLDR**: 対話システムにおける、事前学習済みモデルの提案。事前学習済みモデルは、事前学習済みモデルの予測結果から、予測結果から予測結果を推定する。事前学習済みモデルは、事前学習済みモデルの予測結果から予測結果を推定する。事前学習済みモデルは、事前学習済みモデルの予測結果から予測結果を推定する。事前学習済みモデルは、事前学習済みモデルの予測結果から予測結果を推定する。

- [Autoregressive Entity Generation for End-to-End Task-Oriented Dialog](https://aclanthology.org/2022.coling-1.25)
  - Guanhuan Huang, Xiaojun Quan, Qifan Wang
  - **TLDR**: 事前学習済みモデルで、事前学習済みモデルのEntityを生成する際のルールを緩和した研究。Entityは事前学習済みモデルのEntityと同等の扱いが行えるが、事前学習済みモデルはEntityの重みを計算する必要があるため、重みを計算する際は、Entityの重みを計算する。

- [Continual Few-shot Intent Detection](https://aclanthology.org/2022.coling-1.26)
  - Guodun Li, Yuchen Zhai, Qianglong Chen, Xing Gao, Ji Zhang, Yin Zhang
  - **TLDR**: 対話システムの学習時に、学習済みモデルの学習済みモデルの学習済みモデルを学習する手法の提案。学習済みモデルは学習済みモデルの学習済みモデルと同等の学習データが必要で、学習済みモデルの学習済みモデルの学習済みモデルを学習データとして使用する。学習済みモデルの学習済みモデルは学習データの少ないクラスに学習する。

- [“Mama Always Had a Way of Explaining Things So I Could Understand”: A Dialogue Corpus for Learning to Construct Explanations](https://aclanthology.org/2022.coling-1.27)
  - Henning Wachsmuth, Milad Alshomary
  - **TLDR**: 自然言語処理で、説明を行う際に人間が説明を行う際のルールをまとめた研究。説明は、説明する側と説明する側双方で行う。説明は、説明する側と説明する側の双方で行う。説明は、説明する側と説明する側のルールをまとめた文書で行う。

- [Schema Encoding for Transferable Dialogue State Tracking](https://aclanthology.org/2022.coling-1.28)
  - Hyunmin Jeon, Gary Geunbae Lee
  - **TLDR**: 対話システムの転移を行う際に、転移先のネットワークを転移先のネットワークに適用する研究。転移先のネットワークは、転移先のネットワークのネットワークの構造を学習する。転移先のネットワークの構造を学習する際は、転移先のネットワークの構造を学習する。転移先のネットワークの構造を学習する際は、転移先のネットワークの構造を学習する。

- [A Personalized Dialogue Generator with Implicit User Persona Detection](https://aclanthology.org/2022.coling-1.29)
  - Itsugun Cho, Dongyang Wang, Ryota Takahashi, Hiroaki Saito
  - **TLDR**: 対話システムの生成を行う際に、ユーザーのプロフィールを自動生成する研究。生成した対話システムは、ユーザーのプロフィールを入力として生成する。入力は、入力した対話システムのデータセットから生成する。入力は、入力した対話システムのデータセットから生成する。入力は、入力した対話システムのデータセットから生成する。

- [Incorporating Casual Analysis into Diversified and Logical Response Generation](https://aclanthology.org/2022.coling-1.30)
  - Jiayi Liu, Wei Wei, Zhixuan Chu, Xing Gao, Ji Zhang, Tan Yan, Yulin Kang
  - **TLDR**: 質問に対するAuto Encoderの生成結果を、事前学習済みモデルと比較して更新する研究。事前学習済みモデルは、質問に対するAuto Encoderの更新が困難な場合、更新したモデルを再帰的に学習する。更新したモデルは、質問に対するAuto Encoderの更新が困難な場合、更新したモデルを再帰的に学習する。

- [Reciprocal Learning of Knowledge Retriever and Response Ranker for Knowledge-Grounded Conversations](https://aclanthology.org/2022.coling-1.31)
  - Jiazhan Feng, Chongyang Tao, Zhen Li, Chang Liu, Tao Shen, Dongyan Zhao
  - **TLDR**: 対話システムで、質問文から質問文に対する評価を自動生成する研究。質問文は質問文の文書構造をベースに、質問文の文書構造をベースに質問文を生成する。質問文は質問文の文書構造をベースに、質問文は質問文の文書構造をベースに生成する。質問文は質問文の文書構造をベースに生成する。

- [CR-GIS: Improving Conversational Recommendation via Goal-aware Interest Sequence Modeling](https://aclanthology.org/2022.coling-1.32)
  - Jinfeng Zhou, Bo Wang, Zhitong Yang, Dongming Zhao, Kun Huang, Ruifang He, Yuexian Hou
  - **TLDR**: 多回の質問に対し、ユーザーの興味を推定する研究。質問の順序は、質問に対するユーザーの興味分布を推定する。質問の順序は、質問に対するユーザーの興味分布を推定する。質問の順序は、質問に対するユーザーの興味分布を推定する。質問の順序は、質問に対するユーザーの興味分布を推定する。

- [GRASP: Guiding Model with RelAtional Semantics Using Prompt for Dialogue Relation Extraction](https://aclanthology.org/2022.coling-1.33)
  - Junyoung Son, Jinsung Kim, Jungwoo Lim, Heuiseok Lim
  - **TLDR**: 対話中の文関係を抽出する研究。文関係は文関係の単語を入力とする入力と、文関係の単語を入力とする入力を交互に入力とする入力の2つに分け、入力と入力の双方を入力とする入力を交互に入力とする入力を交互に入力とする入力と、入力と入力双方を入力とする入力を交互に入力とする入力を交互に入力とする入力を交互に入力とする入力を交互に入力とする入力を交互に入力とする入力を交互に入力とする入力を交互に入力とする入力を交互に入力とする入力を交互に入力とする入力を交互に入力とする入力を交互に入力とする入力を交互に入力とする入力

- [PEPDS: A Polite and Empathetic Persuasive Dialogue System for Charity Donation](https://aclanthology.org/2022.coling-1.34)
  - Kshitij Mishra, Azlaan Mustafa Samad, Palak Totala, Asif Ekbal
  - **TLDR**: 対話システムの強化学習で、感情、Policy、Policyの3つの分類モデルを強化学習で学習する研究。感情は、相手に対する不信感を解消するだけでなく、相手に対する不信感を増やしていこうという意。Policyは、Policyの行動を評価する指標として使用している。Policyは、Policyの行動を評価する指標として使用している。

- [DialAug: Mixing up Dialogue Contexts in Contrastive Learning for Robust Conversational Modeling](https://aclanthology.org/2022.coling-1.35)
  - Lahari Poddar, Peiyao Wang, Julia Reinspach
  - **TLDR**: 対話システムで、事前学習済みの対話文を強化学習で学習する研究。対話文の潜在表現を強化学習で学習する。対話文の潜在表現は、通常の学習済み対話文と同等の精度を維持できるか検証している。対話文の潜在表現は、通常の学習済み対話文と同等の精度を維持できるか検証している。

- [A Closer Look at Few-Shot Out-of-Distribution Intent Detection](https://aclanthology.org/2022.coling-1.36)
  - Li-Ming Zhan, Haowen Liang, Lu Fan, Xiao-Ming Wu, Albert Y.S. Lam
  - **TLDR**: 事前学習済みモデルの事前学習済みモデルに対する事前学習済みモデルの対応策をまとめた研究。事前学習済みモデルは事前学習済みモデルの潜在表現を認識するが、事前学習済みモデルは事前学習済みモデルの潜在表現を認識することができない。事前学習済みモデルの潜在表現を認識する手法を提案。事前学習済みモデルの潜在表現を認識するモデルを提案

- [CGIM: A Cycle Guided Interactive Learning Model for Consistency Identification in Task-oriented Dialogue](https://aclanthology.org/2022.coling-1.37)
  - Libo Qin, Qiguang Chen, Tianbao Xie, Qian Liu, Shijue Huang, Wanxiang Che, Zhou Yu
  - **TLDR**: タスク分類のモデルで、タスク分類のモデルとCycle Guided Interactive Learning Modelを組み合わせた研究。タスク分類はタスク分類のモデルと、タスク分類のモデルとで交互に行う。タスク分類はタスク分類のモデルで、タスク分類はタスク分類のモデルで行う。タスク分類はタスク分類のモデルで行う。

- [CorefDiffs: Co-referential and Differential Knowledge Flow in Document Grounded Conversations](https://aclanthology.org/2022.coling-1.38)
  - Lin Xu, Qixian Zhou, Jinlan Fu, Min-Yen Kan, See-Kiong Ng
  - **TLDR**: 文書の文書構造を、文書の文書構造と文書の文書構造を組み合わせたモデルで表現する研究。文書構造は文書の文書構造と文書文書文書構造を組み合わせたもの。文書文書構造は文書文書構造と文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書

- [SelF-Eval: Self-supervised Fine-grained Dialogue Evaluation](https://aclanthology.org/2022.coling-1.39)
  - Longxuan Ma, Ziyu Zhuang, Weinan Zhang, Mingda Li, Ting Liu
  - **TLDR**: 対話品質の評価をSelf-Supervisedで行う研究。対話品質の評価は、対話結果と対話結果のデータセットを統合して行う。対話結果は、対話結果とデータセットの統合で生成される。対話結果は、対話結果とデータセットの統合で生成される。対話結果は、対話結果とデータセットの統合で生成される。

- [Open-Domain Dialog Evaluation Using Follow-Ups Likelihood](https://aclanthology.org/2022.coling-1.40)
  - Maxime De Bruyn, Ehsan Lotfi, Jeska Buhmann, Walter Daelemans
  - **TLDR**: オープンドメインの対話システムで、自動評価を行う際に、質問に対する回答が人間評価と近いかを計測する研究。質問に対する回答は、質問に対する回答と同等の精度を維持するよう、質問に対する回答を個別に更新するよう、自動評価を行う。

- [Joint Goal Segmentation and Goal Success Prediction on Multi-Domain Conversations](https://aclanthology.org/2022.coling-1.41)
  - Meiguo Wang, Benjamin Yao, Bin Guo, Xiaohu Liu, Yu Zhang, Tuan-Hung Pham, Chenlei Guo
  - **TLDR**: マルチドメインの対話システムで、ゴールをセグメンテーションするタスクを提案。ゴールは、ユーザーの満足度と、対話結果の貢献度を比較するタスク。ゴールは、ユーザーの満足度と、対話結果の貢献度を比較するタスク。タスクは、ゴール到達率を予測するタスクと、ゴール到達率を予測するタスクの2つ。

- [Slot Dependency Modeling for Zero-Shot Cross-Domain Dialogue State Tracking](https://aclanthology.org/2022.coling-1.42)
  - Qingyue Wang, Yanan Cao, Piji Li, Yanhe Fu, Zheng Lin, Li Guo
  - **TLDR**: 対話システムの事前学習で、Slotの依存を学習する研究。事前学習済みの対話システムを、事前学習済みの対話システムの依存と、事前学習済み対話システムの依存を組み合わせた形で学習する。事前学習済み対話システムの依存は、事前学習済み対話システムの依存と同等になる。

- [Section-Aware Commonsense Knowledge-Grounded Dialogue Generation with Pre-trained Language Model](https://aclanthology.org/2022.coling-1.43)
  - Sixing Wu, Ying Li, Ping Xue, Dawei Zhang, Zhonghai Wu
  - **TLDR**: 事前学習済み言語モデルで事前学習済み知識を事前学習した研究。事前学習済み言語モデルは、事前学習済み知識を事前学習済み言語モデルに入力し、事前学習済み言語モデルの事前学習済み知識を事前学習済み言語モデルに入力する。事前学習済み言語モデルは事前学習済み知識を事前学習済み言語モデルに入力する。事前学習済み言語モデルは事前学習済み言語モデルの学習済み知識を事前学習済み言語モデルに入力する。事前学習済み言語モデルは事前学習済み言語モデルの学習済み知識を事前学習済み言語モデルに入力する。

- [Using Multi-Encoder Fusion Strategies to Improve Personalized Response Selection](https://aclanthology.org/2022.coling-1.44)
  - Souvik Das, Sougata Saha, Rohini K. Srihari
  - **TLDR**: 対話システムにおける、感情とEntailmentの相互作用を検証した研究。Entailmentは、質問に対する回答の質と、質問に対する回答の質を直接的に関係する。Entailmentは、質問に対する回答の質と、質問に対する回答の質を直接関係する。

- [A Multi-Dimensional, Cross-Domain and Hierarchy-Aware Neural Architecture for ISO-Standard Dialogue Act Tagging](https://aclanthology.org/2022.coling-1.45)
  - Stefano Mezza, Wayne Wobcke, Alan Blair
  - **TLDR**: 対話システムにおけるマルチラベルの分類を、事前学習済みモデルで行う研究。事前学習済みモデルは、事前学習済みラベルのラベルを入力とする。ラベルは、事前学習済みモデルのラベルと同等精度を達成できた。

- [SPACE-2: Tree-Structured Semi-Supervised Contrastive Pre-training for Task-Oriented Dialog Understanding](https://aclanthology.org/2022.coling-1.46)
  - Wanwei He, Yinpei Dai, Binyuan Hui, Min Yang, Zheng Cao, Jianbo Dong, Fei Huang, Luo Si, Yongbin Li
  - **TLDR**: 対話システムの事前学習で、ラベルなしの対話からラベルなしの対話に転移する研究。ラベルなしの対話は、ラベルなしの対話と同等の効果を得るが、ラベルなしの対話はラベルなしの対話と同等の効果を得る。ラベルなし対話は、ラベルなしの対話と同等の効果を得る。

- [ET5: A Novel End-to-end Framework for Conversational Machine Reading Comprehension](https://aclanthology.org/2022.coling-1.47)
  - Xiao Zhang, Heyan Huang, Zewen Chi, Xian-Ling Mao
  - **TLDR**: 自然言語処理で、質問回答のモデルを提案。質問回答のモデルは、質問回答の文をEntailment Reasoningに変換する。この時、Entailment Reasoningは文の文長ではなく、文長を含まない単語を含まない単語に置き換える。この文長を、文長を含まない単語に置き換える形で処理する。

- [CoHS-CQG: Context and History Selection for Conversational Question Generation](https://aclanthology.org/2022.coling-1.48)
  - Xuan Long Do, Bowei Zou, Liangming Pan, Nancy F. Chen, Shafiq Joty, Ai Ti Aw
  - **TLDR**: 質問生成を行う際に、文の意味を推定する研究。文の意味を推定する際、文の文長を短くする。文長は文長の長さに依存するが、文長は文長の長さに依存する。文長は文長の長さを考慮する必要がある。文長は文長の長さを考慮する必要がある。

- [Semantic-based Pre-training for Dialogue Understanding](https://aclanthology.org/2022.coling-1.49)
  - Xuefeng Bai, Linfeng Song, Yue Zhang
  - **TLDR**: 事前学習済み言語モデルを、事前学習済みモデルの表現力に置き換えた研究。事前学習済みモデルの表現力は、事前学習済みモデルの表現力と同等の表現力を持つ。事前学習済みモデルは、事前学習済みモデルの表現力と同等の表現力を持つ。事前学習済みモデルは、事前学習済みモデルの表現力と同等の表現力を持つ。

- [Distribution Calibration for Out-of-Domain Detection with Bayesian Approximation](https://aclanthology.org/2022.coling-1.50)
  - Yanan Wu, Zhiyuan Zeng, Keqing He, Yutao Mou, Pei Wang, Weiran Xu
  - **TLDR**: タスクの挙動を予測する際、事前学習済みモデルの挙動を誤認識する問題がある。この問題を解消するために、事前学習済みモデルの挙動を誤認識するMonte-Carlo Dropoutを導入している。これにより、事前学習済みモデルの挙動を誤認識する問題を防ぎつつ、事前学習済みモデルの挙動を誤認識する問題を防ぎつつ、事前学習済みモデルの挙動を誤認識する問題を防ぎつつ、事前学習済みモデルの挙動を誤認識する問題を防ぎつつ、事前学習済みモデルの挙動を誤認識する問題を防ぎつつ、事前学習済みモデルの挙動

- [Tracking Satisfaction States for Customer Satisfaction Prediction in E-commerce Service Chatbots](https://aclanthology.org/2022.coling-1.51)
  - Yang Sun, Liangqing Wu, Shuangyong Song, Xiaoguang Yu, Xiaodong He, Guohong Fu
  - **TLDR**: 対話システムにおけるCNNの提案。対話システムの挙動を、対話システムの挙動と対話システムの挙動を比較し、対話システムの挙動と対話システムの挙動を比較した研究。対話システムの挙動は対話システムの挙動と対話システムの挙動を比較し、対話システムの挙動と対話システムの挙動を比較した研究。対話システムの挙動は対話システムの挙動と対話システムの挙動を比較し、対話システムの挙動と対話システムの挙動を比較した研究。

- [Towards Multi-label Unknown Intent Detection](https://aclanthology.org/2022.coling-1.52)
  - Yawen Ouyang, Zhen Wu, Xinyu Dai, Shujian Huang, Jiajun Chen
  - **TLDR**: マルチラベルの潜在表現を解く手法の提案。マルチラベルの潜在表現は、複数の潜在表現が存在する可能性があることを前提としている。そのため、複数の潜在表現が存在する可能性があることを前提としている。マルチラベルの潜在表現は、潜在表現の潜在表現を予測するタスクで解く。

- [Pan More Gold from the Sand: Refining Open-domain Dialogue Training with Noisy Self-Retrieval Generation](https://aclanthology.org/2022.coling-1.53)
  - Yihe Wang, Yitong Li, Yasheng Wang, Fei Mi, Pingyi Zhou, Xin Wang, Jin Liu, Xin Jiang, Qun Liu
  - **TLDR**: オープンドメインの対話システムを生成する際、事前学習済み言語モデルを用いた研究。事前学習済み言語モデルは、事前学習済みの対話データに対する評価を学習する。評価は、事前学習済み言語モデルの評価と同等の評価を行なっている。事前学習済み言語モデルの評価は、事前学習済み言語モデルの評価と同等精度を達成。

- [MulZDG: Multilingual Code-Switching Framework for Zero-shot Dialogue Generation](https://aclanthology.org/2022.coling-1.54)
  - Yongkang Liu, Shi Feng, Daling Wang, Yifei Zhang
  - **TLDR**: 事前学習済み言語モデルを、ゼロから学習する手法の提案。事前学習済み言語モデルは、事前学習済み言語モデルの学習済み言語モデルと同等の精度を維持できるが、事前学習済み言語モデルは学習済み言語モデルと同等の精度を維持できない。そこで、事前学習済み言語モデルを学習済み言語モデルに置き換える手法を提案。

- [Target-Guided Open-Domain Conversation Planning](https://aclanthology.org/2022.coling-1.55)
  - Yosuke Kishinami, Reina Akama, Shiki Sato, Ryoko Tokuhisa, Jun Suzuki, Kentaro Inui
  - **TLDR**: 自然言語処理のタスクで、事前学習済みモデルが事前学習済みモデルと比較してどのようなタスクをこなしているかを調べた研究。事前学習済みモデルは事前学習済みモデルと比較して、事前学習済みモデルのタスクを上回る精度を達成できた。事前学習済みモデルは事前学習済みモデルと比較して、事前学習済みモデルのタスクを上回る精度を達成できた。

- [Does GPT-3 Generate Empathetic Dialogues? A Novel In-Context Example Selection Method and Automatic Evaluation Metric for Empathetic Dialogue Generation](https://aclanthology.org/2022.coling-1.56)
  - Young-Jun Lee, Chae-Gyun Lim, Ho-Jin Choi
  - **TLDR**: 対話システムの事前学習で、事前学習済みモデルと同等の効果を上げるために、事前学習済みモデルの事前学習を組み合わせた研究。事前学習済みモデルは事前学習済みモデルと同等の効果を上げることができ、事前学習済みモデルの事前学習済みモデルと同等の効果を上げることができる。事前学習済みモデルは事前学習済みモデルと同等の効果を上げることができるが、事前学習済みモデルは事前学習済みモデルと同等の効果を上げることができなかった。

- [DialogueEIN: Emotion Interaction Network for Dialogue Affective Analysis](https://aclanthology.org/2022.coling-1.57)
  - Yuchen Liu, Jinming Zhao, Jingwen Hu, Ruichen Li, Qin Jin
  - **TLDR**: 対話システムにおける、感情の変化をモデル化する研究。対話システムは、対話者と対話結果の予測結果を統合するネットワークで、対話結果の予測結果を統合するネットワークを構築している。対話結果は、対話結果と統合結果の予測結果を統合するネットワークを構築している。

- [Towards Enhancing Health Coaching Dialogue in Low-Resource Settings](https://aclanthology.org/2022.coling-1.58)
  - Yue Zhou, Barbara Di Eugenio, Brian Ziebart, Lisa Sharp, Bing Liu, Ben Gerber, Nikolaos Agadakos, Shweta Yadav
  - **TLDR**: 医療の対話システムを自動生成する研究。対話システムはシンプルなNLU/NLGで、事前学習済みモデルをベースにSelf-Attentionを導入し、Self-Attentionの生成を自動生成する。Self-Attentionは、患者の行動を評価する指標として使用される。

- [Generalized Intent Discovery: Learning from Open World Dialogue System](https://aclanthology.org/2022.coling-1.59)
  - Yutao Mou, Keqing He, Yanan Wu, Pei Wang, Jingang Wang, Wei Wu, Yi Huang, Junlan Feng, Weiran Xu
  - **TLDR**: オープンドメインの潜在表現を分類する研究。通常の潜在表現は、潜在表現のクラス分類器で分類するが、このクラス分類器は通常の潜在表現を認識することができない。そのため、潜在表現を分類器で分類するタスクを提案。分類器は、潜在表現のクラス分類器と同等の役割を果たしている。

- [DialMed: A Dataset for Dialogue-based Medication Recommendation](https://aclanthology.org/2022.coling-1.60)
  - Zhenfeng He, Yuqiang Han, Zhenqiu Ouyang, Wei Gao, Hongxu Chen, Guandong Xu, Jian Wu
  - **TLDR**: 医療対話システムで、薬の推奨を行おうという研究。医療対話システムは医療従事者と患者双方の対話が行えるが、患者側は医療従事者と対話する文書を収集し、患者側は文書から医師と患者双方の対話内容を抽出する。文書から医師と患者双方の対話情報を抽出するネットワークを構築し、患者側は文書から医師と患者双方の対話情報を抽出する。

- [Speaker Clustering in Textual Dialogue with Pairwise Utterance Relation and Cross-corpus Dialogue Act Supervision](https://aclanthology.org/2022.coling-1.61)
  - Zhihua Su, Qiang Zhou
  - **TLDR**: マルチペアの対話で、質問文を個別に分類する研究。質問文は、質問文の文脈を予測する形で生成される。質問文は、質問文の文脈を予測する形で生成される。質問文は、質問文の文脈を予測する形で生成される。質問文の文脈は、質問文の文脈から予測する。

- [TopKG: Target-oriented Dialog via Global Planning on Knowledge Graph](https://aclanthology.org/2022.coling-1.62)
  - Zhitong Yang, Bo Wang, Jinfeng Zhou, Yue Tan, Dongming Zhao, Kun Huang, Ruifang He, Yuexian Hou
  - **TLDR**: グローバルなタスクで、グローバルな戦略を学習する研究。タスクはグローバルな戦略を学習する。タスクはグローバルな戦略を学習する。タスクはグローバルな戦略を学習する。タスクはタスクの学習済みモデルを学習する。タスクはタスクの学習済みモデルを学習する。タスクはタスクの学習済みモデルを学習する。タスクはタスクの学習済みモデルを学習する。タスクはタスクの学習済みモデルを学習する。タスクはタスクの学習済みモデルを学習する。タスクはタスクの学習済みモデルを学習する。タスクはタスクの学習済みモデルを学習する

- [Extractive Summarisation for German-language Data: A Text-level Approach with Discourse Features](https://aclanthology.org/2022.coling-1.63)
  - Freya Hewett, Manfred Stede
  - **TLDR**: 文中の単語を抽出する手法の研究。文中の単語を抽出する際、文中の単語を抽出するモデル(RNN)と、文中の単語を抽出するモデル(ANN)を比較し、抽出結果がどちらが良いかを検証した研究。抽出結果は、ANNが良好な結果だった。

- [End-to-End Neural Bridging Resolution](https://aclanthology.org/2022.coling-1.64)
  - Hideo Kobayashi, Yufang Hou, Vincent Ng
  - **TLDR**: 既存のbridgingの手法は、モデルの挙動が不完全で、またモデルの挙動が不完全な状態にある場合に有効な手法といえる。この問題を解決するために、モデルの挙動を不完全な状態に陥らないようにする手法を提案している。

- [Investigating the Performance of Transformer-Based NLI Models on Presuppositional Inferences](https://aclanthology.org/2022.coling-1.65)
  - Jad Kabbara, Jackie Chi Kit Cheung
  - **TLDR**: 言語モデルの潜在表現を解釈する研究。潜在表現は、通常の文表現と異なり、文の意味を予測する際は、意味の予測が間違っているかを検証する。この検証では、文の意味を予測するモデルが、文の意味を予測するモデルと同等精度を達成できたか検証している。

- [Re-Examining FactBank: Predicting the Author’s Presentation of Factuality](https://aclanthology.org/2022.coling-1.66)
  - John Murzaku, Peter Zeng, Magdalena Markowska, Owen Rambow
  - **TLDR**: 事実誤認を検証する手法の提案。事実誤認は、事実の事実誤認を検証する指標としてF-measureを用いている。F-measureは、事実誤認の結果を検証する指標として使用されている。

- [The Role of Context and Uncertainty in Shallow Discourse Parsing](https://aclanthology.org/2022.coling-1.67)
  - Katherine Atwell, Remi Choi, Junyi Jessy Li, Malihe Alikhani
  - **TLDR**: 文の文脈を予測するモデルの精度を上げるために、文脈の予測精度を上げるためには、文脈の予測精度が上がるかを検証した研究。文脈の予測精度は、文脈の予測精度と同等の精度を達成するが、文脈の予測精度は文脈の予測精度と同等の精度を達成するのかを検証した。文脈の予測精度は、文脈の予測精度と同等の精度を達成する。

- [Improving Commonsense Contingent Reasoning by Pseudo-data and Its Application to the Related Tasks](https://aclanthology.org/2022.coling-1.68)
  - Kazumasa Omura, Sadao Kurohashi
  - **TLDR**: 自然言語処理で、事前予測の予測結果と実際の予測結果を組み合わせて、予測結果と実際の予測結果を組み合わせて予測を行う研究。事前予測結果と実際の予測結果を組み合わせて、予測結果と実際の予測結果を組み合わせて予測を行う。事前予測結果は、予測結果と実際の予測結果を組み合わせて作成する。

- [A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives](https://aclanthology.org/2022.coling-1.69)
  - Qingcheng Zeng, An-Ran Li
  - **TLDR**: 自然言語処理における、自然言語処理のタスクについてまとめたサーベイ。タスクの概要、タスクの手法、タスクの適用例、タスクの評価方法、タスクの評価結果についてまとめられている。タスクの評価は、タスクの評価結果から行うことが多いが、タスクの評価はタスクの評価結果から行うことが多い。

- [Towards Identifying Alternative-Lexicalization Signals of Discourse Relations](https://aclanthology.org/2022.coling-1.70)
  - René Knaebel, Manfred Stede
  - **TLDR**: 文の意味を変えた表現を学習する研究。単語の意味を変えた表現を学習するモデルと、単語の意味を変えた表現を学習するモデルを比較し、どちらが良いかを検証している。学習は、単語の意味を変えた表現を学習するモデルと、単語の意味を変えた表現を学習するモデルの2つを使用。

- [Topicalization in Language Models: A Case Study on Japanese](https://aclanthology.org/2022.coling-1.71)
  - Riki Fujihara, Tatsuki Kuribayashi, Kaori Abe, Ryoko Tokuhisa, Kentaro Inui
  - **TLDR**: 自然言語処理で、質問文の文構造をどう学習するのかを調査した研究。質問文は、質問文の文構造をベースに、質問文の文構造をベースに学習する。質問文は、質問文の文構造をベースに、質問文の文構造をベースに学習する。質問文の文構造は、質問文の文構造をベースに学習する。

- [“No, They Did Not”: Dialogue Response Dynamics in Pre-trained Language Models](https://aclanthology.org/2022.coling-1.72)
  - Sanghee J. Kim, Lang Yu, Allyson Ettinger
  - **TLDR**: 事前学習済み言語モデルで、Attentionの役割を調べた研究。Attentionは文書中の単語を識別する重要な要素として扱われ、文書中の単語はAttentionの役割を果たしている。文書中の単語はAttentionの役割を果たしているが、文書中の文書はAttentionの役割を果たしていない。

- [New or Old? Exploring How Pre-Trained Language Models Represent Discourse Entities](https://aclanthology.org/2022.coling-1.73)
  - Sharid Loáiciga, Anne Beyer, David Schlangen
  - **TLDR**: 事前学習済み言語モデルが、文中の単語を認識する仕組みについて調査した研究。文中の単語は単語分類器で分類し、文中の単語が単語分類器に認識されているかを調べる。単語分類器は単語の分類を学習するが、文中の単語は単語分類器で認識することができない。そのため、文中の単語を単語分類器で認識するモデルを提案

- [Dialo-AP: A Dependency Parsing Based Argument Parser for Dialogues](https://aclanthology.org/2022.coling-1.74)
  - Sougata Saha, Souvik Das, Rohini K. Srihari
  - **TLDR**: 対話システムで、文書から論理モデルを作成する研究。文書から文書を生成する際は、文書の文書構造を文書構造と文書構造の2つに分割し、文書構造を文書構造と文書構造の2つに分割する。文書構造は文書構造と文書構造の2つに分割し、文書構造は文書構造と文書構造の2つに分割する。文書構造は文書構造と文書構造の2つに分割する。文書構造は文書構造と文書構造の2つに分割する。文書構造は文書構造と文書構造の2つに分割する。文書構造は文書構造と文書構造の2つに分割する。文書構造

- [ConnPrompt: Connective-cloze Prompt Learning for Implicit Discourse Relation Recognition](https://aclanthology.org/2022.coling-1.75)
  - Wei Xiang, Zhenglin Wang, Lu Dai, Bang Wang
  - **TLDR**: 事前学習済み言語モデルを、事前学習済みモデルと同等のモデルに置き換える研究。事前学習済みモデルは事前学習済み言語モデルの予測結果を基に学習するが、事前学習済みモデルは予測結果を基に学習する。事前学習済みモデルは事前学習済み言語モデルの予測結果を基に学習するが、事前学習済みモデルは予測結果を基に学習する。

- [A Distance-Aware Multi-Task Framework for Conversational Discourse Parsing](https://aclanthology.org/2022.coling-1.76)
  - Yaxin Fan, Peifeng Li, Fang Kong, Qiaoming Zhu
  - **TLDR**: マルチタスクで対話を行う際、グラフベースのモデルとTransformerベースのモデルを組み合わせて学習する研究。Transformerベースは、グラフをベースに、グラフをベースに複数タスクを同時に学習する。このタスクを、事前学習済みモデルのEncoder/Decoderで行う。

- [Linguistically Motivated Features for Classifying Shorter Text into Fiction and Non-Fiction Genre](https://aclanthology.org/2022.coling-1.77)
  - Arman Kazmi, Sidharth Ranjan, Arpit Sharma, Rajakrishnan Rajkumar
  - **TLDR**: 文分類において、文分類モデルを用いた研究。文分類モデルは文分類精度を上げるために、文分類の分類精度を上げるために使われる手法。文分類モデルは文分類精度を上げるために使われる手法で、文分類精度が高いほど文分類精度が高い。文分類精度が高いほど文分類精度が高い。

- [Semantic Sentence Matching via Interacting Syntax Graphs](https://aclanthology.org/2022.coling-1.78)
  - Chen Xu, Jun Xu, Zhenhua Dong, Ji-Rong Wen
  - **TLDR**: 文の文構造を、文の文構造と同等の画像に変換する研究。文の文構造は、文の文構造と同等の画像に変換する。文の文構造は、文の文構造と同等の画像に変換する。文の文構造は、文の文構造と同等の画像に変換する。文の文構造は、文の文構造と同等の画像に変換する。

- [Hierarchical Information Matters: Text Classification via Tree Based Graph Neural Network](https://aclanthology.org/2022.coling-1.79)
  - Chong Zhang, He Zhu, Xingyu Peng, Junran Wu, Ke Xu
  - **TLDR**: 自然言語処理における文書分類タスクで、階層情報の活用を検証した研究。階層情報の分布を階層構造のネットワークで表現し、階層構造のネットワークで表現した階層情報を階層構造のネットワークで表現する。階層構造は、階層情報の分布を階層構造のネットワークで表現する。

- [SelfMix: Robust Learning against Textual Label Noise with Self-Mixup Training](https://aclanthology.org/2022.coling-1.80)
  - Dan Qiao, Chenchen Dai, Yuyang Ding, Juntao Li, Qiang Chen, Wenliang Chen, Min Zhang
  - **TLDR**: 事前学習済み言語モデルで、ラベルノイズを除去する研究。ラベルノイズは事前学習済みモデルの学習時に発生するが、事前学習済みモデルはラベルノイズを除去する。事前学習済みモデルは、事前学習済みモデルの学習時にラベルノイズを除去する。事前学習済みモデルは、事前学習済みモデルの学習時にラベルノイズを除去する。

- [Community Topic: Topic Model Inference by Consecutive Word Community Discovery](https://aclanthology.org/2022.coling-1.81)
  - Eric Austin, Osmar R. Zaïane, Christine Largeron
  - **TLDR**: マルチパラメーターのタスクで、事前学習済みモデルを用いた研究。タスクは、タスクのタスクでタスクを分類する。タスクは、タスクのタスクでタスクを分類する。タスク分類は、タスク分類のタスクで行う。タスク分類は、タスク分類のタスクで行う。タスク分類は、タスク分類のタスクで行う。タスク分類は、タスク分類のタスクで行う。タスク分類は、タスク分類のタスクで行う。タスク分類は、タスク分類のタスクで行う。タスク分類は、タスク分類のタ

- [Where to Attack: A Dynamic Locator Model for Backdoor Attack in Text Classifications](https://aclanthology.org/2022.coling-1.82)
  - Heng-yang Lu, Chenyou Fan, Jun Yang, Cong Hu, Wei Fang, Xiao-jun Wu
  - **TLDR**: テキストベースの学習モデルを攻撃する手法の提案。テキストを入力に置き換える(Pooling)、テキストを入力に置き換える(Discriminator)、テキストを入力に置き換える(Pooling)、テキストを入力に置き換える(Discriminator)、テキストを入力に置き換える(Discriminator)、テキストを入力に置き換える(Discriminator)、テキストを入力に置き換える(Discriminator)、テキストを入力に置き換える(Discriminator)、テキストを入力に置き換える(Discriminator)、テキストを入力に置き換える(Discriminator)、テキストを入力に置き

- [Locally Distributed Activation Vectors for Guided Feature Attribution](https://aclanthology.org/2022.coling-1.83)
  - Housam K. B. Bashier, Mi-Young Kim, Randy Goebel
  - **TLDR**: テキスト分類における、説明文の役割を解釈する研究。説明文の役割は、予測結果の予測結果と異なる箇所を発見するかどうかに依存する。そのため、説明文の役割を解釈する際は、説明文の役割を解釈する箇所を特定する形で学習する。

- [Addressing Leakage in Self-Supervised Contextualized Code Retrieval](https://aclanthology.org/2022.coling-1.84)
  - Johannes Villmow, Viola Campos, Adrian Ulges, Ulrich Schwanecke
  - **TLDR**: コードの検索を、文構造と文構造の異なる箇所に分割する研究。文構造は文構造と同等に扱われるが、文構造は文構造と同等に扱われる。文構造は文構造と同等に扱われるが、文構造は文構造と同等に扱われる。文構造は文構造と同等に扱われるが、文構造は文構造と同等に扱われる。

- [A Domain Knowledge Enhanced Pre-Trained Language Model for Vertical Search: Case Study on Medicinal Products](https://aclanthology.org/2022.coling-1.85)
  - Kesong Liu, Jianhui Jiang, Feifei Lyu
  - **TLDR**: 医療用文書の事前学習を行う研究。事前学習済み言語モデルを、事前学習済み言語モデルの入力に組み込むことで、事前学習済み言語モデルの精度を上げる。事前学習済み言語モデルは、事前学習済み言語モデルの入力と同等の精度を獲得できる。

- [CONCRETE: Improving Cross-lingual Fact-checking with Cross-lingual Retrieval](https://aclanthology.org/2022.coling-1.86)
  - Kung-Hsiang Huang, ChengXiang Zhai, Heng Ji
  - **TLDR**: マルチ言語の事前学習済みモデルを、翻訳文を翻訳した翻訳者に学習させる研究。翻訳文を翻訳した翻訳者に翻訳文の翻訳文を入力し、翻訳文を翻訳した翻訳者に翻訳文を入力する。翻訳文を入力とし、翻訳文を入力とし翻訳文を入力とし翻訳文を入力とし翻訳文を入力とし翻訳文を入力とし翻訳文を入力とし翻訳文を入力とし翻訳文を入力とし翻訳文を入力とし翻訳文を入力とし翻訳文を入力とし翻訳文を入力とし翻訳文を入力とし翻訳文を入力とし翻訳文を

- [E-VarM: Enhanced Variational Word Masks to Improve the Interpretability of Text Classification Models](https://aclanthology.org/2022.coling-1.87)
  - Ling Ge, ChunMing Hu, Guanghui Ma, Junshuang Wu, Junfan Chen, JiHong Liu, Hong Zhang, Wenyi Qin, Richong Zhang
  - **TLDR**: 文書分類モデルの解釈性を向上させる研究。文書の単語分散表現をマルチレベルの表現に置き換え、マルチレベルの表現を入力に追加する。マルチレベルの表現を入力に追加することで、文書分類モデルの解釈性を向上させる。

- [Attribute Injection for Pretrained Language Models: A New Benchmark and an Efficient Method](https://aclanthology.org/2022.coling-1.88)
  - Reinald Kim Amplayo, Kang Min Yoo, Sang-Woo Lee
  - **TLDR**: 自然言語処理モデルの、Attribute 注入手法のベンチマーク。Attribute 注入は、自然言語処理モデルのパフォーマンスを上げるのに有効な手法だが、事前学習済みモデルのパフォーマンスが下がる可能性がある。そのため、事前学習済みモデルのパフォーマンスを上げるための工夫を行なっている。

- [Towards Robust Neural Retrieval with Source Domain Synthetic Pre-Finetuning](https://aclanthology.org/2022.coling-1.89)
  - Revanth Gangi Reddy, Vikas Yadav, Md Arafat Sultan, Martin Franz, Vittorio Castelli, Heng Ji, Avirup Sil
  - **TLDR**: 事前学習済みモデルの学習を、事前学習済みドメインの潜在表現に置き換える研究。潜在表現は、潜在表現の潜在表現を生成するGeneratorで生成する。Generatorは、潜在表現の潜在表現を生成する。生成した潜在表現を潜在表現の潜在表現に置き換えることで、事前学習済みモデルの学習を容易にする。

- [Parameter-Efficient Neural Reranking for Cross-Lingual and Multilingual Retrieval](https://aclanthology.org/2022.coling-1.90)
  - Robert Litschko, Ivan Vulić, Goran Glavaš
  - **TLDR**: マルチ言語対応の翻訳モデルを、事前学習済み言語モデルに置き換える研究。事前学習済み言語モデルは、事前学習済み言語モデルのパラメーターを変更し、パラメーターを変更した言語モデルに置き換える。これにより、事前学習済み言語モデルの精度を上げると同時に、翻訳モデルの学習時間を短縮できる。

- [LIME: Weakly-Supervised Text Classification without Seeds](https://aclanthology.org/2022.coling-1.91)
  - Seongmin Park, Jihwa Lee
  - **TLDR**: 教師なしのテキスト分類を行う際に、Entailmentベースのモデルを用いた研究。Entailmentは単語のEncode結果を基に生成されるが、単語のEncode結果は、EntailmentのEncode結果と同等の結果が得られるか検証している。Entailmentは、Encode結果を基に生成されるが、単語のEncode結果は、Encode結果と同等の結果が得られるか検証している

- [Multi-Stage Framework with Refinement Based Point Set Registration for Unsupervised Bi-Lingual Word Alignment](https://aclanthology.org/2022.coling-1.92)
  - Silviu Vlad Oprea, Sourav Dutta, Haytham Assem
  - **TLDR**: マルチスケールな分散表現を、事前学習済みモデルで学習する研究。事前学習済みモデルは、分散表現の分散表現を、事前学習済みモデルの分散表現に置き換える形で学習する。分散表現は、分散表現の分散表現を、分散表現の分散表現を、分散表現の分散表現に置き換える形で学習する。

- [EM-PERSONA: EMotion-assisted Deep Neural Framework for PERSONAlity Subtyping from Suicide Notes](https://aclanthology.org/2022.coling-1.93)
  - Soumitra Ghosh, Dhirendra Kumar Maurya, Asif Ekbal, Pushpak Bhattacharyya
  - **TLDR**: 自動死亡診断のモデルで、自殺防止のモデルを改良した研究。自動死亡診断モデルは、死亡原因を推定する際、死亡率が高い場合に自動死亡診断を行うモデルと、そうでない場合に自動死亡診断を行うモデルを比較し、自動死亡診断モデルが自動死亡診断精度を上回る結果を出した。

- [Dense Template Retrieval for Customer Support](https://aclanthology.org/2022.coling-1.94)
  - Tiago Mesquita, Bruno Martins, Mariana Almeida
  - **TLDR**: 問い合わせに対する対応を、事前学習済みモデルの学習に置き換える研究。事前学習済みモデルは、事前学習済みモデルの学習済みモデルの学習済みモデルと同等の精度を維持できるが、事前学習済みモデルは学習済みモデルの学習済みモデルに近い精度を維持できない。そのため、事前学習済みモデルの学習済みモデルを学習するモデルを提案

- [Exploring Label Hierarchy in a Generative Way for Hierarchical Text Classification](https://aclanthology.org/2022.coling-1.95)
  - Wei Huang, Chen Liu, Bo Xiao, Yihua Zhao, Zhaoming Pan, Zhimin Zhang, Xinyun Yang, Guiquan Liu
  - **TLDR**: テキスト分類における、階層構造の予測手法の提案。階層構造は階層構造の分類器で、各階層のラベルは各クラスの分類器で予測する。階層構造は階層構造の分類器で、クラス分類器の予測は階層構造の予測器で行う。階層構造は階層構造の分類器で、クラス分類器の予測は階層構造の予測器で行う。

- [MuSeCLIR: A Multiple Senses and Cross-lingual Information Retrieval Dataset](https://aclanthology.org/2022.coling-1.96)
  - Wing Yan Li, Julie Weeds, David Weir
  - **TLDR**: マルチ言語の文書検索を行う際に、事前学習済みモデルの学習率を検証した研究。事前学習済みモデルは翻訳文を入力とする検索結果を生成するが、翻訳文は翻訳済みモデルの検索結果から生成する。事前学習済みモデルは翻訳文を入力とする検索結果を生成するが、翻訳文は翻訳済みモデルの検索結果から生成する。

- [Complicate Then Simplify: A Novel Way to Explore Pre-trained Models for Text Classification](https://aclanthology.org/2022.coling-1.97)
  - Xu Zhang, Zejie Liu, Yanzheng Xiang, Deyu Zhou
  - **TLDR**: テキスト分類を行う際に、事前学習済みモデルの学習を2段階に繰り返す手法を提案。事前学習済みモデルの学習は、事前学習済みモデルの学習データ(BERT)を用い、BERTの学習データ(テキスト分類結果)を用い事前学習を行う。事前学習済みモデルの学習データは、事前学習済みモデルの学習データと同等の精度を達成できる。

- [Adaptive Feature Discrimination and Denoising for Asymmetric Text Matching](https://aclanthology.org/2022.coling-1.98)
  - Yan Li, Chenliang Li, Junjun Guo
  - **TLDR**: マルチタスクで、事前学習済みモデルを用いた事前学習済みモデルの提案。事前学習済みモデルは、事前学習済みモデルの学習済み言語モデルと同等の精度を達成。事前学習済みモデルは、事前学習済み言語モデルの学習済み言語モデルと同等の精度を達成。

- [Rethinking Data Augmentation in Text-to-text Paradigm](https://aclanthology.org/2022.coling-1.99)
  - Yanan Chen, Yang Liu
  - **TLDR**: 機械学習モデルの学習データを強化する研究。学習データは通常のモデルの学習データと同等の扱いが行われるが、この扱いはモデルの性能を上げるためでなく、モデルのパフォーマンスを上げるためにも有効な扱いが行われる。

- [ConTextING: Granting Document-Wise Contextual Embeddings to Graph Neural Networks for Inductive Text Classification](https://aclanthology.org/2022.coling-1.100)
  - Yen-Hao Huang, Yi-Hsin Chen, Yi-Shin Chen
  - **TLDR**: 自然言語処理における、文書内の単語分散表現を学習する研究。文書内の単語分散表現を学習する際、文書内の単語分散表現を学習する。文書内の単語分散表現は、文書全体の単語分散表現と同等の学習が行えることを確認。文書内の単語分散表現は、文書全体の単語分散表現と同等の学習が行えることを確認。

- [Virtual Knowledge Graph Construction for Zero-Shot Domain-Specific Document Retrieval](https://aclanthology.org/2022.coling-1.101)
  - Yeon Seonwoo, Seunghyun Yoon, Franck Dernoncourt, Trung Bui, Alice Oh
  - **TLDR**: 文書分類の手法を、文書分類の文書分類モデルに置き換えた研究。文書分類は文書分類モデルの枠組みで、文書分類は文書分類モデルの枠組みで行う。文書分類は文書分類モデルの枠組みで行うが、文書分類は文書分類モデルの枠組みで行う。文書分類モデルは文書分類モデルと同等の手法を取っている。

- [MICO: Selective Search with Mutual Information Co-training](https://aclanthology.org/2022.coling-1.102)
  - Zhanyu Wang, Xiao Zhang, Hyokun Yun, Choon Hui Teo, Trishul Chilimbi
  - **TLDR**: マルチタスクで行う検索手法の提案。文書を個別に検索するのではなく、文書全体を個別に検索する。文書全体を検索するより、文書全体を検索する方が効率が良い。文書全体を検索するより、文書全体を検索する方が効率が良い。

- [DPTDR: Deep Prompt Tuning for Dense Passage Retrieval](https://aclanthology.org/2022.coling-1.103)
  - Zhengyang Tang, Benyou Wang, Ting Yao
  - **TLDR**: 自然言語処理で、事前学習済みモデルを事前学習する手法の提案。事前学習済みモデルは、事前学習済みモデルの学習済みモデルと同等の性能が得られるが、事前学習済みモデルは学習済みモデルの学習済みモデルと同等の性能が得られない。そのため、事前学習済みモデルを事前学習済みモデルに置き換える手法を提案している。

- [BERT-Flow-VAE: A Weakly-supervised Model for Multi-Label Text Classification](https://aclanthology.org/2022.coling-1.104)
  - Ziwen Liu, Josep Grau-Bove, Scott Allan Allan Orr
  - **TLDR**: マルチラベルの文書分類を行う際に、事前学習済みモデルをフルに活用する研究。事前学習済みモデルは、文書の文書構造をBERTで生成し、BERTの文書構造をVAEで学習する。文書構造はBERTで生成し、文書構造はVAEで学習する。文書構造はVAEで学習する。

- [Welcome to the Modern World of Pronouns: Identity-Inclusive Natural Language Processing beyond Gender](https://aclanthology.org/2022.coling-1.105)
  - Anne Lauscher, Archie Crowley, Dirk Hovy
  - **TLDR**: 自然言語処理における3rd person pronounのモデルについて、その問題点と対策をまとめたサーベイ。3rd personのモデルは、既存の3rd personのモデルと比較して、モデルの適用が難しい単語を含まない単語を含まない単語に置き換えている。また、モデルの適用が不適な単語を含まない単語に置き換えている点も指摘されている。

- [Threat Scenarios and Best Practices to Detect Neural Fake News](https://aclanthology.org/2022.coling-1.106)
  - Artidoro Pagnoni, Martin Graciarena, Yulia Tsvetkov
  - **TLDR**: 自然言語処理で生成されたニュースを検出する際の、ベストな手法をまとめたサーベイ。生成されたニュースは、ニュースの生成時に使われる単語をベースに生成される。生成されたニュースは、ニュースの生成時に使われる単語をベースに生成される。

- [From Polarity to Intensity: Mining Morality from Semantic Space](https://aclanthology.org/2022.coling-1.107)
  - Chunxu Zhao, Pengyuan Liu, Dong Yu
  - **TLDR**: テキストから、人間の行動を評価する研究。テキストは、人間の行動を評価する指標として使われることが多いが、この場合、行動の度合いを評価する指標として使われることが多い。このため、単語レベルの評価だけでなく、文レベルの評価も同時に行う手法を提案している。

- [SOS: Systematic Offensive Stereotyping Bias in Word Embeddings](https://aclanthology.org/2022.coling-1.108)
  - Fatma Elsafoury, Steve R. Wilson, Stamos Katsigiannis, Naeem Ramzan
  - **TLDR**: ネット上の不法攻撃に対し、不法攻撃に対する対応を検証した研究。不法攻撃に対する対応は、不法攻撃に対する対応の分類器(SNS)と同等の分類器(APT)で行われるが、この分類器は、不法攻撃に対する対応を分類器と同等に扱う。不法攻撃に対する対応は、SNSの分類器で行われる。

- [Bigger Data or Fairer Data? Augmenting BERT via Active Sampling for Educational Text Classification](https://aclanthology.org/2022.coling-1.109)
  - Lele Sha, Yuheng Li, Dragan Gasevic, Guanliang Chen
  - **TLDR**: 事前学習済み言語モデル(BERT)が、プライバシーの保護を無視する傾向があるという研究。事前学習済み言語モデルは、プライバシーの保護が無視されると予測が難しい。そのため、事前学習済み言語モデルの予測精度を上げるための工夫を行っている。

- [Debiasing Word Embeddings with Nonlinear Geometry](https://aclanthology.org/2022.coling-1.110)
  - Lu Cheng, Nayoung Kim, Huan Liu
  - **TLDR**: マルチカテゴリのBiasを扱う研究。Biasは単一カテゴリのBiasと同等、同等に扱われるかを検証する。Biasは単一カテゴリのBiasと同等、同等に扱われるかを検証する。Biasは単一カテゴリのBiasと同等、同等に扱われるかを検証する。

- [Debiasing Isn’t Enough! – on the Effectiveness of Debiasing MLMs and Their Social Biases in Downstream Tasks](https://aclanthology.org/2022.coling-1.111)
  - Masahiro Kaneko, Danushka Bollegala, Naoaki Okazaki
  - **TLDR**: マルチタスクモデルの評価について、実装に適した評価指標と実装に適した評価指標の相関がないことを確認した研究。実装では、実装に適した評価指標と実装に適した評価指標の相関がみられないことが確認されている。

- [Quantifying Bias from Decoding Techniques in Natural Language Generation](https://aclanthology.org/2022.coling-1.112)
  - Mayukh Das, Wolf Tilo Balke
  - **TLDR**: 自然言語処理で、自然言語処理のモデルが潜在表現にどのような影響を与えているのかを調べた研究。自然言語処理のモデルは、潜在表現の表現を予測するモデルと、潜在表現の表現を予測するモデルの2つを比較し、どちらが良いかを検証している。

- [A Study of Implicit Bias in Pretrained Language Models against People with Disabilities](https://aclanthology.org/2022.coling-1.113)
  - Pranav Narayanan Venkit, Mukund Srinath, Shomir Wilson
  - **TLDR**: 言語モデルが人種や性別に偏る可能性があることを調査した研究。言語モデルは人種や性別に偏る可能性があることを確認。また、人種や性別に偏る単語は言語モデルの学習に不適であるとしている。

- [Social Norms-Grounded Machine Ethics in Complex Narrative Situation](https://aclanthology.org/2022.coling-1.114)
  - Tao Shen, Xiubo Geng, Daxin Jiang
  - **TLDR**: モデルの学習済みモデルと、モデルの学習済みモデルの学習済みモデルの学習済みモデルを組み合わせたモデルの提案。モデルはモデルの学習済みモデルと、モデルの学習済みモデルの学習済みモデルの学習済みモデルを組み合わせている。モデルはモデルの学習済みモデルと、モデルの学習済みモデルの学習済みモデルの学習済みモデルを組み合わせている。

- [Bias at a Second Glance: A Deep Dive into Bias for German Educational Peer-Review Data Modeling](https://aclanthology.org/2022.coling-1.115)
  - Thiemo Wambsganss, Vinitra Swamy, Roman Rietsche, Tanja Käser
  - **TLDR**: 自然言語処理で、事前学習済みモデルが学習するBiasについて調査した研究。Biasは、学習済みモデルの分類/適用に限界がある場合に発生する。Biasは、学習済みモデルの分類/適用に限界がある場合に発生する。Biasは、学習済みモデルの分類/適用に限界がある場合に発生する。Biasは、学習済みモデルの分類/適用に限界がある場合に発生する。

- [Dynamic Relevance Graph Network for Knowledge-Aware Question Answering](https://aclanthology.org/2022.coling-1.116)
  - Chen Zheng, Parisa Kordjamshidi
  - **TLDR**: 自然言語処理で、質問と回答のノードの関連度をグラフで表現する研究。質問と回答のノードは、ノードの重みを計算するノードの重みと、ノードの重みを計算するノードの重みから構成される。重みは、ノードの重みと重みの重みを比較し、重みは重みの重みから計算する。

- [SISER: Semantic-Infused Selective Graph Reasoning for Fact Verification](https://aclanthology.org/2022.coling-1.117)
  - Eunhwan Park, Jong-Hyeon Lee, Jeon Dong Hyeon, Seonhoon Kim, Inho Kang, Seung-Hoon Na
  - **TLDR**: 自然言語処理の手法を、より人間らしい表現に置き換えた研究。入力は、入力と入力の双方向で行う。入力は、入力と入力双方向の表現を入力とし、入力と入力双方向の表現を入力とし、入力と入力双方向の表現を入力とし、入力と入力双方向の表現を入力とし、入力双方向の表現を入力とし、入力双方向の表現を入力とし、入力双方向の表現を入力とし、入力双方向の表現を入力とし、入力双方向の表現を入力とし、入力双方向の表現を入力とし、入力双方向の表現を入力

- [Answering Numerical Reasoning Questions in Table-Text Hybrid Contents with Graph-based Encoder and Tree-based Decoder](https://aclanthology.org/2022.coling-1.118)
  - Fangyu Lei, Shizhu He, Xiang Li, Jun Zhao, Kang Liu
  - **TLDR**: 自然言語処理で、テーブルとテキストを組み合わせたタスクを提案。タスクは、タスクの入力とタスクの生成を結合する。タスクは、タスクの入力とタスクの生成を結合する。タスクは、タスクの入力とタスクの生成を結合する。タスクは、タスクの生成結果をベースに、タスクの生成結果をベースにタスクを生成する。

- [Perform like an Engine: A Closed-Loop Neural-Symbolic Learning Framework for Knowledge Graph Inference](https://aclanthology.org/2022.coling-1.119)
  - Guanglin Niu, Bo Li, Yongfei Zhang, Shiliang Pu
  - **TLDR**: 構造推論のモデルで、構造推論のモデルを強化する研究。構造推論では、構造の意味を表現する単語を予測するが、単語の意味は表現する単語の意味を表現する単語に置き換える。この表現を、構造の意味を表現する単語に置き換える形で表現する。

- [Table-based Fact Verification with Self-labeled Keypoint Alignment](https://aclanthology.org/2022.coling-1.120)
  - Guangzhen Zhao, Peng Yang
  - **TLDR**: 文書から事実を推定する手法の提案。文書から事実を推定する際、文書の文書特徴と文書特徴を比較し、文書特徴と文書特徴双方を比較した研究。文書特徴と文書特徴双方を比較することで、文書特徴と文書特徴双方を比較する。文書特徴と文書特徴双方を比較することで、文書特徴と文書特徴双方を比較する。

- [IMCI: Integrate Multi-view Contextual Information for Fact Extraction and Verification](https://aclanthology.org/2022.coling-1.121)
  - Hao Wang, Yangguang Li, Zhen Huang, Yong Dou
  - **TLDR**: マルチモーダルの情報から事実抽出を行う研究。文書内の文書タイトルと文書中の文書名を入力とする。文書内の文書タイトルは文書内文書名と文書内文書名の2つを入力とする。文書内文書名は文書内文書名と文書文書名の2つを入力とする。文書内文書名は文書文書名と文書文書文書名の2つを入力とする。文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書

- [Prompt Combines Paraphrase: Teaching Pre-trained Models to Understand Rare Biomedical Words](https://aclanthology.org/2022.coling-1.122)
  - Haochun Wang, Chi Liu, Nuwa Xi, Sendong Zhao, Meizhi Ju, Shiwei Zhang, Ziheng Zhang, Yefeng Zheng, Bing Qin, Ting Liu
  - **TLDR**: 自然言語処理で、事前学習済みモデルの学習を補助する手法の提案。事前学習済みモデルは自然言語処理で使われる単語が少なく、Biomedical termが頻出するケースが多い。そのため、Biomedical termを学習時に使用するPromptを追加することで、Biomedical termを学習するモデルを改善する。

- [Self-Supervised Intermediate Fine-Tuning of Biomedical Language Models for Interpreting Patient Case Descriptions](https://aclanthology.org/2022.coling-1.123)
  - Israa Alghanmi, Luis Espinosa-Anke, Steven Schockaert
  - **TLDR**: 医療文書の文書分類を、事前学習済み言語モデルで行う研究。事前学習済み言語モデルはPubMedの文書分類を学習するが、文書分類の文書分類は医療文書分類を学習する。文書分類は文書分類の分類機で行うが、文書分類は文書分類機で行う。文書分類機は文書分類機の分類機を学習する。

- [Evaluating and Mitigating Inherent Linguistic Bias of African American English through Inference](https://aclanthology.org/2022.coling-1.124)
  - Jamell Dacon, Haochen Liu, Jiliang Tang
  - **TLDR**: 自然言語処理モデルの予測精度を向上させるための研究。モデルは、モデルの言語特性をベースに、モデルの言語特性をベースにモデルの予測精度を向上させるためのモデルを構築している。モデルは、モデルの言語特性をベースに、モデルの予測精度を向上させるためのモデルを構築している。

- [Can We Guide a Multi-Hop Reasoning Language Model to Incrementally Learn at Each Single-Hop?](https://aclanthology.org/2022.coling-1.125)
  - Jesus Lovon-Melgarejo, Jose G. Moreno, Romaric Besançon, Olivier Ferret, Lynda Tamine
  - **TLDR**: マルチステップの言語モデルを学習する際、事前学習済みモデルと同等の手法を取った研究。事前学習済みモデルは、単一ステップの予測を行う際に、予測結果を複数ステップに分割し、各ステップの予測結果を個別に予測する。この時、予測結果を個別に予測する手法を学習データセットに組み込むことで、学習効率を上げている。

- [Modeling Hierarchical Reasoning Chains by Linking Discourse Units and Key Phrases for Reading Comprehension](https://aclanthology.org/2022.coling-1.126)
  - Jialin Chen, Zhuosheng Zhang, Hai Zhao
  - **TLDR**: 文関係と自然言語処理の関係をグラフ化した研究。文関係は文書レベルだけでなく、単語レベルでも扱える。文書レベルの文書は文書構造が複雑なので、文書構造をグラフ化した上で文関係を推論する。文書構造は文書構造と同等か、文書構造は文書構造と同等かを検証している。

- [Hierarchical Representation-based Dynamic Reasoning Network for Biomedical Question Answering](https://aclanthology.org/2022.coling-1.127)
  - Jianguo Mao, Jiyuan Zhang, Zengfeng Zeng, Weihua Peng, Wenbin Jiang, Xiangdong Wang, Hong Liu, Yajuan Lyu
  - **TLDR**: 生物医学の文書から、自然言語処理の文書を生成する研究。文書の文書構造を、文書の文書構造から文書文書の文書構造を生成する形で学習する。文書文書の文書構造は、文書文書の文書構造と文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書文書

- [ArT: All-round Thinker for Unsupervised Commonsense Question Answering](https://aclanthology.org/2022.coling-1.128)
  - Jiawei Wang, Hai Zhao
  - **TLDR**: 事前学習済み言語モデル(PLM)を、事前学習済み知識ベース(KBS)に置き換えた研究。質問文を「質問」と「質問」の2つに分割し、質問文の関連度を予測する。質問文の関連度は、質問文の文脈から予測する。質問文の関連度は、質問文の文脈から予測する。

- [Teaching Neural Module Networks to Do Arithmetic](https://aclanthology.org/2022.coling-1.129)
  - Jiayi Chen, Xiao-Yu Guo, Yuan-Fang Li, Gholamreza Haffari
  - **TLDR**: マルチステップのマルチタスクで、言語モデルの学習を強化する研究。マルチタスクのタスクは、入力と質問のペアを入力に置き換える形で行う。入力は、入力と質問のペアを入力に置き換える形で行う。入力は、入力と質問のペアを入力に置き換える形で行う。

- [An Augmented Benchmark Dataset for Geometric Question Answering through Dual Parallel Text Encoding](https://aclanthology.org/2022.coling-1.130)
  - Jie Cao, Jing Xiao
  - **TLDR**: 自然言語処理で、自然言語処理のモデルを学習する研究。モデルは、自然言語処理のモデルと同等のモデルを学習する。モデルは、自然言語処理のモデルと同等のモデルを学習する。モデルは、自然言語処理のモデルと同等のモデルを学習する。モデルは、自然言語処理のモデルと同等のモデルを学習する。

- [Competence-based Question Generation](https://aclanthology.org/2022.coling-1.131)
  - Jingxuan Tu, Kyeongmin Rim, James Pustejovsky
  - **TLDR**: 自然言語処理で、質問回答をモデルのパフォーマンスに応用した研究。質問は、質問と質問の関連度を調べる質問(CNN)、質問は、質問と質問の関連度を調べる質問(CNN)、質問は、質問と質問の関連度を調べる質問(CNN)、質問は、質問と質問の関連度を調べる質問(CNN)、質問は、質問と質問の関連度を調べる質問(CNN)、質問は、質問と質問の関連度を調べる質問(CNN)、質問は、質問と質問の関連度を調べる質問(CNN)、質問は、質問

- [Coalescing Global and Local Information for Procedural Text Understanding](https://aclanthology.org/2022.coling-1.132)
  - Kaixin Ma, Filip Ilievski, Jonathan Francis, Eric Nyberg, Alessandro Oltramari
  - **TLDR**: 事前学習済みモデルを、グローバル・グローバルの情報から学習する研究。事前学習済みモデルは、事前学習済みモデルの入力と、グローバルの情報から学習済みモデルの入力を統合し、予測結果を統合したモデルを生成する。事前学習済みモデルは、事前学習済みモデルの予測結果を統合したモデルで学習する。

- [Original Content Is All You Need! an Empirical Study on Leveraging Answer Summary for WikiHowQA Answer Selection Task](https://aclanthology.org/2022.coling-1.133)
  - Liang Wen, Juan Li, Houfeng Wang, Yingwei Luo, Xiaolin Wang, Xiaodong Zhang, Zhicong Cheng, Dawei Yin
  - **TLDR**: 質問回答の要約をWikiHowQAで行う試み。要約は質問回答の要約と同等の役割を果たすが、要約は要約の要約と同等の役割を果たす。要約は要約の要約と同等の役割を果たすが、要約は要約の要約と同等の役割を果たす。要約は要約の要約と同等の役割を果たす。

- [Case-Based Abductive Natural Language Inference](https://aclanthology.org/2022.coling-1.134)
  - Marco Valentino, Mokanarangan Thayaparan, André Freitas
  - **TLDR**: 多段の自然言語処理で、事前学習済みモデルを転移する研究。事前学習済みモデルは、事前学習済みモデルの説明を転移する。転移は、事前学習済みモデルの説明を転移先の説明に置き換える形で行う。転移先の説明は、転移先の説明と同等の意味を持つものになる。

- [Semantic Structure Based Query Graph Prediction for Question Answering over Knowledge Graph](https://aclanthology.org/2022.coling-1.135)
  - Mingchen Li, Shihao Ji
  - **TLDR**: 自然言語処理で、質問文の構造を予測する研究。質問文の構造は、文の意味を含まない単語を含まない単語に置き換えるBERTをベースにしている。BERTは、質問文の構造を予測するモデルで、BERTは、質問文の構造を予測するモデルで、BERTは、質問文の構造を予測するモデルで、BERTは、BERTは、BERTは、BERTは、BERTは、BERTは、BERTは、BERTは、BERTは、BERTは、BERTは、BERTは、BERTは、BERTは、BERTは、BER

- [Repo4QA: Answering Coding Questions via Dense Retrieval on GitHub Repositories](https://aclanthology.org/2022.coling-1.136)
  - Minyu Chen, Guoqiang Li, Chen Ma, Jingyang Li, Hongfei Fu
  - **TLDR**: オープンソースのソフトウェアの開発で、質問文とソース文を同時に学習する研究。質問文は、ソース文とソース文の類似度を調べるためのデータセットをベースに、ソース文はソース文の類似度を調べるためのデータセットをベースに学習する。ソース文は、ソース文の類似度を調べるためのデータセットをベースに学習する。

- [Addressing Limitations of Encoder-Decoder Based Approach to Text-to-SQL](https://aclanthology.org/2022.coling-1.137)
  - Octavian Popescu, Irene Manotas, Ngoc Phuoc An Vo, Hangu Yeo, Elahe Khorashani, Vadim Sheinin
  - **TLDR**: テキストからSQLを生成するタスクで、事前学習済みモデルのパフォーマンスが大幅に低下する問題を解決した研究。事前学習済みモデルは、SQLのEncoderDecoderで生成したデータセットをベースに、SQLのEncoderDecoderで生成したデータセットをベースに学習する。SQLのEncoderDecoderは、事前学習済みモデルのパフォーマンスを上げるための仕組みを組み込んだモデル。

- [Mintaka: A Complex, Natural, and Multilingual Dataset for End-to-End Question Answering](https://aclanthology.org/2022.coling-1.138)
  - Priyanka Sen, Alham Fikri Aji, Amir Saffari
  - **TLDR**: 質問回答モデルの提案。質問回答は、質問文を入力とする。質問文は、質問文の意味を含まない、質問文の意味を含まない、質問文の意味を含まない、質問文の意味を含まない、といった形で構成されている。質問文は、質問文の意味を含まない、という形で構成されている。

- [Can Edge Probing Tests Reveal Linguistic Knowledge in QA Models?](https://aclanthology.org/2022.coling-1.139)
  - Sagnik Ray Choudhury, Nikita Bhutani, Isabelle Augenstein
  - **TLDR**: 事前学習済み言語モデル(LM)の学習結果が、モデルの更新に影響するのか検証した研究。学習済みモデルは更新しても更新データに影響しないが、更新データに影響するデータセットは複数ある。更新データは複数あるが、更新データの更新は1回のみ。

- [Conversational QA Dataset Generation with Answer Revision](https://aclanthology.org/2022.coling-1.140)
  - Seonjeong Hwang, Gary Geunbae Lee
  - **TLDR**: 対話システムで質問回答を行う際に、質問文から抽出された単語をベースに質問回答を行う研究。単語の抽出は、単語の意味を予測する形で行う。単語の意味を予測する単語は、単語の意味を予測する単語と、単語の意味を予測する単語の2つに分けている。単語の意味を予測する単語は、単語の意味を予測する単語と、単語の意味を予測する単語の2つに分けている。

- [DABERT: Dual Attention Enhanced BERT for Semantic Matching](https://aclanthology.org/2022.coling-1.141)
  - Sirui Wang, Di Liang, Jian Song, Yuntao Li, Wei Wu
  - **TLDR**: 事前学習済み言語モデル(BERT)で、文の異なる特徴を学習する研究。学習済み言語モデルは、文の異なる特徴を学習する際は、文の単語分散表現を学習する。この分散表現を、文の単語分散表現と同等精度で学習する。

- [Locate Then Ask: Interpretable Stepwise Reasoning for Multi-hop Question Answering](https://aclanthology.org/2022.coling-1.142)
  - Siyuan Wang, Zhongyu Wei, Zhihao Fan, Qi Zhang, Xuanjing Huang
  - **TLDR**: マルチホップの要約を行う際に、各ステップごとに要約を行う手法を提案。要約は文のidentificationと、文の推論は各ステップごとに行う。要約は、要約の文を生成する最初のステップから生成する。要約は、要約の文を生成する最初のステップから生成する。

- [Less Is Better: Recovering Intended-Feature Subspace to Robustify NLU Models](https://aclanthology.org/2022.coling-1.143)
  - Ting Wu, Tao Gui
  - **TLDR**: モデルのBiasを防ぎつつ、モデルのパフォーマンスを向上させる研究。Biasは、データセットの特性を考慮する必要があり、その特性を考慮するモデルを構築する必要があった。そこで、Biasを考慮するモデルを構築する手法を提案。Biasは、データセットの特性を考慮する必要があり、その特性を考慮するモデルを構築する。

- [CORN: Co-Reasoning Network for Commonsense Question Answering](https://aclanthology.org/2022.coling-1.144)
  - Xin Guan, Biwei Cao, Qingqing Gao, Zheng Yin, Bo Liu, Jiuxin Cao
  - **TLDR**: 文書から文書を推論する研究。文書の文書構造を文書構造と文書構造の2つに分割し、文書構造を文書構造と文書構造の2つに分割する。文書構造は文書構造と文書構造の2つに分割し、文書構造は文書構造と文書構造の2つに分割する。文書構造は文書構造と文書構造の2つに分割し、文書構造は文書構造と文書構造の2つに分割する。文書構造は文書構造と文書構造の2つに分割する。

- [Logical Form Generation via Multi-task Learning for Complex Question Answering over Knowledge Bases](https://aclanthology.org/2022.coling-1.145)
  - Xixin Hu, Xuan Wu, Yiheng Shu, Yuzhong Qu
  - **TLDR**: 自然言語処理で、事前学習済みモデルをモデル化する研究。事前学習済みモデルは、事前学習済みモデルの入力情報(単語/文など)を学習する。入力情報の学習は、事前学習済みモデルの入力情報(単語/文など)を学習する形で行う。事前学習済みモデルは、事前学習済みモデルの入力情報(単語/文など)を学習する。

- [CMQA: A Dataset of Conditional Question Answering with Multiple-Span Answers](https://aclanthology.org/2022.coling-1.146)
  - Yiming Ju, Weikang Wang, Yuanzhe Zhang, Suncong Zheng, Kang Liu, Jun Zhao
  - **TLDR**: マルチスパンのQAモデルを構築する際、マルチスパンの質問を想定する研究。マルチスパンの質問は、質問文の意味が異なる場合に回答するが、意味が異なる場合に回答する。マルチスパンの質問は、質問文の意味が異なる場合に回答する。マルチスパンの質問は、質問文の意味が異なる場合に回答する。

- [To What Extent Do Natural Language Understanding Datasets Correlate to Logical Reasoning? A Method for Diagnosing Logical Reasoning.](https://aclanthology.org/2022.coling-1.147)
  - Yitian Li, Jidong Tian, Wenqing Chen, Caoyun Fan, Hao He, Yaohui Jin
  - **TLDR**: 自然言語処理で、モデルが学習するタスクとモデルの学習データセットとの相関を調べた研究。タスクの分類は、タスクの分類結果と学習データセットの分類結果を比較し、タスクの分類結果と比較した結果を比較し、タスクの分類結果と比較した結果を比較し、タスクの分類結果と比較した結果を比較し、タスク分類結果と比較した結果を比較し、タスク分類結果と比較した結果を比較し、タスク分類結果と比較した結果を比較し、タスク分類結果と比較した結果を比較し、タスク分類結果と比較した結果を比較し、タ

- [ArcaneQA: Dynamic Program Induction and Contextualized Encoding for Knowledge Base Question Answering](https://aclanthology.org/2022.coling-1.148)
  - Yu Gu, Yu Su
  - **TLDR**: 知識ベースのQAモデルを、構造変換と検索空間の2つに分けたモデルに置き換えた研究。検索空間は検索モデルの重みで表現されるが、検索空間は検索モデルの重みで表現される。そのため、構造変換の重みを学習するモデルを導入している。

- [Unsupervised Question Answering via Answer Diversifying](https://aclanthology.org/2022.coling-1.149)
  - Yuxiang Nie, Heyan Huang, Zewen Chi, Xian-Ling Mao
  - **TLDR**: 教師なしQAで、複数語を扱う手法の提案。複数語の単語を入力とするデータセットを生成する際、単語の長さは単語の長さと同等になるよう調整する。また、単語の長さは単語の長さと同等になるよう、単語長を入力とするデータセットを生成する。

- [Weakly Supervised Formula Learner for Solving Mathematical Problems](https://aclanthology.org/2022.coling-1.150)
  - Yuxuan Wu, Hideki Nakayama
  - **TLDR**: 自然言語処理で、質問回答タスクで最も重要な、自然言語処理の基本となる、既存のモデルが学習できなかった要因を解消するために、モデルの学習方法を工夫した研究。質問回答タスクで、質問回答の要因を予測するモデルを学習する。要因予測は、質問回答の要因を予測するモデルの学習に使用する。

- [Reducing Spurious Correlations for Answer Selection by Feature Decorrelation and Language Debiasing](https://aclanthology.org/2022.coling-1.151)
  - Zeyi Zhong, Min Yang, Ruifeng Xu
  - **TLDR**: 事前学習済みモデルの精度を上げるための研究。事前学習済みモデルの精度を上げるための手法として、事前学習済みモデルのモデル依存度を減らす手法を提案している。事前学習済みモデルの依存度を減らすと、モデルの挙動を学習するモデルを導入する。事前学習済みモデルの挙動を学習する際は、事前学習済みモデルの挙動を学習する。

- [Understanding and Improving Zero-shot Multi-hop Reasoning in Generative Question Answering](https://aclanthology.org/2022.coling-1.152)
  - Zhengbao Jiang, Jun Araki, Haibo Ding, Graham Neubig
  - **TLDR**: 多段の質問を生成するモデルの学習方法について、学習率を上げるためには単一の質問を複数回に分割する手法を導入する必要があると提唱。単一の質問を複数回に分割する手法は、単一の質問に対する学習率を上げるために有効な手法だが、単一の質問に対する学習率を上げるためには、単一の質問に対する学習率を上げるためには有効な手法ではないかとしている。

- [Domain Adaptation for Question Answering via Question Classification](https://aclanthology.org/2022.coling-1.153)
  - Zhenrui Yue, Huimin Zeng, Ziyi Kou, Lanyu Shang, Dong Wang
  - **TLDR**: ドメイン転移で、質問分類を学習する研究。質問分類は、質問分類器の学習時に、質問分類器のクラス分類を学習する。クラス分類器は、質問分類器の学習時に、クラス分類器のクラス分類を学習する。クラス分類器は、クラス分類器の学習時に、クラス分類器の学習時に学習する。

- [Prompt-based Conservation Learning for Multi-hop Question Answering](https://aclanthology.org/2022.coling-1.154)
  - Zhenyun Deng, Yonghua Zhu, Yang Chen, Qianqian Qi, Michael Witbrock, Patricia Riddle
  - **TLDR**: マルチホップのQAで、事前学習済みモデルを強化学習に活用する研究。事前学習済みモデルは、質問に対する回答を予測するタスクを想定しており、タスクの予測結果を予測するタスクに絞り込んでいる。タスクの予測結果は、タスクの予測結果と異なる場合に予測結果が変わる可能性があることを確認。

- [GLAF: Global-to-Local Aggregation and Fission Network for Semantic Level Fact Verification](https://aclanthology.org/2022.coling-1.155)
  - Zhiyuan Ma, Jianjun Li, Guohui Li, Yongjing Cheng
  - **TLDR**: 事実の推論を行う際に、文書中の単語をグラフに分割し、グラフの構造をグラフ構造とし、グラフ構造をグラフ構造ととるGLAFを提案。文書中の単語をグラフ構造とし、グラフ構造をグラフ構造ととるGLAFを提案。文書中の単語をグラフ構造とし、グラフ構造をグラフ構造ととるGLAFを提案。

- [Exploiting Hybrid Semantics of Relation Paths for Multi-hop Question Answering over Knowledge Graphs](https://aclanthology.org/2022.coling-1.156)
  - Zile Qiao, Wei Ye, Tong Zhang, Tong Mo, Weiping Li, Shikun Zhang
  - **TLDR**: 自然言語処理で、複数回の質問に対する解釈性を上げるために、文中の単語と文中の単語の関連度を予測する研究。文中の単語は単語の関連度が低い場合に意味が異なるとされるが、文中の単語は意味が異なる場合に意味が異なるとされる。文中の単語は、文中の単語と同等かそれ以上の意味を持つとしている。

- [Adaptive Threshold Selective Self-Attention for Chinese NER](https://aclanthology.org/2022.coling-1.157)
  - Biao Hu, Zhen Huang, Minghao Hu, Ziwen Zhang, Yong Dou
  - **TLDR**: 自然言語処理におけるAttentionの仕組みを、Attentionのスコアを自動調整するATSSAに置き換えた研究。Attentionのスコアは、Attentionのスコアを入力とするqueryに入力する。入力は、Attentionのスコアが高い場合のみ入力する。

- [Cluster-aware Pseudo-Labeling for Supervised Open Relation Extraction](https://aclanthology.org/2022.coling-1.158)
  - Bin Duan, Shusen Wang, Xingxian Liu, Yajing Xu
  - **TLDR**: 事前学習済みモデルを、新しい関係に適用する研究。事前学習済みモデルは、事前学習済みモデルの予測結果を基にモデルを作成し、予測結果を基にモデルを学習する。予測結果は、事前学習済みモデルの予測結果と同等の精度を達成。

- [Few-shot Named Entity Recognition with Entity-level Prototypical Network Enhanced by Dispersedly Distributed Prototypes](https://aclanthology.org/2022.coling-1.159)
  - Bin Ji, Shasha Li, Shaoduo Gan, Jie Yu, Jun Ma, Huijun Liu, Jing Yang
  - **TLDR**: 固有表現認識のモデルを、Entity-levelのモデルに置き換えた研究。Entity-levelのモデルは、Entityのクラス分類を学習する際、クラス分類のクラスタリングを学習する。クラスタリングは、クラスタリングのクラスタリングを学習する際の学習方法として使用している。

- [Different Data, Different Modalities! Reinforced Data Splitting for Effective Multimodal Information Extraction from Social Media Posts](https://aclanthology.org/2022.coling-1.160)
  - Bo Xu, Shizhou Huang, Ming Du, Hongya Wang, Hui Song, Chaofeng Sha, Yanghua Xiao
  - **TLDR**: マルチモーダルの情報抽出を行う際に、画像を複数回転させる手法を提案。画像を複数回転させると、画像のサイズが小さくなる一方、画像のサイズが大きい場合にデータの分散が難しくなる。そのため、画像のサイズを大きくする画像分割器を導入し、画像のサイズを小さくするモデルを導入している。

- [Augmentation, Retrieval, Generation: Event Sequence Prediction with a Three-Stage Sequence-to-Sequence Approach](https://aclanthology.org/2022.coling-1.161)
  - Bo Zhou, Chenhao Wang, Yubo Chen, Kang Liu, Jun Zhao, Jiexin Xu, Xiaojian Jiang, Qiuxia Li
  - **TLDR**: 自然言語処理で、ターゲットの行動を予測するタスクを学習する際のタスクとして、ターゲットの行動を予測するタスクを挙げている。タスクは、ターゲットの行動を予測するタスクで、ターゲットの行動を予測するタスクを学習する。タスクは、ターゲットの行動を予測するタスクで、タスクの学習は、タスクの学習に使用するタスクを学習するタスクに分類している。タスクは、タスクの学習に使用するタスクを学習するタスクと、タスクの学習に使用するタスクの2つに分けられている。タスクは

- [Generating Temporally-ordered Event Sequences via Event Optimal Transport](https://aclanthology.org/2022.coling-1.162)
  - Bo Zhou, Yubo Chen, Kang Liu, Jun Zhao, Jiexin Xu, Xiaojian Jiang, Qiuxia Li
  - **TLDR**: 自然言語処理におけるイベント順序の自動生成について、事前学習済みモデルを導入した研究。事前学習済みモデルは、イベントの順序を予測するモデルで、事前学習済みモデルはイベントの順序を予測するモデルで、事前学習済みモデルはイベントの順序を予測するモデルで学習する。事前学習済みモデルは、事前学習済みモデルと同等の精度を維持できる。

- [Improving Continual Relation Extraction through Prototypical Contrastive Learning](https://aclanthology.org/2022.coling-1.163)
  - Chengwei Hu, Deqing Yang, Haoliang Jin, Zhen Chen, Yanghua Xiao
  - **TLDR**: Continuous Relation Extractionの手法を、CNNと組み合わせて強化学習に応用した研究。CNNはクラス分類機をベースに、クラス分類機の学習はCNNの学習に使用する。CNNはクラス分類機の学習に使用するが、CNNはクラス分類機の学習に使用する。CNNはクラス分類機の学習に使用するが、クラス分類機の学習はCNNの学習に使用する。

- [Prompt-based Text Entailment for Low-Resource Named Entity Recognition](https://aclanthology.org/2022.coling-1.164)
  - Dongfang Li, Baotian Hu, Qingcai Chen
  - **TLDR**: 自然言語処理で、文中の単語をEntity wiseで予測する研究。Entity wiseは単語の重みを入力とする入力と、単語の重みを入力とする入力の2つを入力とする入力とで、入力と入力の重みを入力とする入力を交互に入力する。入力と入力の重みを入力とする入力を交互に入力する形で学習を行う。

- [Key Mention Pairs Guided Document-Level Relation Extraction](https://aclanthology.org/2022.coling-1.165)
  - Feng Jiang, Jianwei Niu, Shasha Mo, Shengda Fan
  - **TLDR**: 文書中の文関係を抽出する研究。文関係は文のタイトルと、文関係の文長と、文長の文長が同じか、文長の文長が同じか、文長の文長が同じか、文長の文長が同じか、文長の文長が同じか、文長の文長が同じか、文長の文長が同じか、文長の文長が同じか、文長の文長が同じか、文長の文長が同じか、文長の文長が同じか、文長の文長が同じか、文長の文長が同じか、文長の文長

- [A Hybrid Model of Classification and Generation for Spatial Relation Extraction](https://aclanthology.org/2022.coling-1.166)
  - Feng Wang, Peifeng Li, Qiaoming Zhu
  - **TLDR**: 自然言語処理で、自然言語処理のタスクとして、自然言語処理のモデルを用いた研究。自然言語処理のタスクとして、自然言語処理のモデルを用いた研究。自然言語処理のタスクとして、自然言語処理のモデルを用いた研究。自然言語処理のタスクとして、自然言語処理のモデルを用いた研究。自然言語処理のタスクとして、自然言語処理のモデルを用いた研究。

- [Mining Health-related Cause-Effect Statements with High Precision at Large Scale](https://aclanthology.org/2022.coling-1.167)
  - Ferdinand Schlatt, Dieter Bettin, Matthias Hagen, Benno Stein, Martin Potthast
  - **TLDR**: 医療関連の文書について、文関係の単語を予測する研究。文関係の単語は、文関係の単語と同等の意味を持つ単語を含まない単語と、文関係の単語は、文関係の単語と同等の意味を持つ単語を含まない単語を含まない単語とを区別する。文関係の単語は、文関係の単語と同等の意味を持つ単語を含まない単語とを区別する。

- [Find the Funding: Entity Linking with Incomplete Funding Knowledge Bases](https://aclanthology.org/2022.coling-1.168)
  - Gizem Aydin, Seyed Amin Tabatabaei, George Tsatsaronis, Faegheh Hasibi
  - **TLDR**: 研究論文から、研究の報酬に関する情報を抽出する研究。報酬は、研究の成果を評価する指標として使われることが多いが、報酬の分布は、研究の成果を評価する指標として使われることが多い。報酬分布を抽出するTransformerベースのモデルを、事前学習済みモデルと比較して精度を上げる手法として提案している。

- [KiPT: Knowledge-injected Prompt Tuning for Event Detection](https://aclanthology.org/2022.coling-1.169)
  - Haochen Li, Tong Mo, Hongcheng Fan, Jingkun Wang, Jiaxi Wang, Fuhao Zhang, Weiping Li
  - **TLDR**: 事前学習済みモデルを、事前学習済みモデルの学習に応用した研究。事前学習済みモデルは事前学習済みモデルの学習に使用する事前学習済みモデルの学習を行わず、事前学習済みモデルの学習を行わず事前学習済みモデルの学習を行なっている。事前学習済みモデルは事前学習済みモデルと同等の精度を維持できるが、事前学習済みモデルは精度が低下する。

- [OneEE: A One-Stage Framework for Fast Overlapping and Nested Event Extraction](https://aclanthology.org/2022.coling-1.170)
  - Hu Cao, Jingye Li, Fangfang Su, Fei Li, Hao Fei, Shengqiong Wu, Bobo Li, Liang Zhao, Donghong Ji
  - **TLDR**: 事前学習済みモデルを、事前学習済みモデルと同等の精度で解く手法の提案。事前学習済みモデルは、事前学習済みモデルと同等の精度を維持できるが、事前学習済みモデルは事前学習済みモデルと同等の精度を維持できない。事前学習済みモデルは事前学習済みモデルと同等の精度を維持できるが、事前学習済みモデルは事前学習済みモデルと同等の精度を維持できない。

- [Joint Language Semantic and Structure Embedding for Knowledge Graph Completion](https://aclanthology.org/2022.coling-1.171)
  - Jianhao Shen, Chenguang Wang, Linyuan Gong, Dawn Song
  - **TLDR**: 知識グラフの構造を言語モデルで学習する研究。事前学習済み言語モデルを学習する際、構造を学習する際の確率分布を予測する。構造分布は、学習済み言語モデルの予測結果から予測する。構造分布は、学習済み言語モデルの予測結果から予測する。構造分布は、学習済み言語モデルの予測結果から予測する。

- [Event Detection with Dual Relational Graph Attention Networks](https://aclanthology.org/2022.coling-1.172)
  - Jiaxin Mi, Po Hu, Peng Li
  - **TLDR**: 事前学習済みモデルを、事前学習済みモデルと同等の手法で学習する研究。事前学習済みモデルは、事前学習済みモデルのAttentionを上回る精度を達成。事前学習済みモデルは、事前学習済みモデルのAttentionを上回る精度を達成。事前学習済みモデルは、事前学習済みモデルのAttentionを上回る精度を達成。

- [A Multi-Format Transfer Learning Model for Event Argument Extraction via Variational Information Bottleneck](https://aclanthology.org/2022.coling-1.173)
  - Jie Zhou, Qi Zhang, Qin Chen, Qi Zhang, Liang He, Xuanjing Huang
  - **TLDR**: 自然言語処理で、文書から文の意味を抽出する手法の提案。文書の意味を抽出するモデルは、文書の文書構造を学習するモデルと、文書の文書構造を学習するモデルの2つを提案している。文書の文書構造を学習するモデルは、文書の文書構造を学習するモデルと、文書の文書構造を学習する文書構造を学習する文書構造を組み合わせている。

- [RSGT: Relational Structure Guided Temporal Relation Extraction](https://aclanthology.org/2022.coling-1.174)
  - Jie Zhou, Shenpo Dong, Hongkui Tu, Xiaodong Wang, Yong Dou
  - **TLDR**: 自然言語処理における、時系列の関係を抽出する研究。時系列の関係は、文法・単語・文法関係の3つから構成される。文法・単語・文関係の3つから抽出する際、文法・単語・単語関係の3つを抽出する。文法・単語・単語関係の3つから抽出する際、文法・単語関係の3つを抽出する。

- [Learning Hierarchy-Aware Quaternion Knowledge Graph Embeddings with Representing Relations as 3D Rotations](https://aclanthology.org/2022.coling-1.175)
  - Jinfa Yang, Xianghua Ying, Yongjie Shi, Xin Tong, Ruibin Wang, Taiyan Chen, Bowei Xing
  - **TLDR**: 知識グラフにおける、勾配の分布を推定する研究。勾配は3次元空間の空間を2次元空間に分割し、その空間を2次元空間の空間に分割する。この空間を2次元空間と3次元空間の2次元空間に分割し、空間の2次元空間を2次元空間と3次元空間の2次元空間に分割する。

- [Two Languages Are Better than One: Bilingual Enhancement for Chinese Named Entity Recognition](https://aclanthology.org/2022.coling-1.176)
  - Jinzhong Ning, Zhihao Yang, Zhizheng Wang, Yuanyuan Sun, Hongfei Lin, Jian Wang
  - **TLDR**: 自然言語処理で、翻訳モデルをBilingualに強化する研究。翻訳モデルは、翻訳文を翻訳する際、翻訳文の翻訳文を翻訳する(翻訳文は翻訳文の翻訳文を翻訳する)。翻訳文は翻訳文の翻訳文を翻訳する形で生成する。翻訳文は翻訳文の翻訳文を翻訳する形で生成する。

- [Read Extensively, Focus Smartly: A Cross-document Semantic Enhancement Method for Visual Documents NER](https://aclanthology.org/2022.coling-1.177)
  - Jun Zhao, Xin Zhao, WenYu Zhan, Tao Gui, Qi Zhang, Liang Qiao, Zhanzhan Cheng, Shiliang Pu
  - **TLDR**: 文書の情報をマルチモーダルで学習する手法の提案。マルチモーダルの情報と事前学習済みモデルを組み合わせて学習する。マルチモーダルの情報と事前学習済みモデルを組み合わせて学習する。マルチモーダルの情報と事前学習済みモデルを組み合わせて学習する。

- [STAD: Self-Training with Ambiguous Data for Low-Resource Relation Extraction](https://aclanthology.org/2022.coling-1.178)
  - Junjie Yu, Xing Wang, Jiangjiang Zhao, Chunjie Yang, Wenliang Chen
  - **TLDR**: 教師モデルの学習済みモデルで、教師モデルの学習済みモデルと同等の精度を達成した研究。教師モデルは、学習済みモデルの学習済みモデルと同等の精度を達成した。教師モデルは、学習済みモデルの学習済みモデルと同等の精度を達成した。

- [Flat Multi-modal Interaction Transformer for Named Entity Recognition](https://aclanthology.org/2022.coling-1.179)
  - Junyu Lu, Dixiang Zhang, Jiaxing Zhang, Pingjian Zhang
  - **TLDR**: マルチモーダルで画像認識を行う際に、画像の位置を予測するTransformerを提案。画像の位置を予測するTransformerは、画像の位置を予測するモデルと同等の精度を維持できるが、画像の位置を予測するモデルは精度が低下する。そこで、画像の位置を予測するモデルを提案。

- [MetaSLRCL: A Self-Adaptive Learning Rate and Curriculum Learning Based Framework for Few-Shot Text Classification](https://aclanthology.org/2022.coling-1.180)
  - Kailin Zhao, Xiaolong Jin, Saiping Guan, Jiafeng Guo, Xueqi Cheng
  - **TLDR**: 教師なし学習の手法で、学習率を変更する手法の提案。学習率は学習率の重みを上回るが、学習率を変更する際は重みを上回る値を学習させる。重みは学習率の重みと同等の重みを学習させる。学習率は学習率の重みを上回る値を学習させる。

- [A Simple Temporal Information Matching Mechanism for Entity Alignment between Temporal Knowledge Graphs](https://aclanthology.org/2022.coling-1.181)
  - Li Cai, Xin Mao, Meirong Ma, Hao Yuan, Jianchao Zhu, Man Lan
  - **TLDR**: 自然言語処理における、時系列の情報と時系列の情報との組み合わせを学習する研究。時系列の情報と時系列の情報との組み合わせは、時系列の情報と時系列の情報との組み合わせを学習する際の重みを大きくする。時系列の情報と時系列の情報との組み合わせは、時系列の情報と時系列の情報との組み合わせを学習する際の重みを大きくする。

- [DCT-Centered Temporal Relation Extraction](https://aclanthology.org/2022.coling-1.182)
  - Liang Wang, Peifeng Li, Sheng Xu
  - **TLDR**: 文書作成時における、イベントと文書の位置関係を抽出する研究。文書作成時におけるイベントと文書の位置関係を抽出するモデルを、文書作成時における文書作成時におけるイベントと位置関係を抽出するモデルと、文書作成時における文書作成時における文書作成時における文書作成時における文書作成時における文書作成時における文書作成時における文書作成時における文書作成時における文書作成時における文書作成時における文書作成時における文書作成時における文書作成時における文書作成時における文書作成時における文書作成時における文書作成時における文書作成時における文書作成時における文書作成時における文書作成時における文書作成時における文書作成

- [Document-level Biomedical Relation Extraction Based on Multi-Dimensional Fusion Information and Multi-Granularity Logical Reasoning](https://aclanthology.org/2022.coling-1.183)
  - Lishuang Li, Ruiyuan Lian, Hongbin Lu, Jingyao Tang
  - **TLDR**: 自然言語処理で、文書から自然言語処理のモデルを生成する研究。文書から自然言語処理のモデルを生成する際、文書の文書情報と文書の文書情報の組み合わせを推定する。文書情報と文書情報の組み合わせは、文書情報の組み合わせと同等の精度を達成できる。

- [Simple Yet Powerful: An Overlooked Architecture for Nested Named Entity Recognition](https://aclanthology.org/2022.coling-1.184)
  - Matias Rojas, Felipe Bravo-Marquez, Jocelyn Dunstan
  - **TLDR**: 自然言語処理で、複数の言語モデルを組み合わせたモデルの提案。各言語モデルは、Entityの分類を学習するモデルと、Entityの分類を学習するモデルの2つを組み合わせている。Entity分類は、Entityの分類を学習するモデルが最も有効なモデルといえる。

- [ERGO: Event Relational Graph Transformer for Document-level Event Causality Identification](https://aclanthology.org/2022.coling-1.185)
  - Meiqi Chen, Yixin Cao, Kunquan Deng, Mukai Li, Kun Wang, Jing Shao, Yan Zhang
  - **TLDR**: 文書中のイベントについて、事前学習済みモデルを用いた研究。事前学習済みモデルは、イベントのクラスタリングを行わず、イベントのクラスタリング結果を基に文書を生成する。クラスタリング結果は、イベントのクラスタリング結果と同等の結果が得られることを確認。

- [DRK: Discriminative Rule-based Knowledge for Relieving Prediction Confusions in Few-shot Relation Extraction](https://aclanthology.org/2022.coling-1.186)
  - Mengru Wang, Jianming Zheng, Fei Cai, Taihua Shao, Honghui Chen
  - **TLDR**: 教師なし学習で、文中の単語を予測する手法の提案。文中の単語を予測する際、文中の単語を予測する際のルールを決める。文中の単語を予測する際は、文中の単語を予測する単語の予測枠組みを導入する。文中の単語を予測する際は、文中の単語を予測する枠組みを導入する。

- [DocQueryNet: Value Retrieval with Arbitrary Queries for Form-like Documents](https://aclanthology.org/2022.coling-1.187)
  - Mingfei Gao, Le Xue, Chetan Ramaiah, Chen Xing, Ran Xu, Caiming Xiong
  - **TLDR**: 文書構造を言語モデルで学習する研究。文書構造を言語モデルで学習し、文書の構造を言語モデルで学習する。文書構造を言語モデルで学習する際、文書構造を言語モデルで学習する。文書構造を言語モデルで学習する際、文書構造を言語モデルで学習する。文書構造を言語モデルで学習する際、文書構造を言語モデルで学習する。

- [DoSEA: A Domain-specific Entity-aware Framework for Cross-Domain Named Entity Recogition](https://aclanthology.org/2022.coling-1.188)
  - Minghao Tang, Peng Zhang, Yongquan He, Yongxiu Xu, Chengpeng Chao, Hongbo Xu
  - **TLDR**: ドメイン固有の表現を認識する研究。ドメイン固有の表現は、ドメイン固有の表現と同等の意味を持つが、ドメイン固有の表現は別個に認識する。そのため、ドメイン固有の表現を認識するタスクを追加し、タスクの精度を上げるために、タスクの精度を上げるためにタスクを追加する。

- [Incremental Prompting: Episodic Memory Prompt for Lifelong Event Detection](https://aclanthology.org/2022.coling-1.189)
  - Minqian Liu, Shiyu Chang, Lifu Huang
  - **TLDR**: ライフイベントの予測を行う際に、事前学習済みモデルの知識を再帰的に学習させる手法の提案。事前学習済みモデルの知識は、事前学習済みモデルの予測結果から学習する。事前学習済みモデルの知識は、事前学習済みモデルの学習済みモデルの学習済みモデルに復元する。

- [Recent Advances in Text-to-SQL: A Survey of What We Have and What We Expect](https://aclanthology.org/2022.coling-1.190)
  - Naihao Deng, Yulong Chen, Yue Zhang
  - **TLDR**: 自然言語処理でSQLを生成する際のサーベイ。自然言語処理のサーベイから、データセット、手法、また評価についてまとめられている。

- [An MRC Framework for Semantic Role Labeling](https://aclanthology.org/2022.coling-1.191)
  - Nan Wang, Jiwei Li, Yuxian Meng, Xiaofei Sun, Han Qiu, Ziyao Wang, Guoyin Wang, Jun He
  - **TLDR**: 文中の要約文を、文の要約文と同等のモデルで解く研究。要約文は文の要約文と同等のモデルで解くが、要約文は文の要約文と同等のモデルで解く。要約文は要約文の要約文と同等のモデルで解く。要約文は要約文の要約文と同等のモデルで解く。

- [PCBERT: Parent and Child BERT for Chinese Few-shot NER](https://aclanthology.org/2022.coling-1.192)
  - Peichao Lai, Feiyang Ye, Lin Zhang, Zhiwei Chen, Yanggeng Fu, Yingjie Wu, Yilei Wang
  - **TLDR**: 教師なし学習で、事前学習済みモデルを転移させる手法の提案。事前学習済みモデルは、事前学習済みモデルのラベルを入力に追加する。ラベルは、事前学習済みモデルのラベルと同等の精度を維持できるかを検証する。事前学習済みモデルは、事前学習済みモデルのラベルを入力に追加する。

- [Label Smoothing for Text Mining](https://aclanthology.org/2022.coling-1.193)
  - Peiyang Liu, Xiangyu Xi, Wei Ye, Shikun Zhang
  - **TLDR**: テキスト分類タスクで、Soft Labelを生成する研究。Soft Labelは通常のラベルと同等かそれ以上のラベルを生成するが、このラベルを生成する際はラベルの重みを調整する。この重みを、ラベルの重みと同等かそれ以上のラベルを生成する形で学習する。

- [Diverse Multi-Answer Retrieval with Determinantal Point Processes](https://aclanthology.org/2022.coling-1.194)
  - Poojitha Nandigam, Nikhil Rayaprolu, Manish Shrivastava
  - **TLDR**: マルチタスクで質問回答を行う手法の提案。質問に対する回答が複数ある場合、その回答を絞り込む手法を提案。質問に対する回答を絞り込む手法は、BERTをベースにしている。BERTは、質問に対する回答が複数ある場合、BERTをベースに絞り込む手法を採用している。

- [Improving Deep Embedded Clustering via Learning Cluster-level Representations](https://aclanthology.org/2022.coling-1.195)
  - Qing Yin, Zhihua Wang, Yunya Song, Yida Xu, Shuai Niu, Liang Bai, Yike Guo, Xian Yang
  - **TLDR**: テキスト分散表現を強化する研究。通常の分散表現は、クラスタリングのタスクで学習するが、このタスクではクラスタリングのタスクを学習する。タスクはクラスタリングのタスクで、タスクはクラスタリングのタスクで学習する。タスクは、クラスタリングのタスクで学習する。タスクは、タスクのタスクを学習する。タスクは、タスクのタスクを学習する。タスクは、タスクのタスクを学習する。タスクは、タスクのタスクを学習するタスクを追加する。タスクは、タスクのタスクを学習するタ

- [Decoupling Mixture-of-Graphs: Unseen Relational Learning for Knowledge Graph Completion by Fusing Ontology and Textual Experts](https://aclanthology.org/2022.coling-1.196)
  - Ran Song, Shizhu He, Suncong Zheng, Shengxiang Gao, Kang Liu, Zhengtao Yu, Jun Zhao
  - **TLDR**: 自然言語処理で、自然言語処理のモデルを、自然言語処理のモデルと同等精度で学習する研究。自然言語処理では、自然言語の文書と文書の文書を結合し、文書の文書と文書文書の文書文書を結合する。文書文書は文書文書と同等精度で学習するが、文書文書は文書文書と同等精度で学習する。

- [CETA: A Consensus Enhanced Training Approach for Denoising in Distantly Supervised Relation Extraction](https://aclanthology.org/2022.coling-1.197)
  - Ruri Liu, Shasha Mo, Jianwei Niu, Shengda Fan
  - **TLDR**: 文分類において、文中のラベルを変更することで、ラベルの誤差を防ぎつつクラス分類を誤差にさせる研究。ラベルの変更はクラス分類の枠組みで行い、クラス分類の枠組みはクラス分類の枠組みで行う。クラス分類の枠組みは、クラス分類の枠組みと同等か否かを判定する。

- [MedDistant19: Towards an Accurate Benchmark for Broad-Coverage Biomedical Relation Extraction](https://aclanthology.org/2022.coling-1.198)
  - Saadullah Amin, Pasquale Minervini, David Chang, Pontus Stenetorp, Guenter Neumann
  - **TLDR**: 臨床関係の抽出を、事前学習済みモデルで行う研究。事前学習済みモデルは、事前学習済みモデルのデータセットと同等のモデルを用い、事前学習済みモデルのデータセットと同等のモデルを用い事前学習済みモデルのデータセットを比較する。事前学習済みモデルは、事前学習済みモデルのデータセットと同等のモデルを用いることで精度を落とさず、精度を維持している。

- [Decorrelate Irrelevant, Purify Relevant: Overcome Textual Spurious Correlations from a Feature Perspective](https://aclanthology.org/2022.coling-1.199)
  - Shihan Dou, Rui Zheng, Ting Wu, SongYang Gao, Junjie Shan, Qi Zhang, Yueming Wu, Xuanjing Huang
  - **TLDR**: 自然言語処理において、データセットとモデル間の相関が低いと判断するサンプルを低重みでなく、重みを下げて学習する手法を提案。重みは、モデルの学習に使用する特徴量を減らすための手法で、学習時に使用する特徴量を減らすための手法を提案している。

- [Event Causality Identification via Derivative Prompt Joint Learning](https://aclanthology.org/2022.coling-1.200)
  - Shirong Shen, Heng Zhou, Tongtong Wu, Guilin Qi
  - **TLDR**: 自然言語処理で、事前学習済みモデルの予測結果と、事前学習済みモデルの予測結果を組み合わせたモデルの提案。事前学習済みモデルの予測結果と、事前学習済みモデルの予測結果を組み合わせたモデルを提案している。事前学習済みモデルの予測結果と、事前学習済みモデルの予測結果を組み合わせたモデルを提案している。

- [Event Causality Extraction with Event Argument Correlations](https://aclanthology.org/2022.coling-1.201)
  - Shiyao Cui, Jiawei Sheng, Xin Cong, Quangang Li, Tingwen Liu, Jinqiao Shi
  - **TLDR**: 自然言語処理における、イベントの因果推論を強化する研究。因果推論はイベントの構造を調べるのに有効だが、イベントの構造は複雑なので、イベントの構造を調べるのに困難なタスク。そこで、イベント構造を調べるタスクを提案。イベント構造は、イベントのイベント分布から予測する。

- [SCL-RAI: Span-based Contrastive Learning with Retrieval Augmented Inference for Unlabeled Entity Problem in NER](https://aclanthology.org/2022.coling-1.202)
  - Shuzheng Si, Shuang Zeng, Jiaxing Lin, Baobao Chang
  - **TLDR**: ニューラルネットワークの分類器で、同じクラスタリングで扱えるクラスタリングの距離を減らす研究。クラスタリングは、クラスタリングの距離を減らすため、クラスタリングの距離を大きくする。これにより、クラスタリングの距離を小さくする。

- [A Relation Extraction Dataset for Knowledge Extraction from Web Tables](https://aclanthology.org/2022.coling-1.203)
  - Siffi Singh, Alham Fikri Aji, Gaurav Singh, Christos Christodoulopoulos
  - **TLDR**: 自然言語処理で、自然言語処理のモデルを評価するタスクの提案。モデルは自然言語処理のモデルで、モデルの各テーブルは各テーブルのラベルから抽出される。ラベルは、テーブルのラベルとテーブルのラベルの2つから選ぶ。ラベルは、テーブルのラベルとテーブルのラベルの2つから選ぶ。

- [Automatic Keyphrase Generation by Incorporating Dual Copy Mechanisms in Sequence-to-Sequence Learning](https://aclanthology.org/2022.coling-1.204)
  - Siyu Wang, Jianhui Jiang, Yao Huang, Yin Wang
  - **TLDR**: テキスト中のkeywordを生成する研究。keywordは単語の長さではなく長さの長さで生成する。長さは長くなるほど長くなるが、長くなるほど長くなる単語を生成する。長くなる単語を生成する際は、長くなる単語を長くなる単語に置き換える。長くなる単語を長くなる単語に置き換える

- [Dependency-aware Prototype Learning for Few-shot Relation Classification](https://aclanthology.org/2022.coling-1.205)
  - Tianshu Yu, Min Yang, Xiaoyan Zhao
  - **TLDR**: 文中の依存構造を学習する研究。文中の依存構造を学習するネットワークを、文中の依存構造を学習するネットワークに組み込むことで、文中の依存構造を学習する。依存構造は、文中の単語を入力とする文の表現に置き換える形で学習する。

- [MECI: A Multilingual Dataset for Event Causality Identification](https://aclanthology.org/2022.coling-1.206)
  - Viet Dac Lai, Amir Pouran Ben Veyseh, Minh Van Nguyen, Franck Dernoncourt, Thien Huu Nguyen
  - **TLDR**: マルチ言語のデータセットを提案。既存のデータセットは、英語・日本語・英語・日本語・英語・日本語・英語・日本語・英語・英語・日本語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語・英語

- [Method Entity Extraction from Biomedical Texts](https://aclanthology.org/2022.coling-1.207)
  - Waqar Bin Kalim, Robert E. Mercer
  - **TLDR**: 自然言語処理で、自然言語処理の手法を自動生成する研究。自然言語処理では、文中の単語を抽出する際、文中の単語を抽出するモデルを使用。文中の単語は、文中の単語を抽出するモデルに入力し、文中の単語を抽出するモデルを生成する。文中の単語は、文中の単語を抽出するモデルに入力する。

- [Optimal Partial Transport Based Sentence Selection for Long-form Document Matching](https://aclanthology.org/2022.coling-1.208)
  - Weijie Yu, Liang Pang, Jun Xu, Bing Su, Zhenhua Dong, Ji-Rong Wen
  - **TLDR**: 文書の文書配分を行う際に、文書の文特徴を考慮した最適輸送を行う手法の提案。文書の文特徴を考慮した最適輸送を行うと、文書の文特徴が文書全体の一致度に影響するかどうかを検証する。文書の文特徴を考慮した最適輸送を行うと、文書全体の一致度が低い文書でも文書全体の一致度に影響する可能性があることを確認。

- [LightNER: A Lightweight Tuning Paradigm for Low-resource NER via Pluggable Prompting](https://aclanthology.org/2022.coling-1.209)
  - Xiang Chen, Lei Li, Shumin Deng, Chuanqi Tan, Changliang Xu, Fei Huang, Luo Si, Huajun Chen, Ningyu Zhang
  - **TLDR**: 低リソースの言語モデルで、学習済み言語モデルを学習する際、学習済み言語モデルの重みを調整する手法の提案。重みは学習済み言語モデルの重みと同等の重みを適用する。重みは学習済み言語モデルの重みと同等の重みを適用する。

- [Cross-modal Contrastive Attention Model for Medical Report Generation](https://aclanthology.org/2022.coling-1.210)
  - Xiao Song, Xiaodan Zhang, Junzhong Ji, Ying Liu, Pengxu Wei
  - **TLDR**: 画像から文書を作成するモデルの提案。画像の画像特徴をCross-Modal Contrastive Attentionで学習し、文書の文書構造をCross-Modal Attentionで学習する。文書構造は通常の画像と異なり、画像の画像特徴は通常の画像と同等になる。文書構造は通常の画像と同等になるよう、文書構造をCross-Modal Attentionで学習する。文書構造は通常の画像と同等になるよう、文書構造をCross-Modal Attentionで学習する

- [Domain-Specific NER via Retrieving Correlated Samples](https://aclanthology.org/2022.coling-1.211)
  - Xin Zhang, Yong Jiang, Xiaobin Wang, Xuming Hu, Yueheng Sun, Pengjun Xie, Meishan Zhang
  - **TLDR**: 自然言語処理で、自然言語処理のモデルで学習した単語を、自然言語処理のモデルで学習した単語と比較して精度が上がるか検証した研究。単語の学習は、単語の意味を予測するモデルで行う。単語の意味は、単語の意味を予測するモデルで学習する。

- [Type-enriched Hierarchical Contrastive Strategy for Fine-Grained Entity Typing](https://aclanthology.org/2022.coling-1.212)
  - Xinyu Zuo, Haijin Liang, Ning Jing, Shuang Zeng, Zhou Fang, Yu Luo
  - **TLDR**: マルチエンティティの文を生成する際、各エンティティの特徴をモデル化する手法の提案。マルチエンティティの文を生成する際、各エンティティの特徴をモデル化する。マルチエンティティの文を生成する際、マルチエンティティの文を生成する際、マルチエンティティの文を生成する際、マルチエンティティの文を生成する際、マルチエンティの文を生成する際、マルチエンティの文を生成する際、マルチエンティの文を生成する際、マルチエンティの文を生成する際、マルチエンティの文を生成

- [Document-Level Relation Extraction via Pair-Aware and Entity-Enhanced Representation Learning](https://aclanthology.org/2022.coling-1.213)
  - Xiusheng Huang, Hang Yang, Yubo Chen, Jun Zhao, Kang Liu, Weijian Sun, Zuyu Zhao
  - **TLDR**: 文書中の各文のEntity pairを抽出する研究。文書中のEntity pairは、文書全体のEntity pairと同等の意味を持つ。文書全体のEntity pairを抽出する際、文書全体のEntity pairを予測するPair-AwareとEntity-Enhancedの2つを提案。文書全体のEntity pairを予測するPair-Awareと、Entity-Enhancedの2つを提案している。

- [Improving Zero-Shot Entity Linking Candidate Generation with Ultra-Fine Entity Type Information](https://aclanthology.org/2022.coling-1.214)
  - Xuhui Sui, Ying Zhang, Kehui Song, Baohang Zhou, Guoqing Zhao, Xin Wei, Xiaojie Yuan
  - **TLDR**: 自然言語処理で、zero-shotのEntity Linkingを行う際に、事前学習済みモデルの特性を考慮した研究。事前学習済みモデルは、Entityの特性を学習するタスクを複数回行うが、このタスクはタスクの重みを考慮する必要がある。そのため、事前学習済みモデルの特性を考慮したタスクを提案している。

- [CofeNet: Context and Former-Label Enhanced Net for Complicated Quotation Extraction](https://aclanthology.org/2022.coling-1.215)
  - Yequan Wang, Xiang Li, Aixin Sun, Xuying Meng, Huaming Liao, Jiafeng Guo
  - **TLDR**: 文中の「良い点」を抽出する手法の提案。文中の「良い点」は、文の意味を含まない、文の意味を含まない、文の意味を含まない、文の意味を含まない、文の意味を含まない、文の意味を含まない、文の意味を含まない、文の意味を含まない、文の意味を含まない、文の意味を含まない、文の意味を含まない、文の意味を含まない、文の意味を含まない、文の意味を含まない、文の意味を含まない、文の意味を含

- [Supporting Medical Relation Extraction via Causality-Pruned Semantic Dependency Forest](https://aclanthology.org/2022.coling-1.216)
  - Yifan Jin, Jiangmeng Li, Zheng Lian, Chengbo Jiao, Xiaohui Hu
  - **TLDR**: 自然言語処理で、文関係から関係を抽出する研究。文関係は、文関係の意味を含まない単語を含まない単語に置き換える。文関係は、文関係の意味を含まない単語を含まない単語に置き換える。文関係は、文関係の意味を含まない単語に置き換える。文関係は、文関係の意味を含まない単語に置き換える。

- [Aspect-based Sentiment Analysis as Machine Reading Comprehension](https://aclanthology.org/2022.coling-1.217)
  - Yifei Yang, Hai Zhao
  - **TLDR**: 文中のaspectを、文の分類に使用する研究。文中のaspectは、文の分類結果から、文の分類結果から、文の分類結果から、文分類結果から、文分類結果から、文分類結果から、文分類結果から、文分類結果から、文分類結果から、文分類結果から、文分類結果から、文分類結果から、文分類結果から、文分類結果から、文分類結果から、文分類結果から、文分類結果から、文分類結果から、文分類結果から、文分類結果から、文分類結果から、文分類結果から、文分類結果から、文分類結果から、文分類結果から、文

- [Nested Named Entity Recognition as Corpus Aware Holistic Structure Parsing](https://aclanthology.org/2022.coling-1.218)
  - Yifei Yang, Zuchao Li, Hai Zhao
  - **TLDR**: 自然言語処理で、固有表現認識のモデルを学習する研究。固有表現認識は、固有表現の表現を扱うための構造を学習する。固有表現は、単語/文間の関係を考慮する形で学習する。固有表現は、単語/文間の関係を考慮する形で学習する。固有表現は、単語/文間の関係を考慮する形で学習する。固有表現は、固有表現の学習に適した構造を学習する。

- [DESED: Dialogue-based Explanation for Sentence-level Event Detection](https://aclanthology.org/2022.coling-1.219)
  - Yinyi Wei, Shuaipeng Liu, Jianwei Lv, Xiangyu Xi, Hailei Yan, Wei Ye, Tong Mo, Fan Yang, Guanglu Wan
  - **TLDR**: 文中のイベントを説明する研究。文中のイベントを説明する文書を生成し、文書の文書構造を学習する。文書構造は文書の文書構造と同等か、文書構造は文書構造と同等かを学習する。文書構造は文書構造と同等か、文書構造は文書構造と同等かを学習する。文書構造は文書構造と同等か、文書構造は文書構造と同等かを学習する

- [Data Augmentation for Few-Shot Knowledge Graph Completion from Hierarchical Perspective](https://aclanthology.org/2022.coling-1.220)
  - Yuanzhou Yao, Zhao Zhang, Yongjun Xu, Chao Li
  - **TLDR**: 知識グラフの更新を、データ拡張で行う研究。既存のモデルは、各タスクの更新を予測するモデルとして使われてきたが、このモデルは更新が困難なタスクを予測する。そのため、タスクの更新を予測するモデルを提案している。タスクの更新は、タスクの更新を予測するモデルの重みを加味する形で行う。

- [CLIO: Role-interactive Multi-event Head Attention Network for Document-level Event Extraction](https://aclanthology.org/2022.coling-1.221)
  - Yubing Ren, Yanan Cao, Fang Fang, Ping Guo, Zheng Lin, Wei Ma, Yi Liu
  - **TLDR**: 文書から、マルチイベントの情報を抽出する研究。マルチイベントの情報を抽出する際、各イベントの位置を予測するモデルを導入する。位置予測は、各イベントの位置を予測するモデルの重みを計算する形で行う。位置予測は、各イベントの位置を予測するモデルの重みを計算する形で行う。

- [COPNER: Contrastive Learning with Prompt Guiding for Few-shot Named Entity Recognition](https://aclanthology.org/2022.coling-1.222)
  - Yucheng Huang, Kai He, Yige Wang, Xianli Zhang, Tieliang Gong, Rui Mao, Chen Li
  - **TLDR**: 数枚のサンプルから、Entityクラスの類似度を推定する手法の提案。Entityクラスの類似度は、Entityクラスのクラス分類器で学習する。Entityクラスの類似度は、Entityクラスのクラス分類器で学習する。Entityクラスの類似度は、Entityクラスのクラス分類器で学習する。

- [Few Clean Instances Help Denoising Distant Supervision](https://aclanthology.org/2022.coling-1.223)
  - Yufang Liu, Ziyin Huang, Yijun Wang, Changzhi Sun, Man Lan, Yuanbin Wu, Xiaofeng Mou, Ding Wang
  - **TLDR**: 教師なしモデルの評価を改善する研究。モデルの評価は、モデルの学習/評価結果と、モデルの評価結果との相関が大きい場合に行われる。教師なしモデルは、学習データの質が低い場合に評価結果が低い場合に評価結果が低い場合に評価結果が低い場合に評価結果が低い場合に評価結果が低い場合に評価結果が低い場合に評価結果が低い場合に評価結果が低い場合に評価結果が低い場合に評価結果が低い場合に評価結果が低い場合に評価結果が低い場合に評価結果が低い場合に評価結果が低い場合に評価結果が低い場合に評価結果が低い場合に評価結果

- [SEE-Few: Seed, Expand and Entail for Few-shot Named Entity Recognition](https://aclanthology.org/2022.coling-1.224)
  - Zeng Yang, Linhai Zhang, Deyu Zhou
  - **TLDR**: 少数サンプルでFew-shotのEntity検出を行う研究。Entityの分類はテキストをEncodeするEncoderを用い、EncoderはEncodeしたテキストをEncodeしたEntityのEncode結果から学習する。EntityのEncode結果から、Few-shotのEntity検出は難しいと判断する。

- [Ruleformer: Context-aware Rule Mining over Knowledge Graph](https://aclanthology.org/2022.coling-1.225)
  - Zezhong Xu, Peng Ye, Hui Chen, Meng Zhao, Huajun Chen, Wen Zhang
  - **TLDR**: 知識グラフの計算を、Transformerベースで行う研究。Transformerは、文書中の文書構造をAttentionに変換する。Attentionは文書構造を予測する際の重要な情報。文書構造を予測する際は、文書構造を予測する文書構造を予測する。文書構造を予測する際は、文書構造を予測する文書構造を予測する。

- [Are People Located in the Places They Mention in Their Tweets? A Multimodal Approach](https://aclanthology.org/2022.coling-1.226)
  - Zhaomin Xiao, Eduardo Blanco
  - **TLDR**: ニュースや画像から、誰が見ているかを調べる研究。ニュースや画像は、ニュースの投稿から特定できるかを調べるのに有効なツールだが、ニュースの投稿から特定できるかは、ニュースの投稿から特定できるかを調べるのに有効なツールではないかという研究。ニュースの投稿から特定できるかを調べるのに有効なツールは、ニュースの投稿から特定できるかを調べるのに有効なツールとしている。

- [Multi-modal Contrastive Representation Learning for Entity Alignment](https://aclanthology.org/2022.coling-1.227)
  - Zhenxi Lin, Ziheng Zhang, Meng Wang, Yinghui Shi, Xian Wu, Yefeng Zheng
  - **TLDR**: マルチモーダルな知識を組み合わせる手法の提案。マルチモーダルな知識を組み合わせる際は、マルチモーダルな知識を組み合わせる際の重みを考慮する。マルチモーダルな知識は、マルチモーダルな知識の重みを考慮する。マルチモーダルな知識は、マルチモーダルな知識の重みを考慮する。マルチモーダルな知識は、マルチモーダルな知識の重みを考慮する。

- [Nonparametric Forest-Structured Neural Topic Modeling](https://aclanthology.org/2022.coling-1.228)
  - Zhihong Zhang, Xuewen Zhang, Yanghui Rao
  - **TLDR**: 自然言語処理で、各タスクのタスクを個別に分類する研究。タスクはタスク分類(Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-Top-

- [KGE-CL: Contrastive Learning of Tensor Decomposition Based Knowledge Graph Embeddings](https://aclanthology.org/2022.coling-1.229)
  - Zhiping Luo, Wentao Xu, Weiqing Liu, Jiang Bian, Jian Yin, Tie-Yan Liu
  - **TLDR**: 知識グラフの分散表現を、各ノードの類似度を考慮したモデルで学習する研究。各ノードの類似度は、各ノードの類似度と同等か否かで計算する。類似度は、ノードの類似度と同等か否かで計算する。類似度は、ノードの類似度と同等か否かで計算する。

- [A Coarse-to-fine Cascaded Evidence-Distillation Neural Network for Explainable Fake News Detection](https://aclanthology.org/2022.coling-1.230)
  - Zhiwei Yang, Jing Ma, Hechang Chen, Hongzhan Lin, Ziyang Luo, Yi Chang
  - **TLDR**: ニュースの事実誤認を、事前の情報から推論する研究。事前の情報から推論する文書を生成する際、文書の文書分類をCascade-to-fineで行う。文書分類は、文書分類のEncoder/Decoderの2つを用いている。文書分類は、文書分類のEncoder/Decoderの2つを用いている。

- [Document-level Event Factuality Identification via Machine Reading Comprehension Frameworks with Transfer Learning](https://aclanthology.org/2022.coling-1.231)
  - Zhong Qian, Heng Zhang, Peifeng Li, Qiaoming Zhu, Guodong Zhou
  - **TLDR**: 自然言語処理における文書分類タスクの提案。文書分類タスクは文書分類の知識がないと難しいため、文書分類タスクを文書分類タスクと同等のモデルで解く。文書分類タスクは文書分類タスクと同等のモデルで解くが、文書分類タスクは文書分類タスクと同等のモデルで解く必要がある。

- [Unregulated Chinese-to-English Data Expansion Does NOT Work for Neural Event Detection](https://aclanthology.org/2022.coling-1.232)
  - Zhongqiu Li, Yu Hong, Jie Wang, Shiming He, Jianmin Yao, Guodong Zhou
  - **TLDR**: 翻訳文を英語/中国語に拡張する研究。翻訳文の予測結果を、翻訳文の予測結果と異なる単語に置き換える(予測結果が異なる単語を予測する)ことで、翻訳文の予測結果を予測する。予測結果は、翻訳文の予測結果と異なる単語を予測する単語を予測するモデルが有効だった。

- [Finding Influential Instances for Distantly Supervised Relation Extraction](https://aclanthology.org/2022.coling-1.233)
  - Zifeng Wang, Rui Wen, Xi Chen, Shao-Lun Huang, Ningyu Zhang, Yefeng Zheng
  - **TLDR**: 強化学習で、モデルのモデルアノテーションを強化学習モデルに適用した研究。モデルは、モデルのモデルアノテーション(モデルのモデルアノテーション)と、モデルのモデルアノテーション(モデルのモデルアノテーション)の2つを組み合わせている。モデルは、モデルアノテーションの結果をモデルアノテーションの結果と比較し、モデルアノテーションの結果を比較するモデルを提案している。

- [A Simple Model for Distantly Supervised Relation Extraction](https://aclanthology.org/2022.coling-1.234)
  - Ziqin Rao, Fangxiang Feng, Ruifan Li, Xiaojie Wang
  - **TLDR**: 教師なしのモデルで、モデルの重みを計算する手法の提案。重みは、モデルの重みを計算するモデルの重みと、モデルの重みを計算する重みの2つに分けている。重みは、重みの計算時に重みを計算する重みの重みを計算する重みと、重みの計算時に重みを計算する重みの2つに分けている。重みの計算は、重みの計算時に重みの計算を行わず、重みの計算時に重みの計算を行わずに行う。重み計算は、重みの計算を行わずに行う手法を提案している

- [Augmenting Legal Judgment Prediction with Contrastive Case Relations](https://aclanthology.org/2022.coling-1.235)
  - Dugang Liu, Weihao Du, Lei Li, Weike Pan, Zhong Ming
  - **TLDR**: 裁判所の判決文を予測する際、各ケースの個別事実情報を入力として使う手法の提案。個別事実情報を入力として使うと、各ケースの個別事実情報を入力として使うより精度が上がる。個別事実情報を入力として使うと、各ケースの個別事実情報を入力として使うより精度が上がる。

- [Constrained Regeneration for Cross-Lingual Query-Focused Extractive Summarization](https://aclanthology.org/2022.coling-1.236)
  - Elsbeth Turcan, David Wan, Faisal Ladhak, Petra Galuscakova, Sukanta Sen, Svetlana Tchistiakova, Weijia Xu, Marine Carpuat, Kenneth Heafield, Douglas Oard, Kathleen McKeown
  - **TLDR**: 翻訳文から、翻訳文の要約を生成する手法の提案。要約は翻訳文の要約文を生成するが、要約文は要約文の要約文を生成する。要約文は要約文の要約文を生成する。要約文は要約文の要約文を生成する。要約文は要約文の要約文を生成する。

- [Programmable Annotation with Diversed Heuristics and Data Denoising](https://aclanthology.org/2022.coling-1.237)
  - Ernie Chang, Alex Marin, Vera Demberg
  - **TLDR**: 自然言語処理で、モデルの生成を行う際に、モデルのラベルを変更する手法の提案。ラベルは、モデルの生成時に使用するラベルと、モデルの生成時に使用するラベルをそれぞれ別々に作成する。ラベルは、モデルの生成時に使用するラベルと、モデルの生成時に使用するラベルを別々に作成する。

- [Text-to-Text Extraction and Verbalization of Biomedical Event Graphs](https://aclanthology.org/2022.coling-1.238)
  - Giacomo Frisoni, Gianluca Moro, Lorenzo Balzani
  - **TLDR**: 自然言語処理におけるイベント抽出と文書生成の手法を統合した研究。既存の手法は、事前学習済みモデルをベースにしており、事前学習済みモデルは事前学習済みモデルの学習済みモデルと同等の性能を発揮する。事前学習済みモデルは、事前学習済みモデルの学習済みモデルと同等の性能を発揮する。

- [Multimodal Semi-supervised Learning for Disaster Tweet Classification](https://aclanthology.org/2022.coling-1.239)
  - Iustin Sirbu, Tiberiu Sosea, Cornelia Caragea, Doina Caragea, Traian Rebedea
  - **TLDR**: 自然災害による災害発生時に、SNSで投稿されたニュースを分類する研究。分類は、ニュースの投稿から生成されたニュースを分類する。ニュースの投稿は、ニュースの投稿と同等の分類結果になるが、ニュースの投稿は、ニュースの投稿と同等の分類結果になる。

- [Automated Essay Scoring via Pairwise Contrastive Regression](https://aclanthology.org/2022.coling-1.240)
  - Jiayi Xie, Kaiwei Cai, Li Kong, Junsheng Zhou, Weiguang Qu
  - **TLDR**: 文の評価を、文の質と評価の2つに分けて行う手法の提案。文の質と評価の2つを同時に行うことで、文の質と評価の2つを同時に行う。文の質は文の質と評価の2つに分けて評価する。評価は文の質と評価の2つに分けて行う。

- [Medical Question Understanding and Answering with Knowledge Grounding and Semantic Self-Supervision](https://aclanthology.org/2022.coling-1.241)
  - Khalil Mrini, Harpreet Singh, Franck Dernoncourt, Seunghyun Yoon, Trung Bui, Walter W. Chang, Emilia Farcas, Ndapa Nakashole
  - **TLDR**: 医療従事者に対する質問に対する解釈性を高める研究。質問に対する解釈性を高めるために、質問に対する文を検索するモデルを提案。文の検索は、医療従事者から寄せられた質問に対する文を検索する形で行う。文の検索は、質問に対する文の検索結果をベースに、文の検索結果をベースに行う。

- [A Progressive Framework for Role-Aware Rumor Resolution](https://aclanthology.org/2022.coling-1.242)
  - Lei Chen, Guanying Li, Zhongyu Wei, Yang Yang, Baohua Zhou, Qi Zhang, Xuanjing Huang
  - **TLDR**: ニュースに対する推論手法を、事前学習済みモデルで学習する研究。事前学習済みモデルは、ニュースに対する推論結果を、ニュースの投稿から推論結果を推定するネットワークを構築している。事前学習済みモデルは、ニュースの投稿から推論結果を推定するネットワークを構築している。

- [Uncertainty-aware Propagation Structure Reconstruction for Fake News Detection](https://aclanthology.org/2022.coling-1.243)
  - Lingwei Wei, Dou Hu, Wei Zhou, Songlin Hu
  - **TLDR**: フェイクニュースの検知を行う際に、ネットワークの構造を再構築する研究。ネットワークの構造は、ネットワークのネットワークのネットワークのネットワーク構造と同等か、それ以上のネットワーク構造かを検証する。ネットワークの構造は、ネットワークのネットワーク構造と同等か否かを検証する。ネットワークのネットワーク構造は、ネットワークのネットワーク構造と同等か否かを検証する。

- [A Unified Propagation Forest-based Framework for Fake News Detection](https://aclanthology.org/2022.coling-1.244)
  - Lingwei Wei, Dou Hu, Yantong Lai, Wei Zhou, Songlin Hu
  - **TLDR**: フェイクニュースの拡散を行う際に、ネットワークの枝刈りを行う研究。枝刈りはネットワークの枝刈りと同等の手法で、ネットワークの枝刈りはネットワークの枝刈りと同等の手法で行う。ネットワークの枝刈りはネットワークの枝刈りと同等の手法で行う。

- [CLoSE: Contrastive Learning of Subframe Embeddings for Political Bias Classification of News Media](https://aclanthology.org/2022.coling-1.245)
  - Michelle YoungJin Kim, Kristen Marie Johnson
  - **TLDR**: ニュース記事から、政治的偏りを検出する研究。ニュース記事から、政治的偏りを検出するモデルを構築し、モデルの学習データから政治的偏りを検出する。モデルはBERTベースで、BERTの学習データから、政治的偏りを検出するモデルを構築している。

- [Grammatical Error Correction: Are We There Yet?](https://aclanthology.org/2022.coling-1.246)
  - Muhammad Reza Qorib, Hwee Tou Ng
  - **TLDR**: 自然言語処理で、文の誤り訂正が難しいケースを検証した研究。文の誤り訂正は、文の意味を誤らせると誤認識するケースが考えられるが、この場合誤認識する単語は誤認識するべき単語と見なされる。誤認識は、文の意味を誤認識する単語と見なされる場合に起こりうる。

- [CXR Data Annotation and Classification with Pre-trained Language Models](https://aclanthology.org/2022.coling-1.247)
  - Nina Zhou, Ai Ti Aw, Zhuo Han Liu, Cher heng Tan, Yonghan Ting, Wen Xiang Chen, Jordan sim zheng Ting
  - **TLDR**: 臨床データに対する機械学習の適用について、事前学習済みモデルのモデルを改良した研究。事前学習済みモデルは、事前学習済みモデルのモデルの予測精度を上げるためのモデルを自動生成する。事前学習済みモデルは、事前学習済みモデルの予測精度を上げるためのモデルを自動生成する。

- [uChecker: Masked Pretrained Language Models as Unsupervised Chinese Spelling Checkers](https://aclanthology.org/2022.coling-1.248)
  - Piji Li
  - **TLDR**: 自然言語処理で、文の文法誤差を検出するタスクの提案。文法誤差は、文の文法を誤認識するだけでなく、文法の意味を誤認識する潜在表現(単語)に置き換える形で検出される。文法誤差は、文法の意味を誤認識する潜在表現に置き換える形で検出される。

- [Boosting Deep CTR Prediction with a Plug-and-Play Pre-trainer for News Recommendation](https://aclanthology.org/2022.coling-1.249)
  - Qijiong Liu, Jieming Zhu, Quanyu Dai, Xiaoming Wu
  - **TLDR**: ニュースの検索結果を更新する際、事前学習済み言語モデルを活用する研究。事前学習済み言語モデルは、検索結果の更新が早く、更新頻度が高い場合更新が遅くなる傾向がある。そのため、更新頻度が高い場合更新が早いモデルを導入する。更新頻度が高い場合更新が早いモデルを導入する

- [Improving Fake News Detection of Influential Domain via Domain- and Instance-Level Transfer](https://aclanthology.org/2022.coling-1.250)
  - Qiong Nan, Danding Wang, Yongchun Zhu, Qiang Sheng, Yuhui Shi, Juan Cao, Jintao Li
  - **TLDR**: マルチドメインのニュース検知を行う際に、ドメインレベルの転移を行なう研究。ドメインレベルの転移は、転移先のドメインのニュースを予測するネットワークのネットワークを学習する形で行う。転移先のネットワークは、ニュースのニュースソースからの情報量を予測するネットワークと、ニュースのニュースソースからの情報量を予測するネットワークの2つに分けている。

- [Student Surpasses Teacher: Imitation Attack for Black-Box NLP APIs](https://aclanthology.org/2022.coling-1.251)
  - Qiongkai Xu, Xuanli He, Lingjuan Lyu, Lizhen Qu, Gholamreza Haffari
  - **TLDR**: 機械学習モデルに対する、モデルの盗作手法の研究。盗作手法は、モデルの学習済みモデルを転移先のモデルに置き換える手法。転移先のモデルは、転移先のモデルと同等の性能を発揮するが、転移先のモデルは転移先のモデルと同等の性能を発揮する。

- [Combining Compressions for Multiplicative Size Scaling on Natural Language Tasks](https://aclanthology.org/2022.coling-1.252)
  - Rajiv Movva, Jinhao Lei, Shayne Longpre, Ajay Gupta, Chris DuBois
  - **TLDR**: ニューラルネットの計算量を削減する手法の提案。モデルサイズを削減するだけでなく、計算量を削減するモデルサイズの損失を抑えるため、計算量を減らす手法を組み合わせる。計算量を減らすと、計算量の損失を抑えるためモデルサイズを減らす手法を組み合わせる。

- [PlugAT: A Plug and Play Module to Defend against Textual Adversarial Attack](https://aclanthology.org/2022.coling-1.253)
  - Rui Zheng, Rong Bao, Qin Liu, Tao Gui, Qi Zhang, Xuanjing Huang, Rui Xie, Wei Wu
  - **TLDR**: テキスト分類のモデルで、Adversarialの学習を行わずに学習を行う手法の提案。学習済みモデルの挙動を学習する際、学習済みモデルの挙動を学習するモデルの挙動を学習する。学習済みモデルの挙動を学習する際は、学習済みモデルの挙動を学習するモデルの挙動を学習する。

- [Automatic ICD Coding Exploiting Discourse Structure and Reconciled Code Embeddings](https://aclanthology.org/2022.coling-1.254)
  - Shurui Zhang, Bozheng Zhang, Fuxin Zhang, Bo Sang, Wanchun Yang
  - **TLDR**: 自然言語処理で、自然言語処理の論文を自動翻訳する研究。論文のタイトルは「病気」で、論文の文は「病気」の単語を「病気」と置き換えたもの。文書の構造は、論文の文を分類する文と、論文の文を分類する文の2つに分けている。文書の分類は、文書分類の分類器で行う。

- [Towards Summarizing Healthcare Questions in Low-Resource Setting](https://aclanthology.org/2022.coling-1.255)
  - Shweta Yadav, Cornelia Caragea
  - **TLDR**: 医療文書の要約を、自然言語処理で行う研究。要約は、要約の単語を複数回に分けて入力し、要約の単語を予測する形で要約を作成する。要約は、要約の単語を予測するモデルを用い、要約の単語を予測するモデルを用い要約を作成する。要約は、要約の単語を予測するモデルを用い要約を作成する。

- [Doc-GCN: Heterogeneous Graph Convolutional Networks for Document Layout Analysis](https://aclanthology.org/2022.coling-1.256)
  - Siwen Luo, Yihao Ding, Siqu Long, Josiah Poon, Soyeon Caren Han
  - **TLDR**: 文書レイアウトの分類を行う際に、各要素の特徴をグラフで表現する研究。各要素の特徴をグラフで表現し、各要素の特徴をグラフの階層構造で表現する。これにより、文書レイアウトの分類精度を上げることができる。

- [Analytic Automated Essay Scoring Based on Deep Neural Networks Integrating Multidimensional Item Response Theory](https://aclanthology.org/2022.coling-1.257)
  - Takumi Shibata, Masaki Uto
  - **TLDR**: 自動文理モデルの提案。既存のモデルは、各文の評価結果を個別に予測するが、個別の評価結果は個別に予測する。そのため、各文の評価結果を個別に予測するモデルを提案している。

- [DP-Rewrite: Towards Reproducibility and Transparency in Differentially Private Text Rewriting](https://aclanthology.org/2022.coling-1.258)
  - Timour Igamberdiev, Thomas Arnold, Ivan Habernal
  - **TLDR**: プライバシー保護を強化する研究の提案。プライバシー保護の仕組みを、事前学習済みモデルと比較してシンプルかつ高速に更新できる。事前学習済みモデルは、プライバシー保護の仕組みをベースにしている。事前学習済みモデルは、プライバシー保護の仕組みをベースにしている。事前学習済みモデルは、プライバシー保護の仕組みをベースにしている。

- [Harnessing Abstractive Summarization for Fact-Checked Claim Detection](https://aclanthology.org/2022.coling-1.259)
  - Varad Bhatnagar, Diptesh Kanojia, Kameswari Chebrolu
  - **TLDR**: ツイートに対する事実誤認を自動生成する研究。ツイートのタイトルを、ツイートの投稿文から、ツイートの投稿文からツイート文を生成する。ツイートのタイトルは、ツイートのツイートを更新する(ツイートはツイートの更新頻度を表す)、ツイートはツイートの更新頻度を表す(ツイートはツイートの更新頻度を表す)、ツイートはツイートの更新頻度を表す(ツイート更新頻度はツイート更新頻度)、ツイートはツイート更新頻

- [Learning to Generate Explanation from e-Hospital Services for Medical Suggestion](https://aclanthology.org/2022.coling-1.260)
  - Wei-Lin Chen, An-Zi Yen, Hen-Hsen Huang, Hsin-Hsi Chen
  - **TLDR**: 自然言語処理で、病気の説明を生成する研究。患者の質問に対し、質問に対する回答を生成する。質問に対する回答は、質問に対する回答と同等の文書で生成される。文書は文書の文書構造をベースに、文書の文書構造は文書の文書構造をベースにしている。文書は文書構造をベースに、文書の文書構造は文書構造をベースにしている。

- [DeltaNet: Conditional Medical Report Generation for COVID-19 Diagnosis](https://aclanthology.org/2022.coling-1.261)
  - Xian Wu, Shuxin Yang, Zhaopeng Qiu, Shen Ge, Yangtian Yan, Xingwang Wu, Yefeng Zheng, S. Kevin Zhou, Li Xiao
  - **TLDR**: 放射線画像の診断に関する論文を自動生成する研究。画像をEncodeし、Encodeした画像から画像を生成する。Encodeした画像をEncodeした画像から、画像を画像に変換する。生成した画像を画像に変換するEncoderと、画像を画像に変換するDecodeerの2つから生成を行う。

- [MCS: An In-battle Commentary System for MOBA Games](https://aclanthology.org/2022.coling-1.262)
  - Xiaofeng Qi, Chao Li, Zhongping Liang, Jigang Liu, Cheng Zhang, Yuanxin Wei, Lin Yuan, Guang Yang, Lanxiao Huang, Min Li
  - **TLDR**: 対戦データから、対戦結果を生成する研究。対戦結果は、対戦相手の行動を予測する形で生成される。対戦結果は、対戦相手の行動を予測する形で生成される。対戦結果は、対戦相手の行動を予測する形で生成される。対戦結果は、対戦相手の行動を予測する形で生成される。

- [A Two Stage Adaptation Framework for Frame Detection via Prompt Learning](https://aclanthology.org/2022.coling-1.263)
  - Xinyi Mou, Zhongyu Wei, Changjian Jiang, Jiajie Peng
  - **TLDR**: 対話システムの転移を、事前学習済みモデルと事前学習済みモデルの2つに分けて行う研究。事前学習済みモデルは、事前学習済みモデルのEncoder/Promptを転移可能なEncoder/Promptに変換する。この転移は、事前学習済みモデルのEncoder/Promptを転移可能なEncoder/Promptに変換する形で行う。

- [Summarizing Patients’ Problems from Hospital Progress Notes Using Pre-trained Sequence-to-Sequence Models](https://aclanthology.org/2022.coling-1.264)
  - Yanjun Gao, Dmitriy Dligach, Timothy Miller, Dongfang Xu, Matthew M. M. Churpek, Majid Afshar
  - **TLDR**: 入院中の患者の要領説明を自然言語処理で行う研究。要領説明は文書から生成するが、文書から要領説明を生成する際は文書から要領説明を生成する。要領説明は文書から生成するが、文書から要領説明を生成する際は文書から要領説明を生成する。要領説明は文書から生成するが、文書から要領説明を生成する際は文書から要領説明を生成する。

- [Human-in-the-loop Robotic Grasping Using BERT Scene Representation](https://aclanthology.org/2022.coling-1.265)
  - Yaoxian Song, Penglei Sun, Pengfei Fang, Linyi Yang, Yanghua Xiao, Yue Zhang
  - **TLDR**: 自然言語処理で、ロボットのGraspingを自動化する研究。画像をGraspingのモデルに入力し、Graspingモデルの入力からGraspingのモデルを生成する。入力は、画像のGraspingモデルで入力する。入力は、画像のGraspingモデルで入力する。

- [Automated Chinese Essay Scoring from Multiple Traits](https://aclanthology.org/2022.coling-1.266)
  - Yaqiong He, Feng Jiang, Xiaomin Chu, Peifeng Li
  - **TLDR**: 文の分類、文の構成、文の文長、文の文長、文長の4つのタスクを、マルチタスクで評価するモデルの提案。タスクは文分類、文長、文長の4つ。タスクは、文長、文長、文長の4つ。タスクは、文長、文長、文長の4つ。タスクは、文長、文長、文長の4つ。タスクは、文長、文長、文長の4つ。タスクは、文長、文長、文長の4つ。タスクは、文長、文長、文長の4

- [Semantic-Preserving Adversarial Code Comprehension](https://aclanthology.org/2022.coling-1.267)
  - Yiyang Li, Hongqiu Wu, Hai Zhao
  - **TLDR**: 事前学習済み言語モデルに対するAdversarial Attackの対策。事前学習済みモデルは、Adversarial Attackの最小限の防御で学習済みモデルを防衛できるか検証している。事前学習済みモデルは、Adversarial Attackの最小限防御で防衛できるか検証している。

- [Continually Detection, Rapidly React: Unseen Rumors Detection Based on Continual Prompt-Tuning](https://aclanthology.org/2022.coling-1.268)
  - Yuhui Zuo, Wei Zhu, Guoyong GUET Cai
  - **TLDR**: 事前学習済みモデルを、連続的なタスク学習で学習する手法の提案。タスクの重みを固定し、タスクの重みを更新するネットワークを構築する。タスクの重みは、タスクの重みと同等の重みを持たせることで、タスクの重みを更新する。タスクの重みは、タスクの重みと同等の重みを持たせることで、タスクの重みを更新する。

- [AiM: Taking Answers in Mind to Correct Chinese Cloze Tests in Educational Applications](https://aclanthology.org/2022.coling-1.269)
  - Yusen Zhang, Zhongli Li, Qingyu Zhou, Ziyi Liu, Chao Li, Mina Ma, Yunbo Cao, Hongzhi Liu
  - **TLDR**: 教師が教師の手書き文書を認識するタスクで、複数回のタスクを同時に行う手法の提案。タスクは、タスクの回答文を入力に入力する。タスクは、タスクの回答文を入力に入力する。タスクは、タスクの回答文を入力に入力する。タスクは、タスクの回答文を入力に入力する。タスクは、タスクの回答文を入力に入力する。タスクは、タスクの回答文を入力に入力する。タスクは、タスクの回答文を入力に入力する。タスクは、タスクの回答文を入力に入力する。タ

- [TreeMAN: Tree-enhanced Multimodal Attention Network for ICD Coding](https://aclanthology.org/2022.coling-1.270)
  - Zichen Liu, Xuyuan Liu, Yanlong Wen, Guoqing Zhao, Fen Xia, Xiaojie Yuan
  - **TLDR**: 医療記録から、病気の診断結果を自動生成する研究。各文書のAttentionを、各文書のAttentionに近い重みで表現する。文書の重みは、文書の構造を学習するTreeMANで作成している。

- [Gated Mechanism Enhanced Multi-Task Learning for Dialog Routing](https://aclanthology.org/2022.coling-1.271)
  - Ziming Huang, Zhuoxuan Jiang, Ke Wang, Juntao Li, Shanshan Feng, Xian-Ling Mao
  - **TLDR**: マルチタスク学習の提案。タスクごとに、タスクごとに、タスクごとに、タスクごとに、タスクごとに、タスクごとに、タスクごとに、タスクごとに、タスクごとに、タスクごとに、タスクごとに、タスクごとに、タスクごとに、タスクごとに、タスクごとに、タスクごとに、タスクごとに、タスクごとに、タスクごとに、タスクごとに、タスクごとに、タスクごとに、タスクごとに、タスクごとに、タスクごとに、タスクごとに、タスクごとに、タスクごとに、タスクごとに、タスクごとに、タスクごとに、タスクごとに、タスクごとに、タスクごとに、タスクごとに

- [Negation, Coordination, and Quantifiers in Contextualized Language Models](https://aclanthology.org/2022.coling-1.272)
  - Aikaterini-Lida Kalouli, Rita Sevastjanova, Christin Beck, Maribel Romero
  - **TLDR**: 自然言語処理モデルの学習方法を調査した研究。モデルの学習は、単語の意味を予測するモデル(=単語の意味を予測する)と、単語の意味を予測するモデル(=単語の意味を予測する)の2つに分けて行っている。単語の意味を予測するモデルは、単語の意味を予測するモデルと同等の役割を果たしている。

- [Tales and Tropes: Gender Roles from Word Embeddings in a Century of Children’s Books](https://aclanthology.org/2022.coling-1.273)
  - Anjali Adukia, Patricia Chiril, Callista Christ, Anjali Das, Alex Eble, Emileigh Harrison, Hakizumwami Birali Runesha
  - **TLDR**: 教師のテキストで、性別を扱った研究。性別は教師の役割として扱われているが、実際は教師の役割は男性の役割と女性の役割を区別する。教師の役割は、教師の役割と生徒の役割を区別する。教師の役割は、教師の役割と生徒の役割を区別する。教師の役割は、教師の役割と生徒の役割を区別する。

- [CLOWER: A Pre-trained Language Model with Contrastive Learning over Word and Character Representations](https://aclanthology.org/2022.coling-1.274)
  - Borun Chen, Hongyin Tang, Jiahao Bu, Kai Zhang, Jingang Wang, Qifan Wang, Hai-Tao Zheng, Wei Wu, Liqian Yu
  - **TLDR**: 自然言語処理で、文字と単語を同時に学習する手法の提案。文字と単語は、文字と文字の位置関係を認識する潜在表現として使用される。文字と文字は、文字と文字の位置関係を認識する潜在表現として使用される。文字と文字は、文字と文字の位置関係を認識する潜在表現として使用される。

- [On the Nature of BERT: Correlating Fine-Tuning and Linguistic Competence](https://aclanthology.org/2022.coling-1.275)
  - Federica Merendi, Felice Dell’Orletta, Giulia Venturi
  - **TLDR**: 事前学習済みモデルの言語知識と解釈能力の関係を調べた研究。事前学習済みモデルの言語知識は、解釈性の向上に寄与するが解釈性の低下は逆効果。事前学習済みモデルの言語知識は解釈性の向上に寄与するが解釈性の低下は逆効果。

- [LayerConnect: Hypernetwork-Assisted Inter-Layer Connector to Enhance Parameter Efficiency](https://aclanthology.org/2022.coling-1.276)
  - Haoxiang Shi, Rongsheng Zhang, Jiaan Wang, Cen Wang, Yinhe Zheng, Tetsuya Sakai
  - **TLDR**: 自然言語処理で、Transformerの入力をLSTMに置き換える研究。入力の入力をLSTMの入力と入力の入力を入力とし、入力の入力を入力と入力の入力を入力とし入力と入力の入力を入力とし入力と入力の入力を入力とし入力と入力の入力を入力とし入力と入力の入力を入力とし入力と入力の入力を入力とし入力と入力の入力を入力とし入力と入力の入力を入力とし入力と入力の入力を入力とし入力と入力の入力を入力とし入力と入力の入力を入力とし入力と入力の入力を入力とし

- [Effect of Post-processing on Contextualized Word Representations](https://aclanthology.org/2022.coling-1.277)
  - Hassan Sajjad, Firoj Alam, Fahim Dalvi, Nadir Durrani
  - **TLDR**: 事前学習済み言語モデルで、事前学習済みモデルの表現を事前学習済みモデルに適用する際の注意点についてまとめた研究。事前学習済みモデルの表現を事前学習済みモデルに適用する際は、事前学習済みモデルの表現を事前学習済みモデルに適用する。事前学習済みモデルは、事前学習済みモデルの表現を事前学習済みモデルに適用する。

- [Does BERT Rediscover a Classical NLP Pipeline?](https://aclanthology.org/2022.coling-1.278)
  - Jingcheng Niu, Wenjie Lu, Gerald Penn
  - **TLDR**: 文構造を解釈する手法について、文構造の構造を解釈する手法を提案した研究。文構造は、文構造の構造を解釈する手法として、文構造の構造を解釈する手法を提案している。文構造は、文構造の構造を解釈する手法を提案している。文構造は、文構造の構造を解釈する手法を提案している。

- [HG2Vec: Improved Word Embeddings from Dictionary and Thesaurus Based Heterogeneous Graph](https://aclanthology.org/2022.coling-1.279)
  - Qitong Wang, Mohammed J Zaki
  - **TLDR**: 自然言語処理で、単語分散表現を学習する研究。単語分散表現は、単語の類似度(類似度=>類似度の分布)と、単語分散表現の類似度(類似度分布=>分散表現分布)の2つから学習する。単語分散表現は、単語分散表現の学習に使用する。

- [Transferring Knowledge from Structure-aware Self-attention Language Model to Sequence-to-Sequence Semantic Parsing](https://aclanthology.org/2022.coling-1.280)
  - Ran Ji, Jianmin Ji
  - **TLDR**: 自然言語処理で、文の構造を認識する研究。文の構造は、文の意味を含まない単語を含まない単語に置き換える形で表現される。文の意味を含まない単語は、文の意味を含まない単語に置き換える形で表現される。文の意味を含まない単語は、文の意味を含まない単語に置き換える形で表現される。

- [Enhancing Contextual Word Representations Using Embedding of Neighboring Entities in Knowledge Graphs](https://aclanthology.org/2022.coling-1.281)
  - Ryoko Tokuhisa, Keisuke Kawano, Akihiro Nakamura, Satoshi Koide
  - **TLDR**: 自然言語処理で、事前学習済みモデルと併用することで、事前学習済みモデルのパフォーマンスを上げる研究。事前学習済みモデルは、事前学習済みモデルと同等の構造を取っており、事前学習済みモデルの構造を学習する。事前学習済みモデルは、事前学習済みモデルと同等の構造を取っており、事前学習済みモデルの構造を学習する。

- [Generic Overgeneralization in Pre-trained Language Models](https://aclanthology.org/2022.coling-1.282)
  - Sello Ralethe, Jan Buys
  - **TLDR**: 事前学習済み言語モデルで、自然言語処理の知識を注入することで、自然言語処理の過誤を防ぐ研究。事前学習済み言語モデルは、自然言語処理の過誤を防ぐために、自然言語処理の知識を注入する。過誤を防ぐために、事前学習済み言語モデルの知識を注入する。過誤を防ぐために、事前学習済み言語モデルの知識を注入する

- [How about Time? Probing a Multilingual Language Model for Temporal Relations](https://aclanthology.org/2022.coling-1.283)
  - Tommaso Caselli, Irene Dini, Felice Dell’Orletta
  - **TLDR**: マルチ言語のモデルで、過去のイベントと未来のイベントの位置関係を推定する研究。過去のイベントと未来のイベントの位置関係を推定する際、過去のイベントと未来のイベントの位置関係を推定する。過去のイベントと未来のイベントの位置関係を推定する際、過去のイベントと未来のイベントの位置関係を推定する。

- [CogBERT: Cognition-Guided Pre-trained Language Models](https://aclanthology.org/2022.coling-1.284)
  - Xiao Ding, Bowen Chen, Li Du, Bing Qin, Ting Liu
  - **TLDR**: 事前学習済み言語モデルを、事前学習済みモデルと併用する研究。事前学習済みモデルは、事前学習済みモデルの潜在表現を認識するネットワークを構築する。このネットワークを、事前学習済みモデルと併用することで、事前学習済みモデルの潜在表現を認識する。事前学習済みモデルは、事前学習済みモデルの潜在表現を認識するネットワークを構築する。

- [Can Transformers Process Recursive Nested Constructions, Like Humans?](https://aclanthology.org/2022.coling-1.285)
  - Yair Lakretz, Théo Desbordes, Dieuwke Hupkes, Stanislas Dehaene
  - **TLDR**: 自然言語処理において、長い系列の依存を扱うTransformerが、長い系列の依存を扱う際にどう役立っているのか検証した研究。長い系列の依存は、長い系列の依存と同等の性能を発揮するが、長い系列の依存は、長い系列の依存と同等の性能を発揮する。

- [NSP-BERT: A Prompt-based Few-Shot Learner through an Original Pre-training Task —— Next Sentence Prediction](https://aclanthology.org/2022.coling-1.286)
  - Yi Sun, Yu Zheng, Chao Hao, Hangping Qiu
  - **TLDR**: 事前学習済みモデルを、事前学習済みモデルの学習に応用した研究。事前学習済みモデルは、事前学習済みモデルの学習データ(BERT)をベースに学習する。BERTは事前学習済みモデルの学習データ(BERTの学習データ)をベースに学習するが、BERTは学習データの少ないモデルで学習する。BERTは事前学習済みモデルの学習データが少なくて済むため、事前学習済みモデルの学習を行わず学習済みモデルの学習を行なっている。

- [MetaPrompting: Learning to Learn Better Prompts](https://aclanthology.org/2022.coling-1.287)
  - Yutai Hou, Hongyuan Dong, Xinghao Wang, Bohan Li, Wanxiang Che
  - **TLDR**: 事前学習済み言語モデルを、事前学習済みモデルの初期化に応用した研究。事前学習済みモデルは、事前学習済み言語モデルの初期化を学習する。事前学習済みモデルは、事前学習済み言語モデルの初期化を学習する。事前学習済み言語モデルは、事前学習済み言語モデルの初期化を学習する。事前学習済み言語モデルの初期化は、事前学習済み言語モデルの初期化を学習する。

- [Parameter-Efficient Mixture-of-Experts Architecture for Pre-trained Language Models](https://aclanthology.org/2022.coling-1.288)
  - Ze-Feng Gao, Peiyu Liu, Wayne Xin Zhao, Zhong-Yi Lu, Ji-Rong Wen
  - **TLDR**: 事前学習済み言語モデルを、マルチタスクで解く手法の提案。事前学習済み言語モデルは、各タスクのタスクの重みを共有する。タスクの重みは、タスクの重みを共有するネットワークの重みと、タスクの重みを共有するネットワークの重みを共有するネットワークの重みを共有するネットワークの重みとで構成される。

- [Pre-trained Token-replaced Detection Model as Few-shot Learner](https://aclanthology.org/2022.coling-1.289)
  - Zicheng Li, Shoushan Li, Guodong Zhou
  - **TLDR**: 事前学習済み言語モデルを、事前学習済みモデルの予測に応用した研究。事前学習済みモデルは、事前学習済みモデルの予測結果をベースに、事前学習済みモデルの予測結果をベースに学習する。事前学習済みモデルは、事前学習済みモデルの予測結果をベースに、事前学習済みモデルの予測結果をベースに学習する。事前学習済みモデルは、事前学習済みモデルの予測結果をベースに学習する。

- [Evaluating Diversity of Multiword Expressions in Annotated Text](https://aclanthology.org/2022.coling-1.290)
  - Adam Lion-Bouton, Yagmur Ozturk, Agata Savary, Jean-Yves Antoine
  - **TLDR**: マルチワードのアノテーションを評価する手法の提案。マルチワードのアノテーションは、マルチバイトの単語を含まない単語のアノテーションと、マルチバイトの単語を含まない単語のアノテーションを比較する。マルチバイトのアノテーションは、マルチバイトの単語を含まない単語に分類する手法と同等の評価が行えることを確認。

- [CausalQA: A Benchmark for Causal Question Answering](https://aclanthology.org/2022.coling-1.291)
  - Alexander Bondarenko, Magdalena Wolska, Stefan Heindorf, Lukas Blübaum, Axel-Cyrille Ngonga Ngomo, Benno Stein, Pavel Braslavski, Matthias Hagen, Martin Potthast
  - **TLDR**: 検索エンジンで使われる検索エンジンの質問に、因果関係を推定する手法の提案。因果関係は、因果関係の因果関係と同等の意味を持つが、因果関係は同等意味を持つとしている。因果関係は、因果関係の因果関係と同等意味を持つとしている。因果関係の定義は、因果関係の定義と同等意味を持つとしている。

- [MACRONYM: A Large-Scale Dataset for Multilingual and Multi-Domain Acronym Extraction](https://aclanthology.org/2022.coling-1.292)
  - Amir Pouran Ben Veyseh, Nicole Meister, Seunghyun Yoon, Rajiv Jain, Franck Dernoncourt, Thien Huu Nguyen
  - **TLDR**: 自然言語処理で、複数言語の単語を扱うタスクについて、事前学習済みモデルを追加した研究。事前学習済みモデルは、単語の意味を予測するタスクとして扱われるが、単語の意味を予測するタスクは別途学習する必要がある。事前学習済みモデルは、単語の意味を予測するタスクとして扱われる。

- [Curating a Large-Scale Motivational Interviewing Dataset Using Peer Support Forums](https://aclanthology.org/2022.coling-1.293)
  - Anuradha Welivita, Pearl Pu
  - **TLDR**: 対話システムの学習で、対話システムの評価を教師なしで行う研究。教師なしで学習する対話システムは、学習済み教師の評価と同等の評価が得られるか検証する。評価は教師なしで行うが、評価は教師なしで行う。評価は教師なしで行うが、教師なしで学習する場合は、評価の評価を教師なしで行う。

- [CCTC: A Cross-Sentence Chinese Text Correction Dataset for Native Speakers](https://aclanthology.org/2022.coling-1.294)
  - Baoxin Wang, Xingyi Duan, Dayong Wu, Wanxiang Che, Zhigang Chen, Guoping Hu
  - **TLDR**: 翻訳文の翻訳で、翻訳文の文法エラーを検出するデータセットの提案。翻訳文の文法エラーは、翻訳文の文法エラーと同等のエラーになる。翻訳文の文法エラーは、翻訳文の文法エラーと同等のエラーになる。翻訳文の文法エラーは、翻訳文の文法エラーと同等のエラーになる。

- [RealMedDial: A Real Telemedical Dialogue Dataset Collected from Online Chinese Short-Video Clips](https://aclanthology.org/2022.coling-1.295)
  - Bo Xu, Hongtong Zhang, Jian Wang, Xiaokun Zhang, Dezhi Hao, Linlin Zong, Hongfei Lin, Fenglong Ma
  - **TLDR**: 医療対話システムの開発に際して、医療対話のデータセットを構築した研究。医療対話は医療従事者と患者双方の対話で、患者のプライバシーを考慮したデータセットを構築している。医療対話は医療従事者と患者双方の対話で、患者のプライバシーは医療従事者と患者双方の対話で収集している。

- [TempoWiC: An Evaluation Benchmark for Detecting Meaning Shift in Social Media](https://aclanthology.org/2022.coling-1.296)
  - Daniel Loureiro, Aminette D’Souza, Areej Nasser Muhajab, Isabella A. White, Gabriel Wong, Luis Espinosa-Anke, Leonardo Neves, Francesco Barbieri, Jose Camacho-Collados
  - **TLDR**: 自然言語処理モデルのパフォーマンスを評価する指標の提案。既存のモデルは、自然言語処理のモデルと比較して、更新頻度が高い単語を学習する傾向がある。更新頻度が高い単語は、更新頻度が高い単語を学習するモデルに近い傾向がある。更新頻度が高い単語は、更新頻度が高い単語を学習するモデルに近い傾向がある。

- [Automatic Generation of Large-scale Multi-turn Dialogues from Reddit](https://aclanthology.org/2022.coling-1.297)
  - Daniil Huryn, William M. Hutsell, Jinho D. Choi
  - **TLDR**: 多言語の対話システムを構築する研究。投稿から10つ以上のコミュニティから、質問回答を生成する。質問回答は、質問回答の生成に使用するスコアをベースに、質問回答の生成に使用するスコアをベースに学習する。スコアは、質問回答の生成に使用するスコアをベースに、スコアをベースにスコアを算出する。

- [ConFiguRe: Exploring Discourse-level Chinese Figures of Speech](https://aclanthology.org/2022.coling-1.298)
  - Dawei Zhu, Qiusi Zhan, Zhejian Zhou, Yifan Song, Jiebin Zhang, Sujian Li
  - **TLDR**: 自然言語処理で、自然言語処理のモデルで、自然言語処理のモデルで、自然言語処理のモデルで、自然言語処理のモデルで、自然言語処理のモデルで、自然言語処理のモデルで、自然言語処理のモデルで、自然言語処理のモデルで、自然言語処理のモデルで、自然言語処理のモデルで、自然言語処理のモデルで、自然言語処理のモデルで、自然言語処理のモデルで、自然言語処理のモデルで、自然言語処理のモデルで、自然言語処理のモデルで、自然言語処理のモデルで、自然言語処理のモデルで、自然言語処理のモデルで、自然言語処理のモデルで、自然言語

- [Twitter Topic Classification](https://aclanthology.org/2022.coling-1.299)
  - Dimosthenis Antypas, Asahi Ushio, Jose Camacho-Collados, Vitor Silva, Leonardo Neves, Francesco Barbieri
  - **TLDR**: ニュースの分類を行うタスクの提案。分類は、ニュースのタイトル/ニュースソース/ニュースの関連度/関連度の3つを分類する。分類は、ニュースのタイトル/ニュースソース/関連度の3つを分類する。分類は、ニュースソース/ニュースソースの関連度/関連度の3つを分類する。

- [Layer or Representation Space: What Makes BERT-based Evaluation Metrics Robust?](https://aclanthology.org/2022.coling-1.300)
  - Doan Nam Long Vu, Nafise Sadat Moosavi, Steffen Eger
  - **TLDR**: テキスト生成において、事前学習済みモデルと同等の精度を維持できるか検証した研究。事前学習済みモデルは、入力にノイズが入る場合(単語レベルのembedding)、入力に単語レベルのembeddingが入る場合(単語レベルのembedding)、入力にノイズが入る場合(単語レベルのembedding)、入力に単語レベルのembeddingが入る場合(単語レベルのembedding)、といった条件をつける。

- [Evaluating the Performance of Transformer-based Language Models for Neuroatypical Language](https://aclanthology.org/2022.coling-1.301)
  - Duanchen Liu, Zoey Liu, Qingyun Yang, Yujing Huang, Emily Prud’hommeaux
  - **TLDR**: 発達障害を持つ人同士のコミュニケーションについて、対話形式を学習する研究。対話形式は、ASDの学習者とASDの学習者双方で作成し、ASDの学習者と学習者双方で学習する。対話形式は、ASDの学習者と学習者双方で作成する。学習者は、ASDの学習者と学習者双方で学習する。

- [TERMinator: A System for Scientific Texts Processing](https://aclanthology.org/2022.coling-1.302)
  - Elena Bruches, Olga Tikhobaeva, Yana Dementyeva, Tatiana Batura
  - **TLDR**: 自然言語処理のモデルが自然言語処理の精度にどのような影響を与えているかを調べた研究。自然言語処理は、単語の意味を認識するタスクで精度が落ちる傾向がある。そのため、単語の意味を認識するタスクで言語モデルの学習を行なっている。

- [LipKey: A Large-Scale News Dataset for Absent Keyphrases Generation and Abstractive Summarization](https://aclanthology.org/2022.coling-1.303)
  - Fajri Koto, Timothy Baldwin, Jey Han Lau
  - **TLDR**: 文書要約のモデルで、文書要約の文を複数文に分割し、文書要約の文を個別に学習する手法の提案。文書要約は文書要約の文を個別に学習するが、文書要約は文書要約の文を個別に学習する。文書要約は文書要約の文を個別に学習するが、文書要約は文書要約の文を個別に学習する。

- [Understanding Attention for Vision-and-Language Tasks](https://aclanthology.org/2022.coling-1.304)
  - Feiqi Cao, Soyeon Caren Han, Siqu Long, Changwei Xu, Josiah Poon
  - **TLDR**: 画像とテキストをAttentionで区分する研究。画像とテキストをAttentionで区分する手法は、画像とテキストの位置関係を考慮する必要がある。画像とテキストの位置関係を考慮する手法は、画像とテキストの位置関係を考慮する手法と、テキストと画像の位置関係を考慮する手法の2つに分けられている。

- [Effective Data Augmentation for Sentence Classification Using One VAE per Class](https://aclanthology.org/2022.coling-1.305)
  - Frédéric Piedboeuf, Philippe Langlais
  - **TLDR**: 画像の生成を行う際に、生成した画像をクラス分類に使う手法の提案。画像の生成は、画像のサイズを大きくするだけでなく、画像の重みを小さくする(Crop)、画像の重みを小さくする(RNN)、画像の重みを小さくする(Value-Attention)、といった3つの工夫が取られている。

- [NLG-Metricverse: An End-to-End Library for Evaluating Natural Language Generation](https://aclanthology.org/2022.coling-1.306)
  - Giacomo Frisoni, Antonella Carbonaro, Gianluca Moro, Andrea Zammarchi, Marco Avagnano
  - **TLDR**: 自然言語生成モデルの評価を、Pythonベースのモデルで行う試み。評価は、生成されたテキストを生成するモデルの評価と、生成されたテキストの評価を交互に行う。評価は、生成されたテキストの評価と、生成されたテキストの評価を交互に行う。評価は、生成されたテキストの評価と、生成されたテキストの評価を交互に行う。

- [TestAug: A Framework for Augmenting Capability-based NLP Tests](https://aclanthology.org/2022.coling-1.307)
  - Guanqun Yang, Mirazul Haque, Qiaochu Song, Wei Yang, Xueqing Liu
  - **TLDR**: 事前学習済みモデルのテストを自動化する手法の提案。事前学習済みモデルのテストは、事前学習済みモデルのテストセットを生成する。事前学習済みモデルのテストセットは、事前学習済みモデルのテストセットと同等の精度を維持できる。事前学習済みモデルのテストセットは、事前学習済みモデルのテストセットと同等の精度を維持できる。

- [KoCHET: A Korean Cultural Heritage Corpus for Entity-related Tasks](https://aclanthology.org/2022.coling-1.308)
  - Gyeongmin Kim, Jinsung Kim, Junyoung Son, Heuiseok Lim
  - **TLDR**: 自然言語処理で、自然言語処理のタスクで使われる文書をまとめた研究。文書分類のタスクで使われる文書をまとめた文書セット(KNN)、文書分類のタスクで使われる文書セット(KNN)、文書分類のタスクで使われる文書セット(KNN)、文書分類のタスクで使われる文書セット(KNN)、文書分類のタスクで使われる文書セット(KNN)の3つをまとめている。

- [MonoByte: A Pool of Monolingual Byte-level Language Models](https://aclanthology.org/2022.coling-1.309)
  - Hugo Abonizio, Leandro Rodrigues de Souza, Roberto Lotufo, Rodrigo Nogueira
  - **TLDR**: マルチ言語の学習を行えるモデルの提案。学習は学習済みモデルの学習済みモデルと同等の手法で行い、学習は学習済みモデルの学習済みモデルの学習済みモデルの学習済みモデルの学習済みモデルの学習済みモデルの学習済みモデルの学習済みモデルの学習済みモデルの学習済みモデルの学習済みモデルの学習済みモデルの学習済みモデルの学習済みモデルの学習済みモデルの学習済みモデルの学習済みモデルの学習済みモデルの学習済みモデルの学習済みモデルの学習済みモデルの学習済みモデルの学習済みモデルの学習済みモデルの学習済みモデルの学習済みモデルの学習済みモデルの学習済みモデルの学習済みモデルの

- [Wizard of Tasks: A Novel Conversational Dataset for Solving Real-World Tasks in Conversational Settings](https://aclanthology.org/2022.coling-1.310)
  - Jason Ingyu Choi, Saar Kuzi, Nikhita Vedula, Jie Zhao, Giuseppe Castellucci, Marcus Collins, Shervin Malmasi, Oleg Rokhlenko, Eugene Agichtein
  - **TLDR**: タスク分類のタスクで、事前学習済みモデルの精度が低い問題を解決するために、事前学習済みモデルの精度を上げるための研究。事前学習済みモデルは、事前学習済みモデルの精度が低い場合、タスク分類のタスクで精度が低下する傾向がある。事前学習済みモデルは、事前学習済みモデルの精度が低い場合、精度が低下する傾向がある。

- [K-MHaS: A Multi-label Hate Speech Detection Dataset in Korean Online News Comment](https://aclanthology.org/2022.coling-1.311)
  - Jean Lee, Taejun Lim, Heejun Lee, Bogeun Jo, Yangsok Kim, Heegeun Yoon, Soyeon Caren Han
  - **TLDR**: ニュースに対する不信度を調べる研究。ニュースに対する不信度を分類するデータセットを提案。ニュースに対する不信度は、ニュースのタイトル/記事/画像/動画/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽/音楽

- [Domain- and Task-Adaptation for VaccinChatNL, a Dutch COVID-19 FAQ Answering Corpus and Classification Model](https://aclanthology.org/2022.coling-1.312)
  - Jeska Buhmann, Maxime De Bruyn, Ehsan Lotfi, Walter Daelemans
  - **TLDR**: 自然言語処理で、質問回答のモデルを更新した研究。質問回答のモデルは、質問回答の分類器と同等のモデルで、質問回答の分類器は質問回答の分類器と同等のモデルで学習する。質問回答の分類器は、質問回答の分類器と同等のモデルで学習する。

- [Benchmarking Automated Clinical Language Simplification: Dataset, Algorithm, and Evaluation](https://aclanthology.org/2022.coling-1.313)
  - Junyu Luo, Junxian Lin, Chi Lin, Cao Xiao, Xinning Gui, Fenglong Ma
  - **TLDR**: 医療文書を自動翻訳する研究。医療文書を自動翻訳するモデルは、医療文書の文法知識を入力に入力する形で行われることが多いが、文法知識を入力に入力するモデルは、文法知識を入力に入力する形で行われることが多い。この文法知識を入力に入力するモデルを、医療文書の文法知識を入力に入力する形で作成している。

- [WikiHan: A New Comparative Dataset for Chinese Languages](https://aclanthology.org/2022.coling-1.314)
  - Kalvin Chang, Chenxuan Cui, Youngmin Kim, David R. Mortensen
  - **TLDR**: 英語の語彙を抽出する研究。単語の抽出は英語の語彙を抽出する形で行われるが、抽出は英語の語彙を抽出する形で行われる。単語の抽出は英語の語彙を抽出する形で行われるが、単語の抽出は英語の語彙を抽出する形で行われる。

- [Visual Recipe Flow: A Dataset for Learning Visual State Changes of Objects with Recipe Flows](https://aclanthology.org/2022.coling-1.315)
  - Keisuke Shirai, Atsushi Hashimoto, Taichi Nishimura, Hirotaka Kameko, Shuhei Kurita, Yoshitaka Ushiku, Shinsuke Mori
  - **TLDR**: 料理の画像を、画像の画像から生成する研究。画像の画像を生成する際は、画像の画像を生成する画像の画像を生成する。画像の画像生成は、画像の画像生成と画像の画像生成の2つを同時に行う。画像生成は、画像の画像生成と画像の画像生成の2つを同時に行う。

- [IMPARA: Impact-Based Metric for GEC Using Parallel Data](https://aclanthology.org/2022.coling-1.316)
  - Koki Maeda, Masahiro Kaneko, Naoaki Okazaki
  - **TLDR**: 文法誤差を評価する手法の提案。文法誤差を評価するデータセットを、文法誤差の予測に使用する。文法誤差の予測は、文法誤差の予測と文法誤差の予測を同時に行う。文法誤差の予測は、文法誤差の予測と文法誤差の予測を同時に行う。

- [Evons: A Dataset for Fake and Real News Virality Analysis and Prediction](https://aclanthology.org/2022.coling-1.317)
  - Kriste Krstovski, Angela Soomin Ryu, Bruce Kogut
  - **TLDR**: ニュースの投稿を検証するデータセット。記事タイトル/タイトル画像/投稿画像/投稿画像の画像を入力として、投稿画像の画像分類、投稿画像の画像分類、画像分類の3つを検証している。画像分類は、画像分類の精度を上げるために、画像分類の精度を上げるために、画像分類の精度を上げるために、画像分類の精度を上げるために、画像分類の精度を上げるために、画像分類の精度を上げるために、画像分類の精度を上げるために、画像分類の精度を上げるために、画像分類の精度を上げるために、画像分類の精度を上げるために、画像分類の精度を上げるために、画像分類の精度を上げるために、

- [Are Pretrained Multilingual Models Equally Fair across Languages?](https://aclanthology.org/2022.coling-1.318)
  - Laura Cabello Piqueras, Anders Søgaard
  - **TLDR**: マルチ言語モデルの適用事例を検証した研究。マルチ言語モデルは、言語の分布が異なる場合に適用されがちなグループの分布が異なることを確認。また、マルチ言語モデルは、言語の分布が異なる場合に適用されがちな分布を指摘している。

- [Possible Stories: Evaluating Situated Commonsense Reasoning under Multiple Possible Scenarios](https://aclanthology.org/2022.coling-1.319)
  - Mana Ashida, Saku Sugawara
  - **TLDR**: 自然言語処理で、あるシーンを題材に質問を行う研究。質問は、あるシーンがどういう意味か、またその意味がどういう意味かを問う。質問は、あるシーンを題材に、その意味がどういう意味かを問う。質問は、あるシーンを題材に、その意味がどういう意味かを問う。質問は、あるシーンを題材に、その意味がどういう意味かを問う。

- [DiaBiz.Kom - towards a Polish Dialogue Act Corpus Based on ISO 24617-2 Standard](https://aclanthology.org/2022.coling-1.320)
  - Marcin Oleksy, Jan Wieczorek, Dorota Drużyłowska, Julia Klyus, Aleksandra Domogała, Krzysztof Hwaszcz, Hanna Kędzierska, Daria Mikoś, Anita Wróż
  - **TLDR**: 対話システムの提案。対話システムは、質問に対する回答を自動生成する形で構築されている。質問に対する回答は、質問に対する回答と質問に対する回答の2つに分けられる。質問に対する回答は、質問に対する回答と質問に対する回答の2つに分けられる。質問に対する回答は、質問に対する回答と質問に対する回答の2つに分けられる。

- [Towards Explainable Evaluation of Language Models on the Semantic Similarity of Visual Concepts](https://aclanthology.org/2022.coling-1.321)
  - Maria Lymperaiou, George Manoliadis, Orfeas Menis Mastromichalakis, Edmund G. Dervakos, Giorgos Stamou
  - **TLDR**: 事前学習済み言語モデルの評価手法を、実装と実装の差異から検証した研究。実装は実装と同等かそれ以上の評価を行なっているが、実装は実装と同等かそれ以上の評価を行なっている。実装は実装と同等かそれ以上の評価を行なっているが、実装は実装と同等かそれ以上の評価を行なっている。

- [Establishing Annotation Quality in Multi-label Annotations](https://aclanthology.org/2022.coling-1.322)
  - Marian Marchal, Merel Scholman, Frances Yung, Vera Demberg
  - **TLDR**: マルチラベルのアクセシビリティを計算する際、各ラベルのアクセシビリティを考慮する手法の提案。ラベルの重みは、ラベルの重みとラベルの分布を考慮する。重みは、ラベル分布の分布を考慮する。ラベル分布は、ラベル分布の分布を考慮する。

- [Biographically Relevant Tweets – a New Dataset, Linguistic Analysis and Classification Experiments](https://aclanthology.org/2022.coling-1.323)
  - Michael Wiegand, Rebecca Wilm, Katja Markert
  - **TLDR**: ツリー分類のタスクで、プロフィール情報を収集する研究。プロフィールは、プロフィールから収集した情報を基に分類する。プロフィールは、プロフィールから収集した情報を基に分類する。プロフィールは、プロフィールから収集した情報を基に分類する。プロフィールは、プロフィールから収集した情報を基に分類する。

- [BECEL: Benchmark for Consistency Evaluation of Language Models](https://aclanthology.org/2022.coling-1.324)
  - Myeongjun Jang, Deuk Sin Kwon, Thomas Lukasiewicz
  - **TLDR**: 言語モデルの信頼性を評価する指標の提案。事前学習済みモデル(PLM)を評価する指標として、事前学習済みモデルのパフォーマンスを評価する指標を提案している。事前学習済みモデルは、事前学習済みモデルのパフォーマンスを上回る結果を出し得るが、事前学習済みモデルはパフォーマンスが下がるケースがある。

- [KoBEST: Korean Balanced Evaluation of Significant Tasks](https://aclanthology.org/2022.coling-1.325)
  - Myeongjun Jang, Dohyung Kim, Deuk Sin Kwon, Eric Davis
  - **TLDR**: 自然言語処理のベンチマークについて、事前学習済みモデルの評価を検証した研究。事前学習済みモデルの評価は、事前学習済みモデルの学習データ(英語)と、学習済みモデルの学習データ(日本語)のバランスを考慮する。事前学習済みモデルの評価は、事前学習済みモデルの学習データ(英語)と、学習データのバランスを考慮する。

- [A New Public Corpus for Clinical Section Identification: MedSecId](https://aclanthology.org/2022.coling-1.326)
  - Paul Landes, Kunal Patel, Sean S. Huang, Adam Webb, Barbara Di Eugenio, Cornelia Caragea
  - **TLDR**: 臨床医学文書の分類について、各文書の分類を個別に行う手法の提案。各文書の分類は、文書の各フレーズを個別に分類する形で行われる。各フレーズは、文書の各フレーズの分類結果から分類を行う。文書分類は、文書の各フレーズの分類結果から分類を行う。

- [A Data-driven Approach to Named Entity Recognition for Early Modern French](https://aclanthology.org/2022.coling-1.327)
  - Pedro Ortiz Suarez, Simon Gabay
  - **TLDR**: 自然言語処理における、古典文学における、自然言語処理の課題を解明した研究。古典文学における、自然言語処理の課題を解明する研究として、古典文学の文書を抽出するモデルを提案している。古典文学の文書を抽出するモデルは、古典文学の文書を抽出するモデルと同等の精度を維持できるが、文書の文書構造は、古典文学の文書を抽出するモデルと同等に難しい。

- [Reproducibility and Automation of the Appraisal Taxonomy](https://aclanthology.org/2022.coling-1.328)
  - Pradeesh Parameswaran, Andrew Trotman, Veronica Liesaputra, David Eyers
  - **TLDR**: 機械学習で評価を行う際に、評価の分類精度を自動で評価する研究。評価の分類精度は、評価の分類精度と同等の精度を達成できるかを検証する。評価の分類精度は、評価の分類精度と同等の精度を達成できるかを検証する。評価精度は、評価の分類精度と同等の精度を達成できるかを検証する。

- [Few-Shot Table Understanding: A Benchmark Dataset and Pre-Training Baseline](https://aclanthology.org/2022.coling-1.329)
  - Ruixue Liu, Shaozu Yuan, Aijun Dai, Lei Shen, Tiangang Zhu, Meng Chen, Xiaodong He
  - **TLDR**: テーブルの理解を強化する研究。タスクは、テーブルの予測結果を予測する。予測結果は、予測結果を予測するモデルが最もよく、予測結果を予測するモデルが最もよく、という結果。タスクは、予測結果を予測するモデルが最もよく、という結果。タスクは、予測結果を予測するモデルが最もよく、という結果。

- [Tafsir Dataset: A Novel Multi-Task Benchmark for Named Entity Recognition and Topic Modeling in Classical Arabic Literature](https://aclanthology.org/2022.coling-1.330)
  - Sajawel Ahmed, Rob van der Goot, Misbahur Rehman, Carl Kruse, Ömer Özsoy, Alexander Mehler, Gemma Roig
  - **TLDR**: 古典文学の論文を、自然言語処理でモデル化する研究。テキスト分類は、単語分類と同等の手法が取られているが、単語分類は、単語の意味を認識するだけでなく、単語の意味を予測するモデル(Bi-LSTM)を用いている。

- [Resource of Wikipedias in 31 Languages Categorized into Fine-Grained Named Entities](https://aclanthology.org/2022.coling-1.331)
  - Satoshi Sekine, Kouta Nakayama, Masako Nomoto, Maya Ando, Asuka Sumida, Koji Matsuda
  - **TLDR**: 自然言語処理で、Wikipediaの分類を自動生成する研究。Wikipediaの分類は、Wikipediaの分類器を自動生成する。自動生成は、Wikipediaの分類器を自動生成するネットワークの一種。自動生成は、Wikipediaの分類器を自動生成するネットワークの一種。自動生成は、Wikipediaの分類器を自動生成するネットワークの一種。自動生成は、Wikipediaの分類器を自動生成するネットワークの一種。

- [Accuracy meets Diversity in a News Recommender System](https://aclanthology.org/2022.coling-1.332)
  - Shaina Raza, Syed Raza Bashir, Usman Naseem
  - **TLDR**: ニュースの紹介文を生成するネットワークの提案。ニュースの紹介文を生成するEncoder/Decoderの2つから生成する。Encoderはニュースのタイトル/本文を入力とする。Decoderはニュースのタイトル/本文を入力とする。Encoderはニュースのタイトル/本文を入力とする。Decoderはニュースのタイトル/本文を入力とする。

- [Dynamic Nonlinear Mixup with Distance-based Sample Selection](https://aclanthology.org/2022.coling-1.333)
  - Shaokang Zhang, Lei Jiang, Jianlong Tan
  - **TLDR**: 教師なしの学習で、複数のサンプルを生成する研究。入力とラベルの距離を入力とし、ラベルの距離を入力とラベルの距離を入力とし、入力とラベルの距離を入力とし、ラベルの距離を入力とし、入力とラベルの距離を入力とし、入力とラベルの距離を入力とし、入力とラベルの距離を入力とし、入力とラベルの距離を入力とし、入力とラベルの距離を入力とし、入力とラベルの距離を入力とし、入力とラベルの距離を入力とし、入力とラベルの距離を入力とし

- [MultiCoNER: A Large-scale Multilingual Dataset for Complex Named Entity Recognition](https://aclanthology.org/2022.coling-1.334)
  - Shervin Malmasi, Anjie Fang, Besnik Fetahu, Sudipta Kar, Oleg Rokhlenko
  - **TLDR**: 自然言語処理で、Entityの分類を行う際に、事前学習済み言語モデルを用いた研究。Entityの分類は、Entityの分類器を用いた分類モデルと、Entityの分類器を用いた分類モデルの2つを検証。Entityの分類器は、Entityの分類器と同等の精度を維持できるか検証している。

- [Extracting a Knowledge Base of COVID-19 Events from Social Media](https://aclanthology.org/2022.coling-1.335)
  - Shi Zong, Ashutosh Baheti, Wei Xu, Alan Ritter
  - **TLDR**: 自然災害や異常気象に対する知識ベースの提案。ニュースから、異常気象に関するニュースを抽出するBERTベースのクラスタリングモデルを自動生成する。BERTベースのクラスタリングモデルは、異常気象に関するニュースを抽出する際、ニュースの更新頻度を予測する。

- [Accounting for Language Effect in the Evaluation of Cross-lingual AMR Parsers](https://aclanthology.org/2022.coling-1.336)
  - Shira Wein, Nathan Schneider
  - **TLDR**: マルチ言語のAverageの評価手法を、文レベルの人間評価と、文レベルの言語評価の2つに分けて比較した研究。文レベルの評価は、文単位の評価と、文単位の評価の2つに分けて行う。文単位の評価は、文単位の評価と、文単位の評価の2つに分けて行う。

- [QSTS: A Question-Sensitive Text Similarity Measure for Question Generation](https://aclanthology.org/2022.coling-1.337)
  - Sujatha Das Gollapalli, See-Kiong Ng
  - **TLDR**: 質問とテキストの類似度を計測する研究。質問とテキストの類似度は、質問とテキストの類似度との関係性を計測する指標として使用される。質問とテキストの類似度は、質問とテキストの類似度との関係性を計測する指標として使用される。

- [Noun-MWP: Math Word Problems Meet Noun Answers](https://aclanthology.org/2022.coling-1.338)
  - Taehun Cha, Jaeheun Jung, Donghun Lee
  - **TLDR**: 自然言語処理で、単語を入力とするモデルの提案。単語は入力となるテキストから抽出されるが、入力は単語の単語ベクトルから生成される。単語ベクトルは入力テキストから生成されるが、入力テキストは入力テキストから生成される。入力テキストは入力テキストから生成される。

- [ViNLI: A Vietnamese Corpus for Studies on Open-Domain Natural Language Inference](https://aclanthology.org/2022.coling-1.339)
  - Tin Van Huynh, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen
  - **TLDR**: 自然言語処理のモデルを評価する研究。自然言語処理モデルは、自然言語処理のモデルと同等の精度を維持する必要がある。そのため、自然言語処理モデルの作成に際して、自然言語処理モデルの特性を考慮した研究が必要である。

- [InferES : A Natural Language Inference Corpus for Spanish Featuring Negation-Based Contrastive and Adversarial Examples](https://aclanthology.org/2022.coling-1.340)
  - Venelin Kovatchev, Mariona Taulé
  - **TLDR**: 自然言語処理のモデルで、事前学習済みモデルの精度を測る研究。事前学習済みモデルは、事前学習済みモデルの精度を測るためのモデルとして使用されることが多いが、事前学習済みモデルは、事前学習済みモデルの精度を測るためのモデルとして使用されることが多い。事前学習済みモデルは、事前学習済みモデルの精度を測るためのモデルとして使用されることが多い。

- [ParaZh-22M: A Large-Scale Chinese Parabank via Machine Translation](https://aclanthology.org/2022.coling-1.341)
  - Wenjie Hao, Hongfei Xu, Deyi Xiong, Hongying Zan, Lingling Mu
  - **TLDR**: 自然言語処理で、複数文の翻訳から文単位のparaphraseを生成する研究。文単位のparaphraseは、翻訳文の文長が短い場合、翻訳文の文長が長い場合、翻訳文の文長が長い場合に使用する。文単位のparaphraseは、翻訳文の文長が短い場合に使用する。

- [ESimCSE: Enhanced Sample Building Method for Contrastive Learning of Unsupervised Sentence Embedding](https://aclanthology.org/2022.coling-1.342)
  - Xing Wu, Chaochen Gao, Liangjun Zang, Jizhong Han, Zhongyuan Wang, Songlin Hu
  - **TLDR**: 教師なし学習で、文の長さを変えた研究。入力文をTransformerで学習し、TransformerのEncoderで長さを変えた文を入力文に貼り付ける。TransformerのEncoderは、長さの異なる文を入力文に貼り付ける際に、長さの異なる文を入力文に貼り付ける。長さを変えた文は、長さの長さを学習するTransformerのEncoderで学習する。

- [Measuring Robustness for NLP](https://aclanthology.org/2022.coling-1.343)
  - Yu Yu, Abdul Rafae Khan, Jia Xu
  - **TLDR**: 自然言語処理モデルの品質を評価する手法の提案。精度はモデルの信頼性(精度が高いと学習データに依存しない)、信頼性はモデルの信頼性(精度が高いと学習データに依存しない)、信頼性はモデルの信頼性(精度が高いと学習データに依存しない)、信頼性はモデルの信頼性(精度が高いと学習データに依存しない)、信頼性はモデルの信頼性(精度が高いと学習データに依存しない)を評価する。

- [CSL: A Large-scale Chinese Scientific Literature Dataset](https://aclanthology.org/2022.coling-1.344)
  - Yudong Li, Yuqing Zhang, Zhe Zhao, Linlin Shen, Weijie Liu, Weiquan Mao, Hui Zhang
  - **TLDR**: 自然言語処理で、自然言語処理の論文を抽出する研究。論文のタイトル、論文の概要、論文の関連する分野、論文の関連する論文の順番で抽出を行う。抽出は、論文のタイトル、論文の概要、関連する分野の順番で行う。抽出は、論文のタイトル、関連する分野の順番で行う。

- [Singlish Message Paraphrasing: A Joint Task of Creole Translation and Text Normalization](https://aclanthology.org/2022.coling-1.345)
  - Zhengyuan Liu, Shikang Ni, Ai Ti Aw, Nancy F. Chen
  - **TLDR**: 自然言語処理で、英語を英語に置き換える研究。英語を英語に置き換える際に、単語/文の正規化、また文の編集を行う3つのタスクを提案している。単語/文の正規化は、単語/文の編集は、文の編集は、文の編集は、文の編集は、文の編集は、それぞれ行う。

- [CINO: A Chinese Minority Pre-trained Language Model](https://aclanthology.org/2022.coling-1.346)
  - Ziqing Yang, Zihang Xu, Yiming Cui, Baoxin Wang, Min Lin, Dayong Wu, Zhigang Chen
  - **TLDR**: 多言語の事前学習済み言語モデルの提案。事前学習済み言語モデルは、通常の事前学習済み言語モデルと同等の性能が得られるが、事前学習済み言語モデルは学習済み言語モデルと同等の性能が得られない。そのため、事前学習済み言語モデルをベースに、事前学習済み言語モデルをベースに学習する。

- [One Word, Two Sides: Traces of Stance in Contextualized Word Representations](https://aclanthology.org/2022.coling-1.347)
  - Aina Garí Soler, Matthieu Labeau, Chloé Clavel
  - **TLDR**: 文の表現が、人の意見に左右されないか調査した研究。文の表現は、人の意見に左右されないかを検証した。文の表現は、人の意見に左右されないかを検証した。文の表現は、人の意見に左右されないかを検証した。

- [Prepositions Matter in Quantifier Scope Disambiguation](https://aclanthology.org/2022.coling-1.348)
  - Aleksander Leczkowski, Justyna Grudzińska, Manuel Vargas Guzmán, Aleksander Wawer, Aleksandra Siemieniuk
  - **TLDR**: 自然言語処理モデルで、事前学習済みモデルと同等の効果を上げるために、事前学習済みモデルの潜在表現を組み合わせて学習する研究。事前学習済みモデルは、事前学習済みモデルと同等の効果を上げることができる。事前学習済みモデルは、事前学習済みモデルと同等の効果を上げることができる。

- [Modelling Commonsense Properties Using Pre-Trained Bi-Encoders](https://aclanthology.org/2022.coling-1.349)
  - Amit Gajbhiye, Luis Espinosa-Anke, Steven Schockaert
  - **TLDR**: 自然言語処理で、自然言語処理の特性を学習する際の手法を調査した研究。通常の言語モデルは、自然言語処理の特性を学習する際は、学習済みモデルと同等の学習データが必要だが、このデータセットを学習する際は、学習済みモデルの学習データと同等の学習データが必要。

- [COIN – an Inexpensive and Strong Baseline for Predicting Out of Vocabulary Word Embeddings](https://aclanthology.org/2022.coling-1.350)
  - Andrew Schneider, Lihong He, Zhijia Chen, Arjun Mukherjee, Eduard Dragut
  - **TLDR**: 自然言語処理で、単語分散表現を予測する手法の提案。単語分散表現は、単語の語彙数が多い場合に予測が困難になるため、単語分散表現を単語分散表現と同等精度で予測する。単語分散表現は、単語分散表現の語彙数が多い場合に予測が困難になるため、単語分散表現を単語分散表現と同等精度で予測する。

- [DynGL-SDP: Dynamic Graph Learning for Semantic Dependency Parsing](https://aclanthology.org/2022.coling-1.351)
  - Bin Li, Miao Gao, Yunlong Fan, Yikemaiti Sataer, Zhiqiang Gao, Yaocheng Gui
  - **TLDR**: 文書構造とグラフ表現を学習する研究。文書構造は固有表現と同等の扱いが難しいため、文書構造を学習する際は文書構造を学習する。文書構造は文書構造と同等の扱いが難しいため、文書構造を学習する際は文書構造を学習する。文書構造は文書構造と同等の扱いが難しいため、文書構造を学習する際は文書構造を学習する。

- [Knowledge Is Flat: A Seq2Seq Generative Framework for Various Knowledge Graph Completion](https://aclanthology.org/2022.coling-1.352)
  - Chen Chen, Yufei Wang, Bing Li, Kwok-Yan Lam
  - **TLDR**: 文書構造をGraph Completionで表現する研究。文書構造をGraph Completionで表現する手法は、文書構造を表現する文書構造と同等精度を達成できるが、文書構造を表現する文書構造を表現する文書構造と同等精度を達成できない。文書構造を表現する文書構造を、文書構造を表現する文書構造に置き換える手法を提案

- [Modelling Frequency, Attestation, and Corpus-Based Information with OntoLex-FrAC](https://aclanthology.org/2022.coling-1.353)
  - Christian Chiarcos, Elena-Simona Apostol, Besim Kabashi, Ciprian-Octavian Truică
  - **TLDR**: 自然言語処理における、各単語の意味を、各単語の意味に変換する手法の提案。単語の意味を、各単語の意味に変換する手法を、各単語の意味を、各単語の意味に変換する手法を、各単語の意味を、各単語の意味を、各単語の意味を、各単語の意味を、各単語の意味を、各単語の意味を、各単語の意味を、各単語の意味を、各単語の意味を、各単語の意味を、各単語の意味を、各単語の意味を、各単語の意味を、各単語の意味を、各単語の意味を、各単語の意味

- [Contrast Sets for Stativity of English Verbs in Context](https://aclanthology.org/2022.coling-1.354)
  - Daniel Chen, Alexis Palmer
  - **TLDR**: 自然言語処理で、学習済みモデルの学習済みモデルと比較して、学習済みモデルの学習率が近いモデルを学習する手法の提案。学習済みモデルは学習済みモデルの学習率を上回るが、学習済みモデルは学習率が下がる。学習済みモデルは学習率が下がるが、学習済みモデルは学習率が上がる。

- [Multilingual and Multimodal Topic Modelling with Pretrained Embeddings](https://aclanthology.org/2022.coling-1.355)
  - Elaine Zosa, Lidia Pivovarova
  - **TLDR**: マルチモーダルで多言語のデータセットを構築した研究。マルチモーダルで多言語のデータセットを構築し、マルチモーダルで多言語のデータセットを構築する。マルチモーダルで多言語のデータセットを構築する際、マルチモーダルで多言語のデータセットを構築する。マルチモーダルで多言語のデータセットを構築する際、マルチモーダルで多言語のデータセットを構築する。

- [Zero-shot Script Parsing](https://aclanthology.org/2022.coling-1.356)
  - Fangzhou Zhai, Vera Demberg, Alexander Koller
  - **TLDR**: 自然言語処理で、事前学習済みの知識を活用する研究。事前学習済み知識は、通常の学習済み知識と同等の効果を発揮するが、事前学習済み知識は学習済み知識と同等の効果を発揮する。事前学習済み知識は、事前学習済み知識の学習に使用する。事前学習済み知識は、事前学習済み知識の学習に使用する。事前学習済み知識は、事前学習済み知識の学習に使用する。事前学習済み知識は、事前学習済み知識の学習に使用する。事前学習済み知識は、事前学習済み知識の学習に使用する。事前学習済み知識は、事前学習済み知識の学習に使用する。事前学習

- [Word Sense Disambiguation with Knowledge-Enhanced and Local Self-Attention-based Extractive Sense Comprehension](https://aclanthology.org/2022.coling-1.357)
  - Guobiao Zhang, Wenpeng Lu, Xueping Peng, Shoujin Wang, Baoshuo Kan, Rui Yu
  - **TLDR**: 自然言語処理で、文中の単語を単語分類器で抽出する研究。単語分類器は単語の意味を予測するモデルで、単語の意味を予測する単語分類器は単語分類器のモデルで、単語分類器は単語分類器のモデルで学習する。単語分類器は単語分類器のモデルと同等の精度を達成。

- [A Novel Multi-Task Learning Approach for Context-Sensitive Compound Type Identification in Sanskrit](https://aclanthology.org/2022.coling-1.358)
  - Jivnesh Sandhan, Ashish Gupta, Hrishikesh Terdalkar, Tushar Sandhan, Suvendu Samanta, Laxmidhar Behera, Pawan Goyal
  - **TLDR**: 単語の意味を抽出するタスクの提案。単語の意味を抽出するタスクは、単語の意味を抽出するタスクと、単語の意味を抽出するタスクの2つに分けて行う。単語の意味を抽出するタスクは、単語の意味を抽出するタスクと、単語の意味を抽出するタスクの2つに分けて行う。

- [Testing Large Language Models on Compositionality and Inference with Phrase-Level Adjective-Noun Entailment](https://aclanthology.org/2022.coling-1.359)
  - Lorenzo Bertolini, Julie Weeds, David Weir
  - **TLDR**: 文の文構造を、文の文構造を学習するモデルで学習する研究。文の文構造は、文の文構造と文の文構造を結合する形で学習する。文の文構造は、文の文構造と文の文構造を結合する形で学習する。文の文構造は、文の文構造と文の文構造を結合する形で学習する。文の文構造は、文の文構造と文の文構造を結合する形で学習する。

- [Does BERT Recognize an Agent? Modeling Dowty’s Proto-Roles with Contextual Embeddings](https://aclanthology.org/2022.coling-1.360)
  - Mattia Proietti, Gianluca Lebani, Alessandro Lenci
  - **TLDR**: 文中の単語を、文法知識を基にモデル化する研究。文法知識は、文法知識の生成に使われる単語の単語ベクトルから生成される。単語ベクトルは、単語の意味を表現する単語ベクトルとして使われる。単語ベクトルは、単語ベクトルの意味を表現する単語ベクトルとして生成される。

- [Towards Structure-aware Paraphrase Identification with Phrase Alignment Using Sentence Encoders](https://aclanthology.org/2022.coling-1.361)
  - Qiwei Peng, David Weir, Julie Weeds
  - **TLDR**: 文の意味推定を行う際に、文の意味推定を文の意味推定と同等のモデルで行う研究。文の意味推定は、文の意味推定と同等のモデルで行うが、文の意味推定は文の意味推定と同等のモデルで行う。文の意味推定は、文の意味推定と同等のモデルで行う。

- [CILex: An Investigation of Context Information for Lexical Substitution Methods](https://aclanthology.org/2022.coling-1.362)
  - Sandaru Seneviratne, Elena Daskalaki, Artem Lenskiy, Hanna Suominen
  - **TLDR**: 自然言語処理で、文中の単語を単語替わりに変換する手法の提案。文中の単語を単語替わりに変換する際、文中の単語を単語分散表現として使用する。単語分散表現は、文中の単語を単語分散表現として使用する。単語分散表現は、文中の単語分散表現をベースに、文中の単語分散表現を単語分散表現として使用する。

- [Emotion Enriched Retrofitted Word Embeddings](https://aclanthology.org/2022.coling-1.363)
  - Sapan Shah, Sreedhar Reddy, Pushpak Bhattacharyya
  - **TLDR**: 自然言語処理で、感情を表現するembeddingを学習する研究。学習済みのembeddingを、学習済みのembeddingに置き換える。学習済みのembeddingは、感情を表現する潜在表現を学習する。学習済みのembeddingは、感情表現を表現する潜在表現の重みを更新する。

- [Metaphor Detection via Linguistics Enhanced Siamese Network](https://aclanthology.org/2022.coling-1.364)
  - Shenglong Zhang, Ying Liu
  - **TLDR**: 文脈の類似度を計測する手法の提案。単語の意味と類似度を計測するMIPと、単語の意味と類似度を計測するSPVを組み合わせて計測を行う。SPVは単語の意味と類似度を計測するが、MIPは単語の意味と類似度を計測する。

- [Fast and Accurate End-to-End Span-based Semantic Role Labeling as Word-based Graph Parsing](https://aclanthology.org/2022.coling-1.365)
  - Shilin Zhou, Qingrong Xia, Zhenghua Li, Yu Zhang, Yu Hong, Min Zhang
  - **TLDR**: 文単位の文列を、文単位の文列に置き換える研究。文単位の文列は、文単位の文列と同等の構造を取っており、文単位の文列を入力とする。文単位の文列は、文単位の文列と同等に扱えるよう、文単位の文列を入力とする文列を追加する。文単位の文列は、文単位の文列を入力とする文列と同等に扱えるよう、文単位の文列を入力とする文列を追加する。

- [Unsupervised Lexical Substitution with Decontextualised Embeddings](https://aclanthology.org/2022.coling-1.366)
  - Takashi Wada, Timothy Baldwin, Yuji Matsumoto, Jey Han Lau
  - **TLDR**: 文の代替表現を学習する研究。文の意味を予測するだけでなく、文の意味を予測する潜在表現を学習する。文の意味を予測する潜在表現は、文の意味を予測する潜在表現と、文の意味を予測する潜在表現の2つに分けている。文の意味を予測する潜在表現は、文の意味を予測する潜在表現と、文の意味を予測する潜在表現の2つに分けている。

- [Transparent Semantic Parsing with Universal Dependencies Using Graph Transformations](https://aclanthology.org/2022.coling-1.367)
  - Wessel Poelman, Rik van Noord, Johan Bos
  - **TLDR**: 自然言語処理で使われるDiscourse Representation Theoryを、文関係の表現に適用した研究。文関係の表現を、文関係の表現に変換する。文関係の表現は、文関係の表現を表現する文関係の表現に置き換える。文関係の表現は、文関係の表現を表現する文関係の表現に置き換える。

- [Multilingual Word Sense Disambiguation with Unified Sense Representation](https://aclanthology.org/2022.coling-1.368)
  - Ying Su, Hongming Zhang, Yangqiu Song, Tong Zhang
  - **TLDR**: マルチ言語のWord Sense Disambiguationを、教師なしで行う研究。教師なしで学習する単語を、教師なしで学習するモデルをベースに、教師なしで学習するモデルをベースに学習する。教師なしで学習するモデルは、単語の意味を予測するモデルと同等の精度を維持できる。

- [A Transition-based Method for Complex Question Understanding](https://aclanthology.org/2022.coling-1.369)
  - Yu Xia, Wenbin Jiang, Yajuan Lyu, Sujian Li
  - **TLDR**: 複雑な質問に対する解釈性を検証した研究。質問の解釈性を検証する際、解釈性を検証する際は解釈性を検証するモデルを用い、解釈性を検証するモデルは解釈性を検証するモデルと同等かそれ以上のモデルを用いるべきとしている。

- [Semantic Role Labeling as Dependency Parsing: Exploring Latent Tree Structures inside Arguments](https://aclanthology.org/2022.coling-1.370)
  - Yu Zhang, Qingrong Xia, Shilin Zhou, Yong Jiang, Guohong Fu, Min Zhang
  - **TLDR**: 自然言語処理における、文構造の表現を強化する研究。文構造は固有表現の表現として扱われがちだが、文構造を表現する表現は固有表現の表現として扱われる。固有表現表現は、文構造の表現を表現する表現として扱われる。固有表現表現は、文構造の表現を表現する表現として扱われる。

- [Noisy Label Regularisation for Textual Regression](https://aclanthology.org/2022.coling-1.371)
  - Yuxia Wang, Timothy Baldwin, Karin Verspoor
  - **TLDR**: ニューラルネットのモデルで、ノイズのあるラベルを検出する研究。ノイズのあるラベルは学習時に学習するが、ノイズのあるラベルは学習時に学習する。ノイズのあるラベルは学習時に学習するが、ノイズのあるラベルは学習時に学習する。ノイズのあるラベルは学習時に学習する。

- [Detecting Suicide Risk in Online Counseling Services: A Study in a Low-Resource Language](https://aclanthology.org/2022.coling-1.372)
  - Amir Bialer, Daniel Izmaylov, Avi Segal, Oren Tsur, Yossi Levi-Belz, Kobi Gal
  - **TLDR**: 自殺を防ぐモデルの提案。事前学習済み言語モデル(PLM)と、事前学習済みモデルの2つを組み合わせて、自動生成したモデルを作成する。事前学習済みモデルは、事前学習済みモデルの予測精度を上げるための事前学習済みモデルの役割を果たしている。

- [Does Meta-learning Help mBERT for Few-shot Question Generation in a Cross-lingual Transfer Setting for Indic Languages?](https://aclanthology.org/2022.coling-1.373)
  - Aniruddha Roy, Rupak Kumar Thakur, Isha Sharma, Ashim Gupta, Amrith Krishna, Sudeshna Sarkar, Pawan Goyal
  - **TLDR**: マルチタスクのQAモデルを、事前学習済みモデルに組み込んだ研究。事前学習済みモデルは、事前学習済み言語モデルの学習済みモデルと同等の精度を維持できるが、事前学習済み言語モデルは学習済み言語モデルの学習済み言語モデルと同等の精度を維持できない。そのため、事前学習済み言語モデルを事前学習済み言語モデルに組み込んだモデルに組み込んだ研究。

- [Revisiting Syllables in Language Modelling and Their Application on Low-Resource Machine Translation](https://aclanthology.org/2022.coling-1.374)
  - Arturo Oncevay, Kervy Dante Rivas Rojas, Liz Karen Chavez Sanchez, Roberto Zariquiey
  - **TLDR**: 自然言語処理で、単語を扱う際に、単語の重みを加味したモデルを提案。単語の重みは、単語の重みと同等か、単語の重みは単語の重みと同等か、という2つの観点から検証している。単語の重みは、単語の重みと同等か、という観点から検証している。

- [Aligning Multilingual Embeddings for Improved Code-switched Natural Language Understanding](https://aclanthology.org/2022.coling-1.375)
  - Barah Fazili, Preethi Jyothi
  - **TLDR**: マルチ言語のモデルで、文の表現をマルチドメインに拡張する研究。単語の表現をマルチドメインに拡張する際、単語の表現を同じドメインに拡張する(単語の表現を同じドメインに拡張する)ことで、言語間の類似度を調整する。言語間の類似度を調整する手法は、言語モデルの学習に有効な手法といえる。

- [Fashioning Local Designs from Generic Speech Technologies in an Australian Aboriginal Community](https://aclanthology.org/2022.coling-1.376)
  - Éric Le Ferrand, Steven Bird, Laurent Besacier
  - **TLDR**: 自然言語処理で、言語モデルの役割を再認識する研究。言語モデルは、自然言語処理の枠組みで使われることが多いが、言語モデルの役割は、言語モデルの学習と、言語モデルの学習を同時に行うという点にある。学習は、言語モデルの学習と、言語モデルの学習を交互に行う形で行う。

- [Few-Shot Pidgin Text Adaptation via Contrastive Fine-Tuning](https://aclanthology.org/2022.coling-1.377)
  - Ernie Chang, Jesujoba O. Alabi, David Ifeoluwa Adelani, Vera Demberg
  - **TLDR**: 多言語の対話システムで、事前学習済み言語モデルを用いた事前学習手法の提案。事前学習済み言語モデルは、事前学習済み言語モデルの学習データと、事前学習済み言語モデルの学習データの組み合わせで学習を行う。事前学習済み言語モデルは、事前学習済み言語モデルの学習データと学習データの組み合わせで学習を行う。

- [Penalizing Divergence: Multi-Parallel Translation for Low-Resource Languages of North America](https://aclanthology.org/2022.coling-1.378)
  - Garrett Nicolai, Changbing Yang, Miikka Silfverberg
  - **TLDR**: マルチペアの翻訳において、翻訳モデルの学習データが少ない場合に、翻訳モデルの学習率を上げるために、翻訳モデルの学習率を上げるために、翻訳モデルの学習率を上げるために、翻訳モデルの学習率を上げるために、翻訳モデルの学習率を上げるために、翻訳モデルの学習率を上げるために、学習率を上げるために、学習率を上げるために、学習率を上げるために、学習率を上げるために、学習率を上げるために、学習率を上げるために、学習率を上げるために、学習率を上げるために、学習率を上げるために、学習率を上げるために、学習率を上げるために、学習率を上げるために、学習率を上げ

- [Assessing Digital Language Support on a Global Scale](https://aclanthology.org/2022.coling-1.379)
  - Gary F. Simons, Abbey L. L. Thomas, Chad K. K. White
  - **TLDR**: デジタル環境における言語サポートの評価手法の提案。クラウドソーシングやニュース配信など、デジタル環境の活用方法について、各ツールの紹介から評価を行う。評価手法は、クラウドソーシングの紹介から、ニュース配信の紹介までを対象としている。

- [Persian Natural Language Inference: A Meta-learning Approach](https://aclanthology.org/2022.coling-1.380)
  - Heydar Soudani, Mohammad Hassan Mojab, Hamid Beigy
  - **TLDR**: 自然言語処理で、異なる言語のタスクを学習する手法の提案。タスクの学習は、タスクの学習済み言語を学習する形で行う。タスクは、タスクの学習済み言語のタスクと、タスクの学習済み言語のタスクのタスクを区別する形で行う。タスクは、タスクの学習済み言語のタスクと、タスクの学習済み言語のタスクを区別する形で行う。

- [Global Readiness of Language Technology for Healthcare: What Would It Take to Combat the Next Pandemic?](https://aclanthology.org/2022.coling-1.381)
  - Ishani Mondal, Kabir Ahuja, Mohit Jain, Jacki O’Neill, Kalika Bali, Monojit Choudhury
  - **TLDR**: 自然言語処理の技術について、現状の状況をまとめたサーベイ。英語、中国語、インドネシア語、インドネシア語、インドネシア語、インドネシア語、インドネシア語、インドネシア語、インドネシア語、インドネシア語、インドネシア語、インドネシア語、インドネシア語、インドネシア語、インドネシア語、インドネシア語、インドネシア語、インドネシア語、インドネシア語、インドネシア語、インドネシア語、インドネシア語、インドネシア語、インドネシア語、インドネシア語、インドネシア語、インドネシア語、

- [Adapting Pre-trained Language Models to African Languages via Multilingual Adaptive Fine-Tuning](https://aclanthology.org/2022.coling-1.382)
  - Jesujoba O. Alabi, David Ifeoluwa Adelani, Marius Mosbach, Dietrich Klakow
  - **TLDR**: マルチ言語の事前学習済みモデルを、事前学習済み言語に適用する研究。事前学習済み言語は、事前学習済み言語の単語をベースに学習する。単語ベースのモデルは、事前学習済み言語の単語をベースに学習する。事前学習済み言語は、事前学習済み言語の単語をベースに学習する。事前学習済み言語は、事前学習済み言語の単語をベースに学習する。

- [Noun Class Disambiguation in Runyankore and Related Languages](https://aclanthology.org/2022.coling-1.383)
  - Joan Byamugisha
  - **TLDR**: 自然言語処理で、同じクラスの単語を学習する研究。単語のクラスは、クラスの名称と同等/異なり、クラスの単語は同じクラスの単語と同等/異なり、という2つのベクトルで学習する。ベクトルは、クラスの名称と同等/異なり、という2つのベクトルで学習する。

- [Improving Low-resource RRG Parsing with Cross-lingual Self-training](https://aclanthology.org/2022.coling-1.384)
  - Kilian Evang, Laura Kallmeyer, Jakub Waszczuk, Kilu von Prince, Tatiana Bladier, Simon Petitjean
  - **TLDR**: 低リソース言語の学習を、学習済み言語のモデルで行う研究。学習済み言語のモデルは、学習済み言語の単語を学習データとして使用し、学習データの分布を学習する。学習データは、学習済み言語の単語分布を学習データとして使用する。学習データは、学習済み言語の単語分布を学習データとして使用する。

- [A Simple and Effective Method to Improve Zero-Shot Cross-Lingual Transfer Learning](https://aclanthology.org/2022.coling-1.385)
  - Kunbo Ding, Weijie Liu, Yuejian Fang, Weiquan Mao, Zhe Zhao, Tao Zhu, Haoyan Liu, Rong Tian, Yiren Chen
  - **TLDR**: マルチ言語の分散表現を学習する研究。学習済み言語の分散表現を、学習済み言語の分散表現に置き換える。分散表現は、学習済み言語の分散表現を学習する際の重みを考慮する。分散表現は、学習済み言語の分散表現を学習する際の重みを考慮する。

- [Towards Multi-Sense Cross-Lingual Alignment of Contextual Embeddings](https://aclanthology.org/2022.coling-1.386)
  - Linlin Liu, Thien Hai Nguyen, Shafiq Joty, Lidong Bing, Luo Si
  - **TLDR**: マルチ言語の単語分散表現を学習する研究。単語分散表現は、単語の意味を予測する(単語の意味を予測する)ために、単語の意味を予測する単語分散表現を使用。単語分散表現は、単語の意味を予測する単語分散表現を使用。単語分散表現は、単語分散表現の学習時に使用する。

- [How to Parse a Creole: When Martinican Creole Meets French](https://aclanthology.org/2022.coling-1.387)
  - Ludovic Mompelat, Daniel Dakota, Sandra Kübler
  - **TLDR**: 文の分散表現を学習する手法の提案。文の分散表現は、文の分散表現を学習するモデルで学習する。分散表現は、文の分散表現を学習するモデルで学習する。分散表現は、文の分散表現を学習するモデルで学習する。分散表現は、文の分散表現を学習するモデルで学習する。

- [Byte-based Multilingual NMT for Endangered Languages](https://aclanthology.org/2022.coling-1.388)
  - Mengjiao Zhang, Jia Xu
  - **TLDR**: マルチ言語の翻訳で、単語分散表現を導入した研究。単語分散表現は、単語の分散表現を予測するモデルで、単語分散表現は単語分散表現の予測をベースにしている。単語分散表現は、単語分散表現の予測をベースにしている。単語分散表現は、単語分散表現の予測をベースにしている。

- [BRCC and SentiBahasaRojak: The First Bahasa Rojak Corpus for Pretraining and Sentiment Analysis Dataset](https://aclanthology.org/2022.coling-1.389)
  - Nanda Putri Romadhona, Sin-En Lu, Bo-Han Lu, Richard Tzong-Han Tsai
  - **TLDR**: 言語のコード変換を学習する研究。学習済み言語モデルを、学習済み言語モデルの学習済み言語モデルに組み込む形で学習する。学習済み言語モデルは、言語の入力を言語モデルの学習済み言語に置き換える形で学習する。言語モデルは、言語の入力を言語モデルの学習済み言語に置き換える形で学習する。

- [WordNet-QU: Development of a Lexical Database for Quechua Varieties](https://aclanthology.org/2022.coling-1.390)
  - Nelsi Melgarejo, Rodolfo Zevallos, Hector Gomez, John E. Ortega
  - **TLDR**: 自然言語処理のモデルで、Quechuaを検索する際の単語のスパース数を算出する研究。スパース数は、単語の意味を表す単語のスパース数(単語の意味を表す単語のスパース数)と、単語の意味を表す単語のスパース数(単語の意味を表す単語のスパース数)を算出する。スパース数は、単語の意味を表す単語のスパース数を算出する。

- [When the Student Becomes the Master: Learning Better and Smaller Monolingual Models from mBERT](https://aclanthology.org/2022.coling-1.391)
  - Pranaydeep Singh, Els Lefever
  - **TLDR**: マルチ言語のモデルを蒸留する研究。蒸留は蒸留器の重みを蒸留器の重みに変換する形で行う。蒸留器は蒸留器の重みを蒸留器の重みに変換する形で行う。蒸留器は蒸留器の重みを蒸留器の重みに変換する形で行う。蒸留器は蒸留器の重みを蒸留器の重みに変換する形で行う。

- [Zero-shot Disfluency Detection for Indian Languages](https://aclanthology.org/2022.coling-1.392)
  - Rohit Kundu, Preethi Jyothi, Pushpak Bhattacharyya
  - **TLDR**: 音声認識で、言語の偏りを検出する研究。学習済み言語の翻訳モデルをベースに、翻訳モデルの学習済み言語をベースに学習する。学習済み言語の翻訳モデルは、翻訳モデルの学習済み言語をベースに学習する。翻訳モデルは、翻訳モデルの学習済み言語をベースに学習する。

- [Evaluating Word Embeddings in Extremely Under-Resourced Languages: A Case Study in Bribri](https://aclanthology.org/2022.coling-1.393)
  - Rolando Coto-Solano
  - **TLDR**: 自然言語処理で、単語分散表現を学習する研究。単語分散表現は、単語の類似度(類似度=単語の重み)、単語分散表現の重み(単語分散表現の重み)、単語分散表現の重み(単語分散表現の重み)、単語分散表現の重み(単語分散表現の重み)を学習する。単語分散表現は、単語分散表現の重みを学習するモデルで学習する。

- [Applying Natural Annotation and Curriculum Learning to Named Entity Recognition for Under-Resourced Languages](https://aclanthology.org/2022.coling-1.394)
  - Valeriy Lobov, Alexandra Ivoylova, Serge Sharoff
  - **TLDR**: 教師なし学習で、学習済み言語のモデルを学習する研究。学習済み言語のモデルは学習済み言語のモデルと同等の性能が得られるが、学習済み言語のモデルは学習済み言語のモデルと同等の性能が得られない。そのため、学習済み言語のモデルを学習する際は、学習済み言語のモデルを学習するモデルと同等の性能が得られるようにする。

- [Taking Actions Separately: A Bidirectionally-Adaptive Transfer Learning Method for Low-Resource Neural Machine Translation](https://aclanthology.org/2022.coling-1.395)
  - Xiaolin Xing, Yu Hong, Minhan Xu, Jianmin Yao, Guodong Zhou
  - **TLDR**: 低リソース言語を学習するTransformerを、事前学習済みモデルに転移する研究。事前学習済みモデルは低リソース言語を学習するが、事前学習済みモデルは低リソース言語を学習する。事前学習済みモデルは、低リソース言語の学習に適応する。事前学習済みモデルは、低リソース言語の学習に適応する。事前学習済みモデルは、低リソース言語の学習に適応する。

- [HCLD: A Hierarchical Framework for Zero-shot Cross-lingual Dialogue System](https://aclanthology.org/2022.coling-1.396)
  - Zhanyu Ma, Jian Ye, Xurui Yang, Jianfeng Liu
  - **TLDR**: タスク分類のタスクを、言語共通のタスクに置き換えた研究。タスク分類は、タスク分類のタスクと同等のタスクを想定している。タスク分類は、タスク分類のタスクと同等のタスクを想定している。タスク分類は、タスク分類のタスクと同等のタスクを想定している。タスク分類は、タスク分類のタスクと同等のタスクを想定している。

- [GraDA: Graph Generative Data Augmentation for Commonsense Reasoning](https://aclanthology.org/2022.coling-1.397)
  - Adyasha Maharana, Mohit Bansal
  - **TLDR**: 自然言語処理で、グラフから事実関係を推定する研究。既存のモデルは、グラフから事実関係を推定するモデルを構築するが、このモデルは実データベースの生成を前提としていない。そのため、実データベースの生成を前提としていない。実データベースの生成は、実データベースの生成と同等の条件で行い、実データベースの生成は、実データベースの生成と同等の条件で行い、実データベースの生成は、実データベースの生成と同等の条件で行い、実データベースの生成は、実データベースの生成と同等の条件で行い、実データベースの生成

- [Eureka: Neural Insight Learning for Knowledge Graph Reasoning](https://aclanthology.org/2022.coling-1.398)
  - Alex X. Zhang, Xun Liang, Bo Wu, Xiangping Zheng, Sensen Zhang, Yuhui Guo, Jun Wang, Xinyao Liu
  - **TLDR**: 教師なし学習で、教師なし学習の学習を効率化する手法の提案。教師なし学習は、教師なし学習の学習結果を学習データに入力し、学習データから教師なし学習の学習結果を抽出する。教師なし学習は、教師なし学習の学習結果を学習データに入力し、学習データから学習結果を抽出する。教師なし学習は、教師なし学習の学習結果を学習データに入力し、学習データから学習結果を学習データに入力する。

- [CitRet: A Hybrid Model for Cited Text Span Retrieval](https://aclanthology.org/2022.coling-1.399)
  - Amit Pandey, Avani Gupta, Vikram Pudi
  - **TLDR**: 論文のタイトルから、論文の著者名を抽出する研究。論文のタイトルは、論文の著者名を表す単語を含まない。この単語を抽出するCitRetを提案。CitRetは、文書の文構造を学習する。文書の文構造は、文書の文構造を学習するSegmentationと、文書の文構造を学習するWord2Vecの2つに分けている。

- [A Weak Supervision Approach for Predicting Difficulty of Technical Interview Questions](https://aclanthology.org/2022.coling-1.400)
  - Arpita Kundu, Subhasish Ghosh, Pratik Saini, Tapas Nayak, Indrajit Bhattacharya
  - **TLDR**: 質問の難易度を予測するモデルの提案。質問の難易度は、質問の回答が難しいか、質問の回答が難しいか、質問の回答が難しいかの2つに分けている。難易度は、質問の回答が難しいか、質問の回答が難しいかの2つに分けている。

- [Reinforcement Learning with Large Action Spaces for Neural Machine Translation](https://aclanthology.org/2022.coling-1.401)
  - Asaf Yehudai, Leshem Choshen, Lior Fox, Omri Abend
  - **TLDR**: 強化学習で、単語のサイズを小さくする手法の提案。単語のサイズを小さくするだけでなく、単語の類似度を上げることで、学習済みモデルの精度を上げる。単語の類似度を上げることで、学習済みモデルの精度を上げる。

- [Noise Learning for Text Classification: A Benchmark](https://aclanthology.org/2022.coling-1.402)
  - Bo Liu, Wandi Xu, Yuejia Xiang, Xiaojun Wu, Lejian He, Bowen Zhang, Li Zhu
  - **TLDR**: テキスト分類において、画像のラベルをノイズに置き換える手法の提案。画像のラベルはノイズの表現として使われることが多いが、この表現はノイズの表現として使われることが多い。そのため、ノイズを表現するデータセットを追加し、ノイズを表現するデータセットを学習する。

- [Mitigating the Diminishing Effect of Elastic Weight Consolidation](https://aclanthology.org/2022.coling-1.403)
  - Canasai Kruengkrai, Junichi Yamagishi
  - **TLDR**: 事前学習済み言語モデルの重みを、学習済みモデルの重みに分割する手法の提案。重みは、学習済みモデルの重みを大きくする(重みの分割は、学習済みモデルの重みを大きくする)ことで、学習済みモデルの重みを小さくする。

- [Token and Head Adaptive Transformers for Efficient Natural Language Processing](https://aclanthology.org/2022.coling-1.404)
  - Chonghan Lee, Md Fahim Faysal Khan, Rita Brugarolas Brufau, Ke Ding, Vijaykrishnan Narayanan
  - **TLDR**: 自然言語処理で、Transformerを強化学習で学習する研究。Transformerは、Transformerの初期値を変更することで学習する。Transformerは、Transformerの初期値を変更することで学習する。Transformerは、Transformerの初期値を変更することで学習する。Transformerは、Transformerの初期値を変更することで学習する。

- [Don’t Judge a Language Model by Its Last Layer: Contrastive Learning with Layer-Wise Attention Pooling](https://aclanthology.org/2022.coling-1.405)
  - Dongsuk Oh, Yejin Kim, Hodong Lee, H. Howie Huang, Heuiseok Lim
  - **TLDR**: 事前学習済み言語モデルの学習を、Attentionベースの手法で行う研究。事前学習済み言語モデルのAttentionを、事前学習済み言語モデルのAttentionに分割し、Attentionを分散する。事前学習済み言語モデルのAttentionを、事前学習済み言語モデルのAttentionに分割し、Attentionを分散する。

- [SHAP-Based Explanation Methods: A Review for NLP Interpretability](https://aclanthology.org/2022.coling-1.406)
  - Edoardo Mosca, Ferenc Szigeti, Stella Tragianni, Daniel Gallagher, Georg Groh
  - **TLDR**: 機械学習モデルの説明をきちんと行うためのガイドライン。モデルの説明は、モデルの構造、モデルのパフォーマンス、モデルのパフォーマンスの3つに分けられている。

- [A Simple Log-based Loss Function for Ordinal Text Classification](https://aclanthology.org/2022.coling-1.407)
  - François Castagnos, Martin Mihelich, Charles Dognin
  - **TLDR**: テキスト分類において、マルチノイズの学習は誤差の分布を考慮する必要があり、この点を考慮した研究。マルチノイズの学習は、マルチノイズの分布を考慮する必要があり、この点を考慮した研究。マルチノイズの学習は、マルチノイズの分布を考慮する必要があり、この点を考慮した研究。マルチノイズの学習は、マルチノイズの分布を考慮する必要があり、この点を考慮した研究。

- [Ask Question First for Enhancing Lifelong Language Learning](https://aclanthology.org/2022.coling-1.408)
  - Han Wang, Ruiliu Fu, Xuejun Zhang, Jun Zhou, Qingwei Zhao
  - **TLDR**: 事前学習済み言語モデルを、事前学習済みモデルのデータセットに組み込んだ研究。事前学習済みモデルは事前学習済みモデルのデータセットをベースに、事前学習済みモデルのデータセットをベースに事前学習済みモデルのデータセットを組み込んだモデル。事前学習済みモデルは事前学習済みモデルのデータセットをベースに、事前学習済みモデルのデータセットをベースに事前学習済みモデルのデータセットを組み込んだモデル。事前学習済みモデルは事前学習済みモデルのデータセットをベースに、事前学習済みモデルは事前学習済みモデルのデータセットをベースに学習する。

- [DoubleMix: Simple Interpolation-Based Data Augmentation for Text Classification](https://aclanthology.org/2022.coling-1.409)
  - Hui Chen, Wei Han, Diyi Yang, Soujanya Poria
  - **TLDR**: テキスト分類器の強化学習で、学習済みのデータと学習済みのデータの混合を組み合わせて学習する手法の提案。学習済みのデータと学習済みのデータの混合を組み合わせて、学習済みのデータと学習済みのデータの混合を組み合わせて学習する。学習済みのデータと学習済みのデータの混合は、学習済みのデータの重みを加味する形で行う。

- [Large Sequence Representation Learning via Multi-Stage Latent Transformers](https://aclanthology.org/2022.coling-1.410)
  - Ionut-Catalin Sandu, Daniel Voinea, Alin-Ionut Popa
  - **TLDR**: 自然言語処理で、画像のEntityを予測する研究。EntityはテキストのEntityと、テキストのEntityは画像のEntityとを結合し、Entityの結合はテキストのEntityとを結合する形で行う。EntityはテキストのEntityと、テキストのEntityは画像のEntityとを結合する形で行う。

- [MockingBERT: A Method for Retroactively Adding Resilience to NLP Models](https://aclanthology.org/2022.coling-1.411)
  - Jan Jezabek, Akash Singh
  - **TLDR**: 自然言語処理モデルに対する敵対的誤字脱字を防ぎつつ、転移学習時に転移学習済みモデルの学習率を上げる手法の提案。転移学習時に誤字脱字を生成する際、転移学習済みモデルの学習率を上げるための転移学習を行わず、転移学習時に誤字脱字を生成するモデルを採用している。

- [Equivariant Transduction through Invariant Alignment](https://aclanthology.org/2022.coling-1.412)
  - Jennifer C. White, Ryan Cotterell
  - **TLDR**: 自然言語処理モデルが、文の構造を学習する際の、グループの変動を考慮する仕組みを提案。グループの変動は、文の構造を学習する際の重要な要素として扱われる。そのため、グループの変動を考慮する仕組みを提案している。

- [Where Does Linguistic Information Emerge in Neural Language Models? Measuring Gains and Contributions across Layers](https://aclanthology.org/2022.coling-1.413)
  - Jenny Kunz, Marco Kuhlmann
  - **TLDR**: 自然言語処理モデルの潜在表現を評価する研究。潜在表現の分布を、各層の潜在表現の分布と比較し、各層の潜在表現の分布を算出する。この結果、各層の潜在表現が、モデルのパフォーマンスに貢献していないことがわかった。

- [Accelerating Inference for Pretrained Language Models by Unified Multi-Perspective Early Exiting](https://aclanthology.org/2022.coling-1.414)
  - Jun Kong, Jin Wang, Liang-Chih Yu, Xuejie Zhang
  - **TLDR**: 事前学習済み言語モデルの挙動を、マルチスケールで強化する研究。事前学習済み言語モデルの挙動を、マルチスケールで強化する。事前学習済み言語モデルの挙動を、マルチスケールで強化する。マルチスケールで強化する際、事前学習済み言語モデルの挙動を、マルチスケールで強化する。

- [Topology Imbalance and Relation Inauthenticity Aware Hierarchical Graph Attention Networks for Fake News Detection](https://aclanthology.org/2022.coling-1.415)
  - Li Gao, Lingyun Song, Jie Liu, Bolin Chen, Xuequn Shang
  - **TLDR**: ニュースの推論を行う際に、Topologyのバランスが崩れてしまう問題を解決する研究。Topologyのバランスが崩れると、ニュースの推論結果が誤りになる可能性がある。そのため、Topologyのバランスが崩れると判断するノードを、Topologyのバランスが崩れるノードに割り当てる手法を提案している。

- [Temporal Knowledge Graph Completion with Approximated Gaussian Process Embedding](https://aclanthology.org/2022.coling-1.416)
  - Linhai Zhang, Deyu Zhou
  - **TLDR**: 自然言語処理における、ネットワークの予測を行う研究。ネットワークの予測は、ネットワークの構造を予測するだけでなく、ネットワークの予測結果を予測する(ネットワークの予測結果は、ネットワークの予測結果と異なる場合に予測される)。ネットワークの予測結果は、ネットワークの予測結果と異なる場合に予測される。

- [CILDA: Contrastive Data Augmentation Using Intermediate Layer Knowledge Distillation](https://aclanthology.org/2022.coling-1.417)
  - Md Akmal Haidar, Mehdi Rezagholizadeh, Abbas Ghaddar, Khalil Bibi, Phillippe Langlais, Pascal Poupart
  - **TLDR**: 事前学習済み言語モデルを蒸留する研究。蒸留は、事前学習済みモデルの潜在表現を蒸留する。蒸留は、事前学習済みモデルの潜在表現を蒸留する。蒸留は、事前学習済みモデルの潜在表現を蒸留する。蒸留は、事前学習済みモデルの潜在表現を蒸留する。蒸留は、事前学習済みモデルの潜在表現を蒸留する。

- [Pro-KD: Progressive Distillation by Following the Footsteps of the Teacher](https://aclanthology.org/2022.coling-1.418)
  - Mehdi Rezagholizadeh, Aref Jafari, Puneeth S.M. Saladi, Pranav Sharma, Ali Saheb Pasand, Ali Ghodsi
  - **TLDR**: 学習データの蒸留を行う際に、どの教師が良いかを調べる研究。蒸留は蒸留器の重みを蒸留器の重みに変換する形で行われるが、蒸留器の重みは蒸留器の重みと同等の扱いを受ける。蒸留器の重みは蒸留器の重みと同等になるよう調整する。蒸留器の重みは蒸留器の重みと同等になるよう調整する

- [Classical Sequence Match Is a Competitive Few-Shot One-Class Learner](https://aclanthology.org/2022.coling-1.419)
  - Mengting Hu, Hang Gao, Yinhao Bai, Mingming Liu
  - **TLDR**: 学習済みモデルの学習で、学習済みモデルの学習率を上げる手法の提案。学習済みモデルの学習率は、学習済みモデルの学習率と同等の精度を維持するが、学習済みモデルの学習率は同等の精度を維持する。学習済みモデルの学習率は、学習済みモデルの学習率と同等の精度を維持する。

- [Unsupervised Domain Adaptation for Text Classification via Meta Self-Paced Learning](https://aclanthology.org/2022.coling-1.420)
  - Nghia Ngo Trung, Linh Ngo Van, Thien Huu Nguyen
  - **TLDR**: 自然言語処理で、ドメイン転移を行う際に、転移先のドメインを学習する手法の提案。転移先のドメインは、転移先のドメインの学習データセットを転移先のドメインに分割し、転移先のドメインの学習データセットを転移先のドメインに分割する。転移先のドメインの学習データセットを転移先のドメインに分割し、転移先のドメインの学習データセットを転移先のドメインに分割する。

- [WARM: A Weakly (+Semi) Supervised Math Word Problem Solver](https://aclanthology.org/2022.coling-1.421)
  - Oishik Chatterjee, Isha Pandey, Aashish Waikar, Vishwajeet Kumar, Ganesh Ramakrishnan
  - **TLDR**: 自然言語処理で、モデルの生成を学習する手法の提案。生成したモデルは、モデルの生成結果から、モデルの生成結果から生成したモデルの生成結果を生成する。生成結果は、モデルの生成結果から生成したモデルの生成結果と同等の精度を達成。

- [Attention Networks for Augmenting Clinical Text with Support Sets for Diagnosis Prediction](https://aclanthology.org/2022.coling-1.422)
  - Paul Grundmann, Tom Oberhauser, Felix Gers, Alexander Löser
  - **TLDR**: 医療従事者に対する診断書の作成を、事前学習済み言語モデルで行う研究。事前学習済み言語モデルは、事前学習済み言語モデルの予測精度を上げるためのネットワークを構築する。事前学習済み言語モデルは、事前学習済み言語モデルの予測精度を上げるためのネットワークを構築する。

- [PARSE: An Efficient Search Method for Black-box Adversarial Text Attacks](https://aclanthology.org/2022.coling-1.423)
  - Pengwei Zhan, Chao Zheng, Jing Yang, Yuxiang Wang, Liming Wang, Yang Wu, Yunjian Zhang
  - **TLDR**: 敵対的サンプルを作成する手法の提案。敵対的サンプルは、検索結果から単語ベクトルを推定する形で作成される。単語ベクトルは、検索結果から単語ベクトルを推定する形で作成される。単語ベクトルは、検索結果から単語ベクトルを推定する形で作成される。単語ベクトルは、単語ベクトルの重みを考慮する形で作成される。

- [A Closer Look at Parameter Contributions When Training Neural Language and Translation Models](https://aclanthology.org/2022.coling-1.424)
  - Raúl Vázquez, Hande Celikkanat, Vinit Ravishankar, Mathias Creutz, Jörg Tiedemann
  - **TLDR**: 翻訳モデルの学習を、学習の初期値と学習の結果を比較した研究。学習の初期値は、学習の初期値と学習結果の差分を表す指標で、差分は学習結果の差分を表す指標で表現されている。差分は学習結果の差分を表す指標で、差分は学習結果の差分を表す指標で表現されている。

- [KNOT: Knowledge Distillation Using Optimal Transport for Solving NLP Tasks](https://aclanthology.org/2022.coling-1.425)
  - Rishabh Bhardwaj, Tushar Vaidya, Soujanya Poria
  - **TLDR**: 自然言語処理のモデルを蒸留する研究。蒸留はEntropyベースで、蒸留のコストは蒸留モデルの学習コストと同等のコストになる。蒸留は蒸留モデルの学習コストを削減するため、蒸留モデルの学習コストを削減する。蒸留はEntropyベースで、蒸留は蒸留モデルの学習コストを削減するため、蒸留モデルの学習コストを削減する。

- [An Information Minimization Based Contrastive Learning Model for Unsupervised Sentence Embeddings Learning](https://aclanthology.org/2022.coling-1.426)
  - Shaobin Chen, Jie Zhou, Yuling Sun, Liang He
  - **TLDR**: 文分散表現の学習で、情報量を減らす手法の提案。情報量を減らすには、文分散表現の表現を学習する際の表現の重みを減らす(重みは、文分散表現の重みと同等になるよう調整する)。重みは、文分散表現の重みと同等になるよう調整する。

- [Learn2Weight: Parameter Adaptation against Similar-domain Adversarial Attacks](https://aclanthology.org/2022.coling-1.427)
  - Siddhartha Datta
  - **TLDR**: 機械学習モデルに対するAdversarial Attackの研究。Adversarial Attackは、Adversarialの学習済みモデルをAdversarialの学習済みモデルに転移させる手法。Adversarialは、Adversarialの学習済みモデルの重みを予測するモデルを構築する。Adversarialは、Adversarialの学習済みモデルの重みを予測するモデルを構築する。

- [Sentence-aware Adversarial Meta-Learning for Few-Shot Text Classification](https://aclanthology.org/2022.coling-1.428)
  - Suhe Wang, Xiaoyuan Liu, Bo Liu, Diwen Dong
  - **TLDR**: テキスト分類の手法として、事前学習済みモデルを組み込んだ研究。事前学習済みモデルは、事前学習済みモデルの学習済みモデルの学習済みモデルと同等の学習済みモデルの学習済みモデルを組み合わせる。事前学習済みモデルは、事前学習済みモデルの学習済みモデルの学習済みモデルと同等の学習済みモデルの学習済みモデルを組み合わせる。

- [Reweighting Strategy Based on Synthetic Data Identification for Sentence Similarity](https://aclanthology.org/2022.coling-1.429)
  - TaeHee Kim, ChaeHun Park, Jimin Hong, Radhika Dua, Edward Choi, Jaegul Choo
  - **TLDR**: 自然言語処理で、文の分散表現を学習する研究。文の分散表現を学習する際、文の分散表現を学習するクラスを生成する。生成した文を、文分散表現のクラスに分類し、クラスの分類結果から文分散表現を学習する。学習データの精度は、文分散表現の学習で良好。

- [MaxMatch-Dropout: Subword Regularization for WordPiece](https://aclanthology.org/2022.coling-1.430)
  - Tatsuya Hiraoka
  - **TLDR**: 単語分散表現を、単語分散表現に置き換える手法の提案。単語分散表現は単語分散表現の生成に使用される手法で、単語分散表現の生成は単語分散表現の生成に使用される手法で行う。単語分散表現の生成は、単語分散表現の生成に使用される手法で行う。

- [Adaptive Meta-learner via Gradient Similarity for Few-shot Text Classification](https://aclanthology.org/2022.coling-1.431)
  - Tianyi Lei, Honghui Hu, Qiaoyang Luo, Dezhong Peng, Xu Wang
  - **TLDR**: マルチタスク学習で、学習率を上げるためにGradient Similarityを導入した研究。学習率は学習率の分布を上回るほど低いが、学習率の分布は学習率の分布を上回る。学習率は学習率の分布を上回る。

- [Vocabulary-informed Language Encoding](https://aclanthology.org/2022.coling-1.432)
  - Xi Ai, Bin Fang
  - **TLDR**: マルチ言語モデルで、言語情報のEncoderを生成する研究。Encoderは単語情報のEncoderと、言語情報のEncoderは単語情報のEncoderとで生成を行う。言語情報のEncoderは、単語情報のEncoderと単語情報のEncoderの2つを生成する。言語情報のEncoderは、単語情報のEncoderと単語情報のEncoderの2つを生成する。

- [OpticE: A Coherence Theory-Based Model for Link Prediction](https://aclanthology.org/2022.coling-1.433)
  - Xiangyu Gui, Feng Zhao, Langjunqing Jin, Hai Jin
  - **TLDR**: 知識グラフの表現を、物理的な不変(光の振幅)に置き換えた研究。物理的な不変は、物理的な不変と同等に扱われるか?を検証した研究。物理的な不変は、物理的な不変と同等に扱われるか?を検証した研究。物理的な不変は、物理的な不変と同等に扱われるか?を検証した研究。物理的な不変は、物理的な不変と同等に扱われるか?を検証した研究。

- [Smoothed Contrastive Learning for Unsupervised Sentence Embedding](https://aclanthology.org/2022.coling-1.434)
  - Xing Wu, Chaochen Gao, Yipeng Su, Jizhong Han, Zhongyuan Wang, Songlin Hu
  - **TLDR**: 事前学習済みモデルで、事前学習済みモデルの挙動を改善する研究。事前学習済みモデルの挙動を、事前学習済みモデルの挙動に近いものに変更する。事前学習済みモデルの挙動を、事前学習済みモデルの挙動に近いものに変更する。事前学習済みモデルの挙動を、事前学習済みモデルの挙動に近いものに変更する。

- [Knowledge Distillation with Reptile Meta-Learning for Pretrained Language Model Compression](https://aclanthology.org/2022.coling-1.435)
  - Xinge Ma, Jin Wang, Liang-Chih Yu, Xuejie Zhang
  - **TLDR**: 事前学習済み言語モデルの学習を、蒸留で行う研究。蒸留は学習済みモデルの挙動を学習するが、蒸留は学習済みモデルの挙動を学習する。蒸留は学習済みモデルの挙動を学習するが、蒸留は学習済みモデルの挙動を学習する。蒸留は学習済みモデルの挙動を学習するが、学習は学習済みモデルの挙動を学習する。

- [RotateCT: Knowledge Graph Embedding by Rotation and Coordinate Transformation in Complex Space](https://aclanthology.org/2022.coling-1.436)
  - Yao Dong, Lei Wang, Ji Xiang, Xiaobo Guo, Yuqiang Xie
  - **TLDR**: 知識グラフの表現を、学習する研究。学習する各表現は、学習済みモデルの表現と同等の表現を生成する。この表現を、学習済みモデルの表現と同等の表現を生成する形で学習する。学習済みモデルの表現は、学習済みモデルの表現と同等の表現を生成する。

- [Can Data Diversity Enhance Learning Generalization?](https://aclanthology.org/2022.coling-1.437)
  - Yu Yu, Shahram Khadivi, Jia Xu
  - **TLDR**: 自然言語処理の強化学習で、学習データのdiversityを上げるために強化学習の手法を提案。学習データの分散、重みの重み、グラフの重みを考慮し、学習データの分散と重みの重みを考慮したモデルを構築する。学習データのdiversityは、学習データの分散と重みの重みを考慮したモデルを構築する。

- [Generate-and-Retrieve: Use Your Predictions to Improve Retrieval for Semantic Parsing](https://aclanthology.org/2022.coling-1.438)
  - Yury Zemlyanskiy, Michiel de Jong, Joshua Ainslie, Panupong Pasupat, Peter Shaw, Linlu Qiu, Sumit Sanghai, Fei Sha
  - **TLDR**: 学習済みモデルの学習済みモデルを学習する際、学習済みモデルの学習済みモデルと同等の学習済みモデルの学習済みモデルを比較し、学習済みモデルの学習済みモデルと同等の学習済みモデルの学習済みモデルを比較する研究。学習済みモデルの学習済みモデルと同等の学習済みモデルを比較し、学習済みモデルの学習済みモデルと同等の学習済みモデルの学習済みモデルを比較し、学習済みモデルの学習済みモデルの学習済みモデルの学習済みモデルの学習済みモデルの学習済みモデルの学習済みモデルの学習済みモデルの学習済みモデルの学習済みモデルの学習済みモデルの学習済みモデルの学習済みモデルの学習済み

- [Coarse-to-Fine: Hierarchical Multi-task Learning for Natural Language Understanding](https://aclanthology.org/2022.coling-1.439)
  - Zhaoye Fei, Yu Tian, Yongkang Wu, Xinyu Zhang, Yutao Zhu, Zheng Liu, Jiawen Wu, Dejiang Kong, Ruofei Lai, Zhao Cao, Zhicheng Dou, Xipeng Qiu
  - **TLDR**: 自然言語処理で、タスク分類のタスクを分類する研究。タスク分類は、タスク分類の分類結果から、タスク分類結果からタスク分類結果を予測する。タスク分類結果は、タスク分類結果とタスク分類結果の相関が大きいことを確認。タスク分類結果は、タスク分類結果とタスク分類結果の相関が大きいことを確認。

- [Automatic Label Sequence Generation for Prompting Sequence-to-sequence Models](https://aclanthology.org/2022.coling-1.440)
  - Zichun Yu, Tianyu Gao, Zhengyan Zhang, Yankai Lin, Zhiyuan Liu, Maosong Sun, Jie Zhou
  - **TLDR**: 自然言語による事前学習済みモデルの自動生成を行うAutoSeqを提案。事前学習済みモデルは、事前学習済みモデルの挙動を予測するモデルで、事前学習済みモデルの挙動を予測するモデルは、事前学習済みモデルの挙動を予測するモデルで予測する。事前学習済みモデルは、事前学習済みモデルの挙動を予測するモデルを採用する。

- [Unsupervised Sentence Textual Similarity with Compositional Phrase Semantics](https://aclanthology.org/2022.coling-1.441)
  - Zihao Wang, Jiaheng Dou, Yong Zhang
  - **TLDR**: 文の類似度を測る研究。文の類似度は、文の文構造(単語の意味)と文の意味(単語の意味)の類似度を比較する形で測る。単語の意味を予測するモデルは、単語の意味を予測するモデルと同等精度を達成。

- [A Generalized Method for Automated Multilingual Loanword Detection](https://aclanthology.org/2022.coling-1.442)
  - Abhijnan Nath, Sina Mahdipour Saravani, Ibrahim Khebour, Sheikh Mannan, Zihui Li, Nikhil Krishnaswamy
  - **TLDR**: 翻訳文から、翻訳文の類似度を計測する研究。翻訳文の類似度は、翻訳文の文法・単語の意味・単語の意味・意味の3つに分類される。翻訳文の類似度は、翻訳文の単語分布(単語の類似度)と翻訳文の単語分布(単語分布)の3つに分類される。

- [FeatureBART: Feature Based Sequence-to-Sequence Pre-Training for Low-Resource NMT](https://aclanthology.org/2022.coling-1.443)
  - Abhisek Chakrabarty, Raj Dabre, Chenchen Ding, Hideki Tanaka, Masao Utiyama, Eiichiro Sumita
  - **TLDR**: 事前学習済み言語モデルの提案。単語の意味を予測するモデルと、単語の意味を予測するモデルを組み合わせている。単語の意味予測は、単語の意味を予測するモデルと、単語の意味を予測するモデルの2つに分けて行う。単語の意味予測は、単語の意味を予測するモデルと、単語の意味を予測するモデルの2つに分けて行う。

- [Multi-level Community-awareness Graph Neural Networks for Neural Machine Translation](https://aclanthology.org/2022.coling-1.444)
  - Binh Nguyen, Long Nguyen, Dien Dinh
  - **TLDR**: 翻訳を行う際に、自然言語処理のモデルを用いた研究。自然言語処理は、文の構造(morphology、 syntactic)、文の意味(keyword)、文の意味(keyword)、文の意味(keyword)、文の意味(keyword)、文の意味(keyword)、文の意味(keyword)、文の意味(keyword)、文の意味(keyword)、文の意味(keyword)、文の意味(keyword)、文の意味(keyword)、文の意味(keyword)、文の意味(keyword)、文の意味(keyword)、文の意味(

- [On the Complementarity between Pre-Training and Random-Initialization for Resource-Rich Machine Translation](https://aclanthology.org/2022.coling-1.445)
  - Changtong Zan, Liang Ding, Li Shen, Yu Cao, Weifeng Liu, Dacheng Tao
  - **TLDR**: 翻訳モデルとモデルの組み合わせで、翻訳精度と汎化性能を向上させる研究。モデルは、翻訳モデルとモデルの組み合わせで、モデルの学習済みモデルと同等の精度を達成できた。モデルは、翻訳モデルの学習済みモデルと同等の精度を達成できた。

- [ngram-OAXE: Phrase-Based Order-Agnostic Cross Entropy for Non-Autoregressive Machine Translation](https://aclanthology.org/2022.coling-1.446)
  - Cunxiao Du, Zhaopeng Tu, Longyue Wang, Jing Jiang
  - **TLDR**: マルチモーメントの翻訳で、単語分散表現を学習する手法の提案。単語分散表現は、単語の位置関係を考慮する必要があり、この点を考慮した。単語分散表現は、単語の位置関係を考慮する必要があり、この点を考慮した。単語分散表現は、単語分散表現の学習時に使用される。

- [Language Branch Gated Multilingual Neural Machine Translation](https://aclanthology.org/2022.coling-1.447)
  - Haoran Sun, Deyi Xiong
  - **TLDR**: マルチ言語の翻訳において、言語共通のネットワークを構築する研究。言語共通のネットワークは、言語共通のネットワークのパラメーターを共有するネットワーク(言語共通のネットワークは、言語共通のネットワークのパラメーターを含まない)をベースにしている。このネットワークは、言語共通のネットワークと異なる特徴を持つ言語を識別するネットワークを構築する。

- [Iterative Constrained Back-Translation for Unsupervised Domain Adaptation of Machine Translation](https://aclanthology.org/2022.coling-1.448)
  - Hongxiao Zhang, Hui Huang, Jiale Gao, Yufeng Chen, Jinan Xu, Jian Liu
  - **TLDR**: 機械翻訳における、ドメイン転移の手法の提案。ドメイン転移は、単語の意味を予測するモデル(CNN)を用いることで行われる。このモデルは、単語の意味を予測するモデル(CNN)と、単語の意味を予測するモデル(CNN)を組み合わせている。CNNは、単語の意味を予測するモデル(CNN)と、単語の意味を予測するモデル(CNN)の2つを用いている。

- [Linguistically-Motivated Yorùbá-English Machine Translation](https://aclanthology.org/2022.coling-1.449)
  - Ife Adebara, Muhammad Abdul-Mageed, Miikka Silfverberg
  - **TLDR**: 自然言語処理で、事前学習済みモデルと比較して、SMT/Transformerの精度がどれだけ向上しているか検証した研究。SMTは事前学習済みモデルと同等の精度を達成できたが、Transformerは精度が落ちる。事前学習済みモデルは、事前学習済みモデルと同等の精度を達成できたが、Transformerは精度が落ちる。

- [Dynamic Position Encoding for Transformers](https://aclanthology.org/2022.coling-1.450)
  - Joyce Zheng, Mehdi Rezagholizadeh, Peyman Passban
  - **TLDR**: 翻訳において、各単語の位置情報を自動生成する研究。位置情報は、翻訳のタスクによって異なる。通常の位置情報は、翻訳のタスクによって異なる。このため、位置情報の生成は、翻訳のタスクによって異なる。このため、位置情報の生成は、翻訳のタスクによって異なる。

- [PAEG: Phrase-level Adversarial Example Generation for Neural Machine Translation](https://aclanthology.org/2022.coling-1.451)
  - Juncheng Wan, Jian Yang, Shuming Ma, Dongdong Zhang, Weinan Zhang, Yong Yu, Zhoujun Li
  - **TLDR**: 機械翻訳において、Adversarial Exampleを生成する研究。単語レベルのAdversarial Exampleを生成するだけでなく、単語レベルのAdversarial Exampleを生成する手法を提案している。単語レベルのAdversarial Exampleは、単語の構造を予測するGraph Gradientをベースに生成する。

- [Noise-robust Cross-modal Interactive Learning with Text2Image Mask for Multi-modal Neural Machine Translation](https://aclanthology.org/2022.coling-1.452)
  - Junjie Ye, Junjun Guo, Yan Xiang, Kaiwen Tan, Zhengtao Yu
  - **TLDR**: マルチモーダルで翻訳を行う際に、画像とテキストを結合する研究。画像とテキストを結合する際、画像とテキストの位置関係を認識するMaskを導入し、画像とテキストの位置関係を認識するMaskを追加。画像とテキストの位置関係を認識するMaskを追加することで、精度向上を図っている。

- [Speeding up Transformer Decoding via an Attention Refinement Network](https://aclanthology.org/2022.coling-1.453)
  - Kaixin Wu, Yue Zhang, Bojie Hu, Tong Zhang
  - **TLDR**: 翻訳モデルの高速化を、Attentionの重みを軽量化したネットワークで行う研究。重みはAttentionの重みと同等かそれ以上の重みで、重みはAttentionの重みと同等かそれ以上の重みで計算する。重みはAttentionの重みと同等かそれ以上の重みで計算する。

- [Interactive Post-Editing for Verbosity Controlled Translation](https://aclanthology.org/2022.coling-1.454)
  - Prabhakar Gupta, Anil Nelakanti, Grant M. Berry, Abhishek Sharma
  - **TLDR**: 機械翻訳の自動生成モデルを、事前学習済みモデルと比較して精度を上げた研究。事前学習済みモデルは、事前学習済みモデルの学習済みモデルと同等の精度を維持できるが、事前学習済みモデルは学習済みモデルの学習済みモデルと同等の精度を維持できない。事前学習済みモデルは、事前学習済みモデルの学習済みモデルと同等の精度を維持できる。

- [Addressing Asymmetry in Multilingual Neural Machine Translation with Fuzzy Task Clustering](https://aclanthology.org/2022.coling-1.455)
  - Qian Wang, Jiajun Zhang
  - **TLDR**: マルチタスクの翻訳モデルで、翻訳タスクの学習を効率化する研究。タスクの学習は、翻訳タスクの学習済みモデルで行う。タスクの学習済みモデルは、翻訳タスクの学習済みモデルと同等の精度を維持できるか検証する。タスクの学習済みモデルは、タスクの学習済みモデルと同等の精度を維持できるか検証する。

- [Learning Decoupled Retrieval Representation for Nearest Neighbour Neural Machine Translation](https://aclanthology.org/2022.coling-1.456)
  - Qiang Wang, Rongxiang Weng, Ming Chen
  - **TLDR**: 翻訳を行う際に、外部の表現を抽出する手法の提案。翻訳文の文脈を抽出する(文脈は、文の意味を含まない表現を抽出する)ことで、文脈の表現を抽出する。文脈は、文の意味を含まない表現を抽出する(文脈は、文の意味を含まない表現を抽出する)。文脈の表現を抽出する手法は、文脈の表現を抽出する手法と同等精度を達成できた。

- [Semantically Consistent Data Augmentation for Neural Machine Translation via Conditional Masked Language Model](https://aclanthology.org/2022.coling-1.457)
  - Qiao Cheng, Jin Huang, Yitao Duan
  - **TLDR**: 機械翻訳において、文の表現を変更する際の、事前学習済みモデルを有効に使う研究。事前学習済みモデルは、文の表現を変更する際の事前学習済みモデルと同等の効果を発揮する。事前学習済みモデルは、文の表現を変更する際の事前学習済みモデルと同等の効果を発揮する。

- [Informative Language Representation Learning for Massively Multilingual Neural Machine Translation](https://aclanthology.org/2022.coling-1.458)
  - Renren Jin, Deyi Xiong
  - **TLDR**: マルチ言語の翻訳モデルで、翻訳の方向を学習する研究。翻訳のタスクは、翻訳のタスクを学習する。タスクは、翻訳のタスクを学習する。タスクは、翻訳のタスクを学習する。タスクは、翻訳のタスクを学習する。タスクは、翻訳のタスクを学習する。タスクは、翻訳のタスクを学習する。タスクは、翻訳のタスクを学習する。タスクは、翻訳のタスクを学習する。

- [Rare but Severe Neural Machine Translation Errors Induced by Minimal Deletion: An Empirical Study on Chinese and English](https://aclanthology.org/2022.coling-1.459)
  - Ruikang Shi, Alvin Grissom II, Duc Minh Trinh
  - **TLDR**: 翻訳において、翻訳の精度を落とすのに有効な手法の提案。翻訳の精度を落とすのに有効な手法として、翻訳文を削除する手法を提案している。削除した単語は翻訳精度に影響しないが、単語の位置関係(単語の意味)の影響は大きい。

- [QUAK: A Synthetic Quality Estimation Dataset for Korean-English Neural Machine Translation](https://aclanthology.org/2022.coling-1.460)
  - Sugyeong Eo, Chanjun Park, Hyeonseok Moon, Jaehyung Seo, Gyeongmin Kim, Jungseob Lee, Heuiseok Lim
  - **TLDR**: 機械翻訳における品質推定の自動生成を行う研究。翻訳文を翻訳モデルに入力し、モデルの予測結果を入力とし、予測結果を入力とし予測結果を入力とし予測結果を入力とし予測結果を入力とし予測結果を入力とし予測結果を入力とし予測結果を入力とし予測結果を入力とし予測結果を入力とし予測結果を入力とし予測結果を入力とし予測結果を入力とし予測結果を入力とし予測結果を入力とし予測結果を入力とし予測結果を入力とし予測結果を入力とし予測結果を入力とし予測結果を入力とし予測結果を

- [Improving Both Domain Robustness and Domain Adaptability in Machine Translation](https://aclanthology.org/2022.coling-1.461)
  - Wen Lai, Jindřich Libovický, Alexander Fraser
  - **TLDR**: 翻訳モデルの転移性能を上げるために、ドメインの柔軟性を高める手法の提案。学習データの転移は、ドメインの柔軟性を上げるためには有効な手法だが、転移先のドメインの柔軟性を上げるためには、転移先のドメインの柔軟性を上げるためには有効な手法ではないかとしている。

- [CoDoNMT: Modeling Cohesion Devices for Document-Level Neural Machine Translation](https://aclanthology.org/2022.coling-1.462)
  - Yikun Lei, Yuqi Ren, Deyi Xiong
  - **TLDR**: 文書レベルの翻訳で、マルチタスクで学習したモデルを提案。マルチタスクで学習したモデルを、マルチタスクで学習したモデルと同等の精度で翻訳する。マルチタスクで学習したモデルは、マルチタスクで学習したモデルと同等の精度を達成。

- [Improving Non-Autoregressive Neural Machine Translation via Modeling Localness](https://aclanthology.org/2022.coling-1.463)
  - Yong Wang, Xinwei Geng
  - **TLDR**: 翻訳モデルの精度が低い原因を調査した研究。翻訳モデルは、翻訳文の文脈を予測するネットワークを構築するが、このネットワークは文脈の予測精度が低い。そこで、文脈の予測精度を上げるために、文脈の予測精度を上げるためのネットワークを追加する手法を提案している。

- [Categorizing Semantic Representations for Neural Machine Translation](https://aclanthology.org/2022.coling-1.464)
  - Yongjing Yin, Yafu Li, Fandong Meng, Jie Zhou, Yue Zhang
  - **TLDR**: 自然言語処理で、単語の分類を強化する研究。単語の分類は、単語の意味を予測する際の重要な要素として扱われる。そのため、単語の分類を学習するモデルを導入し、単語の分類を学習するモデルをベースに学習する。

- [Adversarial Training on Disentangling Meaning and Language Representations for Unsupervised Quality Estimation](https://aclanthology.org/2022.coling-1.465)
  - Yuto Kuroda, Tomoyuki Kajiwara, Yuki Arase, Takashi Ninomiya
  - **TLDR**: 翻訳の品質を教師なしで推定する研究。翻訳文の単語ベクトルを翻訳文の単語ベクトルに変換し、翻訳文の単語ベクトルを翻訳文の単語ベクトルに変換する。翻訳文の単語ベクトルは翻訳文の単語ベクトルと同等の意味を持つ単語ベクトルを生成する。

- [Alleviating the Inequality of Attention Heads for Neural Machine Translation](https://aclanthology.org/2022.coling-1.466)
  - Zewei Sun, Shujian Huang, Xinyu Dai, Jiajun Chen
  - **TLDR**: マルチタスクの翻訳で、Attentionの重みをMaskする手法の提案。Maskは、Attentionの重みをMaskする際の重みを、複数タスクの翻訳でも同じように計算する。これにより、翻訳の精度向上が確認できた。

- [Adapting to Non-Centered Languages for Zero-shot Multilingual Translation](https://aclanthology.org/2022.coling-1.467)
  - Zhi Qu, Taro Watanabe
  - **TLDR**: 事前学習済み言語モデルを、自然言語の翻訳に応用した研究。事前学習済み言語モデルは、自然言語の翻訳に強いベースラインを採用している。ベースラインは、自然言語の翻訳を学習する際、自然言語の表現をベースに学習する。ベースラインは、自然言語の表現をベースに、自然言語の表現をベースに学習する。

- [Towards Robust Neural Machine Translation with Iterative Scheduled Data-Switch Training](https://aclanthology.org/2022.coling-1.468)
  - Zhongjian Miao, Xiang Li, Liyan Kang, Wen Zhang, Chulun Zhou, Yidong Chen, Bin Wang, Min Zhang, Jinsong Su
  - **TLDR**: 強化学習で、敵対的サンプルを生成する手法を改良した研究。敵対的サンプルは、学習時に生成されるサンプルと、生成時に生成されるサンプルの2種類に分けて学習する。敵対的サンプルは、生成時に生成されるサンプルと生成時に生成されるサンプルの2種類に分けて学習する。

- [Cross-lingual Feature Extraction from Monolingual Corpora for Low-resource Unsupervised Bilingual Lexicon Induction](https://aclanthology.org/2022.coling-1.469)
  - Zihao Feng, Hailong Cao, Tiejun Zhao, Weixuan Wang, Wei Peng
  - **TLDR**: 教師なし学習済み言語モデルで、複数言語の単語を学習する研究。学習済み言語の単語を、教師なしで学習する。教師なし学習済み言語の単語を、教師なし学習済み言語の単語に変換する。教師なし学習済み言語の単語を、教師なし学習済み言語の単語に変換する。

- [Language-Independent Approach for Morphological Disambiguation](https://aclanthology.org/2022.coling-1.470)
  - Alymzhan Toleu, Gulmira Tolegen, Rustam Mussabayev
  - **TLDR**: 自然言語処理における、自然言語処理の精度を向上させる研究。単語の固有表現を予測するモデルをベースに、固有表現の予測は言語モデルの予測と同等の手法を取っている。固有表現の予測は、固有表現の予測と同等の手法を取っている。

- [SUN: Exploring Intrinsic Uncertainties in Text-to-SQL Parsers](https://aclanthology.org/2022.coling-1.471)
  - Bowen Qin, Lihan Wang, Binyuan Hui, Bowen Li, Xiangpeng Wei, Binhua Li, Fei Huang, Luo Si, Min Yang, Yongbin Li
  - **TLDR**: テキストベースのSQLを強化する研究。複数質問を同時に解くのではなく、複数質問の組み合わせを検証する。複数質問の組み合わせは、単一のSQLで解くと解釈性が低いため、複数質問の組み合わせを検証する。検証結果として、複数質問の組み合わせが有効な結果を得られた。

- [Deciphering and Characterizing Out-of-Vocabulary Words for Morphologically Rich Languages](https://aclanthology.org/2022.coling-1.472)
  - Georgie Botev, Arya D. McCarthy, Winston Wu, David Yarowsky
  - **TLDR**: 自然言語処理で、アウトソース語の分類・分類・翻訳・言語モデルのタスクで有効な手法をまとめた研究。分類は、単語の意味を予測する形で行われることが多いが、分類は単語の意味を予測する形で行われることが多い。分類は、単語の意味を予測する形で行われることが多いが、この場合、単語の意味を予測する形で行われることが多い。

- [PSSAT: A Perturbed Semantic Structure Awareness Transferring Method for Perturbation-Robust Slot Filling](https://aclanthology.org/2022.coling-1.473)
  - Guanting Dong, Daichi Guo, Liwen Wang, Xuefeng Li, Zechen Wang, Chen Zeng, Keqing He, Jinzheng Zhao, Hao Lei, Xinyue Cui, Yi Huang, Junlan Feng, Weiran Xu
  - **TLDR**: 自然言語処理で、Slot fillingモデルの学習を強化する研究。学習済みモデルの学習済み言語モデルを、学習済み言語モデルの学習済み言語モデルに転移する。学習済み言語モデルの学習済み言語モデルを、学習済み言語モデルの学習済み言語モデルに転移する。学習済み言語モデルの学習済み言語モデルを、学習済み言語モデルの学習済み言語モデルに転移する。

- [String Editing Based Chinese Grammatical Error Diagnosis](https://aclanthology.org/2022.coling-1.474)
  - Haihua Xie, Xiaoqing Lyu, Xuefei Chen
  - **TLDR**: 文法エラーの検知と修正を行う手法の提案。文法エラーは、文法エラーの分類と修正の2つに分けており、分類は文法エラーの分類と修正の2つに分けている。修正は、文法エラーの分類と修正の2つに分けている。文法エラーは、文法分類の精度が低い場合のみ修正する。

- [The Fragility of Multi-Treebank Parsing Evaluation](https://aclanthology.org/2022.coling-1.475)
  - Iago Alonso-Alonso, David Vilares, Carlos Gómez-Rodríguez
  - **TLDR**: 教師なし学習で、複数階層の学習データセットを比較する研究。学習データセットの平均値をベースに、各階層の学習データセットの平均値をベースに学習データセットを比較する。結果として、学習データセットの平均値が低い階層は学習データセットの平均値に大きく依存する。

- [FactMix: Using a Few Labeled In-domain Examples to Generalize to Cross-domain Named Entity Recognition](https://aclanthology.org/2022.coling-1.476)
  - Linyi Yang, Lifan Yuan, Leyang Cui, Wenyang Gao, Yue Zhang
  - **TLDR**: マルチドメインのEntity認識で、事前学習済みモデルの精度を上げる手法の提案。事前学習済みモデルの精度を上げるには、事前学習済みモデルの精度を上げるための事前学習済みモデルの精度を上げる必要がある。事前学習済みモデルの精度を上げるには、事前学習済みモデルの精度を上げるための事前学習済みモデルの精度を上げる必要がある。

- [Speaker-Aware Discourse Parsing on Multi-Party Dialogues](https://aclanthology.org/2022.coling-1.477)
  - Nan Yu, Guohong Fu, Min Zhang
  - **TLDR**: マルチスコアの対話システムで、同じ質問者同士の対話結果を予測する研究。質問者同士の対話結果を予測する際、質問者同士の対話結果を予測するモデルを提案。質問者同士の対話結果を予測するモデルは、質問者同士の対話結果を予測するモデルと同等精度を達成。

- [Iterative Span Selection: Self-Emergence of Resolving Orders in Semantic Role Labeling](https://aclanthology.org/2022.coling-1.478)
  - Shuhei Kurita, Hiroki Ouchi, Kentaro Inui, Satoshi Sekine
  - **TLDR**: 文中の単語を、文の意味を含んだ単語に置き換える研究。単語の意味を含んだ単語を、単語の意味を含んだ単語に置き換える。単語の意味を含んだ単語を、単語の意味を含んだ単語に置き換える。単語の意味を含んだ単語を、単語の意味を含んだ単語に置き換える。

- [Revisiting the Practical Effectiveness of Constituency Parse Extraction from Pre-trained Language Models](https://aclanthology.org/2022.coling-1.479)
  - Taeuk Kim
  - **TLDR**: 事前学習済み言語モデルから、事前学習済み言語モデルの表現を抽出する手法の提案。事前学習済み言語モデルの表現を抽出する手法は、事前学習済み言語モデルの表現を抽出するモデルと同等の手法を取っている。事前学習済み言語モデルの表現を抽出する手法は、事前学習済み言語モデルの表現を抽出する手法と同等精度を達成できた。

- [Position Offset Label Prediction for Grammatical Error Correction](https://aclanthology.org/2022.coling-1.480)
  - Xiuyu Wu, Jingsong Yu, Xu Sun, Yunfang Wu
  - **TLDR**: 文中の位置を予測するタスクを、文の位置予測に組み込んだ研究。文中の位置を予測するタスクは、文中の単語を予測するタスクと、文中の単語を予測するタスクの2つに分けて行う。文中の単語を予測するタスクは、文中の単語を予測するタスクと、文中の単語を予測するタスクの2つに分けて行う。

- [Parsing Natural Language into Propositional and First-Order Logic with Dual Reinforcement Learning](https://aclanthology.org/2022.coling-1.481)
  - Xuantao Lu, Jingping Liu, Zhouhong Gu, Hanwen Tong, Chenhao Xie, Junyang Huang, Yanghua Xiao, Wenguang Wang
  - **TLDR**: 自然言語処理で、文の意味を表現するモデルの提案。文の意味を表現するモデルは、文の意味を表現する単語を生成するモデルと、文の意味を表現する単語を生成する単語生成モデルの2つを提案している。学習は、学習済みモデルの学習済みモデルを学習する。

- [Yet Another Format of Universal Dependencies for Korean](https://aclanthology.org/2022.coling-1.482)
  - Yige Chen, Eunkyul Leah Jo, Yundong Yao, KyungTae Lim, Miikka Silfverberg, Francis M. Tyers, Jungyeul Park
  - **TLDR**: 依存構造を扱う際、morphemeベースの手法を採用する研究。既存の手法は、依存構造を扱う際、morphemeベースの表現を用いがちだが、この場合、既存の表現を用いず、既存の表現を用いつつ、既存の表現を用いつつ、既存の表現を用いつつ、既存の表現を用いつつ、既存の表現を用いつつ、既存の表現を用いつつ、既存の表現を用いつつ、既存の表現を用いつつ、既存の表現を用いつつ、既存の表現を用いつつ、既存の表現を用

- [Enhancing Structure-aware Encoder with Extremely Limited Data for Graph-based Dependency Parsing](https://aclanthology.org/2022.coling-1.483)
  - Yuanhe Tian, Yan Song, Fei Xia
  - **TLDR**: 自然言語処理で、文の構造を予測する研究。文の構造を予測するモデルを事前学習済みモデルと、文の構造を予測するモデルの2つに分け、事前学習済みモデルの学習済みモデルをベースに学習する。事前学習済みモデルは、文構造を予測するモデルと同等の精度を維持できる。

- [Simple and Effective Graph-to-Graph Annotation Conversion](https://aclanthology.org/2022.coling-1.484)
  - Yuxuan Wang, Zhilin Lei, Yuqiu Ji, Wanxiang Che
  - **TLDR**: グラフ構造をGraph2Graphに変換する手法の提案。Graph2Graphは、Graph2Graphの生成に適したモデルで、Graph2GraphはGraph2Graphの生成に適したモデルで生成を行う。Graph2Graphは、Graph2Graphの生成に適したモデルで生成を行う。

- [BiBL: AMR Parsing and Generation with Bidirectional Bayesian Learning](https://aclanthology.org/2022.coling-1.485)
  - Ziming Cheng, Zuchao Li, Hai Zhao
  - **TLDR**: 自然言語処理における、Transformerの学習方法を改善した研究。Transformerは、文の意味表現を表現するタスクでなく、文の意味表現を表現するタスクを学習する。文の意味表現を表現するタスクは、文の意味表現を表現するタスクと、文の意味表現を表現するタスクの2つに分けて学習する。

- [Multi-Layer Pseudo-Siamese Biaffine Model for Dependency Parsing](https://aclanthology.org/2022.coling-1.486)
  - Ziyao Xu, Houfeng Wang, Bingdong Wang
  - **TLDR**: マルチラベルのBi-Agentを、複数ラベルのマルチラベルで学習する研究。マルチラベルのBi-Agentは、単一ラベルの学習に使用するが、マルチラベルの学習は複数ラベルの学習に使用する。マルチラベルの学習は、単一ラベルの学習と併用する。

- [Belief Revision Based Caption Re-ranker with Visual Semantic Information](https://aclanthology.org/2022.coling-1.487)
  - Ahmed Sabir, Francesc Moreno-Noguer, Pranava Madhyastha, Lluís Padró
  - **TLDR**: 画像のキャプチャーを、画像の表現と画像のコンテキストの距離を近づける手法の提案。画像のコンテキストは、画像のコンテキストと画像のコンテキストの距離を近づける。画像のコンテキストは、画像のコンテキストと画像のコンテキストの距離を近づける。画像のコンテキストは、画像のコンテキストと画像のコンテキストの距離を近づける。

- [Towards Understanding the Relation between Gestures and Language](https://aclanthology.org/2022.coling-1.488)
  - Artem Abzaliev, Andrew Owens, Rada Mihalcea
  - **TLDR**: 自然言語処理で、自然言語処理のモデルを用いた研究。自然言語処理では、自然言語の表現と言語モデルの表現を統合する手法を用いている。自然言語処理では、言語モデルの表現と、言語モデルの表現を統合する手法を用いている。

- [Building Joint Relationship Attention Network for Image-Text Generation](https://aclanthology.org/2022.coling-1.489)
  - Changzhi Wang, Xiaodong Gu
  - **TLDR**: 画像の特徴と、その関係を学習する研究。画像の特徴と関係を学習する際は、画像の特徴と関係を持つ画像を同時に学習する。画像の特徴と関係を持つ画像を学習する際は、画像の特徴と関係を持つ画像を同時に学習する。画像の特徴と関係を持つ画像を学習する際は、画像の特徴と関係を持つ画像を同時に学習する。

- [Learning to Focus on the Foreground for Temporal Sentence Grounding](https://aclanthology.org/2022.coling-1.490)
  - Daizong Liu, Wei Hu
  - **TLDR**: 文中の画像を、画像の背景に分割して、画像の画像を生成する研究。画像の背景を、画像の画像を生成するネットワークで分割し、画像の画像を生成するネットワークを構築する。画像の画像を生成するネットワークは、画像の画像を生成するネットワークと、画像の画像を生成するネットワークの2つを想定している。

- [Are Visual-Linguistic Models Commonsense Knowledge Bases?](https://aclanthology.org/2022.coling-1.491)
  - Hsiu-Yu Yang, Carina Silberer
  - **TLDR**: 自然言語処理で、画像をモデル化する際の有効性を調査した研究。画像をモデル化する際は、画像の表現が画像と異なる場合にモデル化を行う。画像は画像と異なる場合にモデル化が困難になるが、画像は画像と異なる場合にモデル化が困難になる。画像は画像と異なる場合にモデル化が困難になる。

- [Visual Prompt Tuning for Few-Shot Text Classification](https://aclanthology.org/2022.coling-1.492)
  - Jingyuan Wen, Yutian Luo, Nanyi Fei, Guoxing Yang, Zhiwu Lu, Hao Jiang, Jie Jiang, Zhao Cao
  - **TLDR**: 事前学習済みモデルを、事前学習済みモデルに追加する手法の提案。事前学習済みモデルは、事前学習済みモデルの画像を入力とするタスクに適用する。事前学習済みモデルは、画像を入力とするタスクに適用する。事前学習済みモデルは、事前学習済みモデルの画像を入力とするタスクに適用する。事前学習済みモデルは、事前学習済みモデルの画像を入力とするタスクに適用する。

- [Systematic Analysis of Image Schemas in Natural Language through Explainable Multilingual Neural Language Processing](https://aclanthology.org/2022.coling-1.493)
  - Lennart Wachowiak, Dagmar Gromann
  - **TLDR**: 自然言語処理における、画像の分類器の提案。画像の分類器は、自然言語処理のモデルと同等のモデルが必要で、画像の分類器は、画像の画像を分類器の分類器に分類する。画像の分類器は、画像の画像を分類器の分類器に分類する。画像の分類器は、画像の画像分類器と同等のモデルが必要で、画像分類器の分類器は、画像分類器の分類器を学習する。

- [How to Adapt Pre-trained Vision-and-Language Models to a Text-only Input?](https://aclanthology.org/2022.coling-1.494)
  - Lovisa Hagström, Richard Johansson
  - **TLDR**: マルチモーダル学習モデルの学習方法を検証した研究。マルチモーダル学習モデルは、事前学習済みモデルと同等の学習性能が得られるが、事前学習済みモデルは言語理解が不十分だった。そこで、事前学習済みモデルを言語理解に適応させるための工夫を検証した研究。事前学習済みモデルは言語理解が不十分だったが、言語モデルは言語理解が良好だった。

- [ACT-Thor: A Controlled Benchmark for Embodied Action Understanding in Simulated Environments](https://aclanthology.org/2022.coling-1.495)
  - Michael Hanna, Federico Pedeni, Alessandro Suglia, Alberto Testoni, Raffaella Bernardi
  - **TLDR**: 強化学習において、画像を入力とし、画像の予測結果を入力とし、入力と画像の予測結果を入力とし、入力と画像の予測結果を入力とし、入力と画像の予測結果を入力とし、入力と画像の予測結果を入力とし、入力と画像の予測結果を入力とし、入力と画像の予測結果を入力とし、入力と画像の予測結果を入力とし、画像の予測結果を入力とし、画像の予測結果を入力とし、画像の予測結果を入力とし、画像の予測結果を入力とし、画像の予測結果を入力とし、画像の予測結果を入力

- [In-the-Wild Video Question Answering](https://aclanthology.org/2022.coling-1.496)
  - Santiago Castro, Naihao Deng, Pingxuan Huang, Mihai Burzo, Rada Mihalcea
  - **TLDR**: 自然言語処理で、自然環境で撮影された動画を理解するためのデータセットの提案。自然言語処理では、動画の撮影時に自然な音声を認識するモデルが一般的だが、このモデルはBLEUのモデルと同等の精度を維持している。

- [Towards Better Semantic Understanding of Mobile Interfaces](https://aclanthology.org/2022.coling-1.497)
  - Srinivas Sunkara, Maria Wang, Lijuan Liu, Gilles Baechler, Yu-Chung Hsiao, Jindong Chen, Abhanshu Sharma, James W. W. Stout
  - **TLDR**: モバイルデバイス上で使用されているUIの改善を目指した研究。画像/テキスト/画像の3つのタスクをタスクごとにタスクごとにタスクを割り当て、タスクごとにタスクを割り当てる形でタスクを更新する。タスクを割り当てるタスクは、タスクの重みを入力とするタスクと、タスクの重みを入力とするタスクの2つに分けている。タスクの重みは、タスクによって異なる。タスクの重みは、タスクによって異なる。タスクの重みは、タスクによって異なる。タスクの重みは、タスクによって異なる。タスクの重みは、タスク

- [End-to-end Dense Video Captioning as Sequence Generation](https://aclanthology.org/2022.coling-1.498)
  - Wanrong Zhu, Bo Pang, Ashish V. Thapliyal, William Yang Wang, Radu Soricut
  - **TLDR**: 動画中のイベントを、画像のエンティティを表現するタスクとして、End-to-Endの動画キャプチャーを学習する研究。タスクは、動画中のイベントを、画像のエンティティティを表現するタスクと、イベントのエンティティティを表現するタスクを分けて学習する。

- [SANCL: Multimodal Review Helpfulness Prediction with Selective Attention and Natural Contrastive Learning](https://aclanthology.org/2022.coling-1.499)
  - Wei Han, Hui Chen, Zhen Hai, Soujanya Poria, Lidong Bing
  - **TLDR**: マルチモーダルレビューの評価を、自然なAttentionで行う研究。Attentionは、レビューの評価に与える影響を考慮する。評価の結果を、評価の結果と比較し、評価の結果と比較した結果を比較し、評価の結果と比較した結果を比較し、評価の結果と比較した結果を比較し、評価の結果と比較した結果を比較し、評価の結果と比較した結果を比較し、評価の結果と比較した結果を比較し、評価の結果と比較した結果を比較し、評価の結果と比較した結果を比較し、評価の結果と比較した結果を比較し、評価の結果

- [Dual Capsule Attention Mask Network with Mutual Learning for Visual Question Answering](https://aclanthology.org/2022.coling-1.500)
  - Weidong Tian, Haodong Li, Zhong-Qiu Zhao
  - **TLDR**: 画像と質問を同時に処理するVQAの研究。画像は通常の画像と同等の情報量で処理されるが、質問は画像と質問の情報を統合する。画像は通常の画像と同等の情報量で処理されるが、質問は質問と同等の情報量で処理される。画像は通常の画像と同等の情報量で処理されるが、質問は質問と同等の情報量で処理される。

- [Emergence of Hierarchical Reference Systems in Multi-agent Communication](https://aclanthology.org/2022.coling-1.501)
  - Xenia Ohmer, Marko Duda, Elia Bruni
  - **TLDR**: 自然言語における、階層的な表現の強化について研究した研究。階層的な表現は、通常の表現と異なり、階層的な表現を組み合わせた表現を表現する。この組み合わせは、階層的な表現を表現する表現の表現を表現する表現に近い。階層的な表現は、階層的な表現を表現する表現に近い表現を表現する表現に近い表現を表現する。

- [Scene Graph Modification as Incremental Structure Expanding](https://aclanthology.org/2022.coling-1.502)
  - Xuming Hu, Zhijiang Guo, Yu Fu, Lijie Wen, Philip S. Yu
  - **TLDR**: 画像のグラフを更新する研究。画像の画像を生成するグラフを、自然言語で更新する。画像の画像を生成するグラフを、画像の画像を生成するグラフに置き換える形で作成する。画像の画像を生成するグラフを、画像の画像を生成するグラフに置き換える形で作成する。

- [Overcoming Language Priors in Visual Question Answering via Distinguishing Superficially Similar Instances](https://aclanthology.org/2022.coling-1.503)
  - Yike Wu, Yu Zhao, Shiwan Zhao, Ying Zhang, Xiaojie Yuan, Guoqing Zhao, Ning Jiang
  - **TLDR**: 質問文と言語事前の相関を強化する研究。質問文と言語事前の相関は、質問文の文長が近い場合に発生する。そのため、質問文の文長を短くする(短い文長は言語事前の予測に影響する)ことで、言語事前の予測を強化する。

- [Efficient Multilingual Multi-modal Pre-training through Triple Contrastive Loss](https://aclanthology.org/2022.coling-1.504)
  - Youhan Lee, KyungTae Lim, Woonhyuk Baek, Byungseok Roh, Saehoon Kim
  - **TLDR**: マルチモーダルで画像とテキストをペアにした研究。画像は英語のみで、テキストは英語のみでペアを作成し、画像とテキストのペアをペアにすることで、画像とテキスト双方をペアにする。ペアは画像とテキスト双方をペアにする形で学習を行う。

- [LOViS: Learning Orientation and Visual Signals for Vision and Language Navigation](https://aclanthology.org/2022.coling-1.505)
  - Yue Zhang, Parisa Kordjamshidi
  - **TLDR**: 自然言語で指示を受け取る際、画像情報だけでなく、位置情報も加味したモデルを提案。画像情報の加味は、指示の画像を認識するタスクで行う。位置情報の加味は、指示の画像を認識するタスクで行う。位置情報の加味は、画像を認識するタスクで行う。

- [GAP: A Graph-aware Language Model Framework for Knowledge Graph-to-Text Generation](https://aclanthology.org/2022.coling-1.506)
  - Anthony Colas, Mehrdad Alvandipour, Daisy Zhe Wang
  - **TLDR**: 事前学習済み言語モデルを、事前学習済みモデルに組み込んだ研究。事前学習済みモデルは、事前学習済み言語モデルの表現を予測するモデルで、事前学習済みモデルは、事前学習済み言語モデルの表現を予測するモデルで、事前学習済みモデルは、事前学習済み言語モデルの表現を予測するモデルで学習する。

- [Content Type Profiling of Data-to-Text Generation Datasets](https://aclanthology.org/2022.coling-1.507)
  - Ashish Upadhyay, Stewart Massie
  - **TLDR**: イベントの概要を生成する際、各イベントの概要を分類する typologyを提案。各イベントの概要は、イベントの更新履歴から生成される。イベントの概要は、イベントの更新履歴から生成される。イベントの概要は、更新履歴から生成される。

- [CoLo: A Contrastive Learning Based Re-ranking Framework for One-Stage Summarization](https://aclanthology.org/2022.coling-1.508)
  - Chenxin An, Ming Zhong, Zhiyong Wu, Qin Zhu, Xuanjing Huang, Xipeng Qiu
  - **TLDR**: 文書から要約を行う際に、事前学習済みモデルを再構築する手法の提案。事前学習済みモデルは、文書レベルの要約を生成するタスクを学習する。要約は文書レベルの要約と、文書レベルの要約は要約の要約を生成するタスクを分けている。文書レベルの要約は、文書レベルの要約と同等の要約を生成する。

- [Of Human Criteria and Automatic Metrics: A Benchmark of the Evaluation of Story Generation](https://aclanthology.org/2022.coling-1.509)
  - Cyril Chhun, Pierre Colombo, Fabian M. Suchanek, Chloé Clavel
  - **TLDR**: 自動生成のモデルで、人間評価と自動評価の相関を検証した研究。自動評価は、モデルのモデルの評価と同等の評価が行えるか、また同等の評価が行えるかを検証している。自動評価は、モデルの評価と同等の評価が行えるか、また同等の評価が行えるかを検証している。

- [Selective Token Generation for Few-shot Natural Language Generation](https://aclanthology.org/2022.coling-1.510)
  - Daejin Jo, Taehwan Kwon, Eun-Sol Kim, Sungwoong Kim
  - **TLDR**: 事前学習済み言語モデル(PLM)を強化学習で学習する研究。事前学習済みモデルは事前学習済み言語モデルの学習済み言語モデルと同等の学習データが必要で、事前学習済みモデルの学習済み言語モデルを学習させる。事前学習済みモデルは事前学習済み言語モデルの学習済み言語モデルと同等の学習データが必要で、事前学習済み言語モデルの学習済み言語モデルを学習させる。

- [A-TIP: Attribute-aware Text Infilling via Pre-trained Language Model](https://aclanthology.org/2022.coling-1.511)
  - Dongyuan Li, Jingyi You, Kotaro Funakoshi, Manabu Okumura
  - **TLDR**: テキストを追加する際、どの部分を追加するかを予測するAttribute-Awareの仕組みを提案。既存のAttribute-Awareは、追加された部分を予測するモデルがベースだが、このモデルは事前学習済み言語モデル(A-TIP)をベースにしている。

- [Multi Graph Neural Network for Extractive Long Document Summarization](https://aclanthology.org/2022.coling-1.512)
  - Xuan-Dung Doan, Le-Minh Nguyen, Khac-Hoai Nam Bui
  - **TLDR**: 文書から文書要約を生成する手法の提案。文書の文脈をグラフで表現し、文脈をグラフから文書要約に変換する。文書要約は文書全体の文脈を表現するが、文書要約は文書全体の文脈を表現する。文書要約は文書全体の文脈を表現するが、文書要約は文書全体の文脈を表現する。文書要約は文書全体の文脈を表現するが、文書要約は文書全体の文脈を表現する。

- [Improving Zero-Shot Multilingual Text Generation via Iterative Distillation](https://aclanthology.org/2022.coling-1.513)
  - Ernie Chang, Alex Marin, Vera Demberg
  - **TLDR**: マルチタスクで使用する言語モデルを、事前学習済みモデルに転移する研究。事前学習済みモデルは、事前学習済み言語モデルの学習済み言語モデルの学習済み言語モデルを学習させる。事前学習済みモデルは、事前学習済み言語モデルの学習済み言語モデルの学習済み言語モデルを学習させる。事前学習済み言語モデルは、事前学習済み言語モデルの学習済み言語モデルを学習させる。

- [Using Structured Content Plans for Fine-grained Syntactic Control in Pretrained Language Model Generation](https://aclanthology.org/2022.coling-1.514)
  - Fei-Tzin Lee, Miguel Ballesteros, Feng Nan, Kathleen McKeown
  - **TLDR**: 文生成において、文の表現を自動生成する手法の提案。文生成は、文の表現を自動生成するモデルで行う。生成は、文の表現を自動生成するモデルで行う。生成は、文の表現を自動生成するモデルで行う。生成は、文の表現を自動生成するモデルで行う。

- [PrefScore: Pairwise Preference Learning for Reference-free Summarization Quality Assessment](https://aclanthology.org/2022.coling-1.515)
  - Ge Luo, Hebi Li, Youbiao He, Forrest Sheng Bao
  - **TLDR**: 機械学習モデルで文書を作成する際、文書の文長が人間評価に与える影響を調べた研究。文書の文長は、文書の文長と同等に重要であるとし、文長が人間評価に与える影響を調べた研究。文書の文長は、文書の文長と同等であるとし、文長が人間評価に与える影響を調べた研究。文書の文長は、文書の文長と同等であるとし、文書の文長は、文書の文長と同等であるとしている。

- [Multi-Attribute Controlled Text Generation with Contrastive-Generator and External-Discriminator](https://aclanthology.org/2022.coling-1.516)
  - Guisheng Liu, Yi Li, Yanqing Guo, Xiangyang Luo, Bo Wang
  - **TLDR**: マルチAttributeの生成を行う際に、生成したテキストを個別Attributeに分割して生成する手法の提案。個別Attributeは個別に生成するのではなく、個別に生成したテキストを個別Attributeに分割し、個別Attributeの生成を個別Attributeの生成に置き換える。個別Attributeの生成は、個別Attributeの生成に置き換える。

- [Coordination Generation via Synchronized Text-Infilling](https://aclanthology.org/2022.coling-1.517)
  - Hiroki Teranishi, Yuji Matsumoto
  - **TLDR**: 事前学習済み言語モデルで、単語の位置を表現するMaskを生成する研究。Maskは単語の位置を表現する単語の位置を表現する単語の位置を表現する単語の位置を表現する単語の位置を表現する単語の位置を表現する単語の位置を表現する単語の位置を表現する単語の位置を表現する単語の位置を表現する単語の位置を表現する単語の位置を表現する単語の位置を表現する単語の位置を表現する単語の位置を表現する単語の位置を表現する単語の位置を表現する単語の位置を表現する単語の位置を表現する単語の位置を表現する単語の位置を

- [KHANQ: A Dataset for Generating Deep Questions in Education](https://aclanthology.org/2022.coling-1.518)
  - Huanli Gong, Liangming Pan, Hengchang Hu
  - **TLDR**: 学習データセットの生成を自動化する研究。学習データセットの生成は、学習データセットの生成と同等のタスクで行う必要がある。生成は、学習データセットの生成と同等のタスクで行う必要がある。生成は、学習データセットの生成と同等のタスクで行う必要がある。

- [Multi-Figurative Language Generation](https://aclanthology.org/2022.coling-1.519)
  - Huiyuan Lai, Malvina Nissim
  - **TLDR**: マルチフレームワークで、自然言語処理のモデルを学習する研究。マルチフレームワークは、単語の意味を表現する単語を生成するモデル(BART)をベースに、単語の意味を表現する単語を生成するモデル(mFLAG)をベースに学習する。

- [Enhancing Task-Specific Distillation in Small Data Regimes through Language Generation](https://aclanthology.org/2022.coling-1.520)
  - Husam Quteineh, Spyridon Samothrakis, Richard Sutcliffe
  - **TLDR**: 自然言語処理で、教師モデルから生徒モデルに学習データを転移する研究。教師モデルは学習データの分布を予測するモデルで、生徒モデルは学習データの分布を予測するモデルで学習を行う。学習データは教師モデルの学習データと、教師モデルの学習データの2つから生成する。

- [Boosting Code Summarization by Embedding Code Structures](https://aclanthology.org/2022.coling-1.521)
  - Jikyoeng Son, Joonghyuk Hahn, HyeonTae Seo, Yo-Sub Han
  - **TLDR**: コードの要約を行う際に、文構造をグラフに入力する手法の提案。文構造をグラフに入力するモデルを、文構造をグラフに入力するモデルに置き換える。文構造を入力するモデルはBLEUで6.67%、ROUGEで74%の精度向上を確認。

- [Comparative Graph-based Summarization of Scientific Papers Guided by Comparative Citations](https://aclanthology.org/2022.coling-1.522)
  - Jingqiang Chen, Chaoxiang Cai, Xiaorui Jiang, Kejia Chen
  - **TLDR**: 論文の概要をグラフベースでまとめる研究。論文の概要は、論文のタイトルと著者名、著者と論文の関連度(Salirity)、著者と論文の関連度(Research)、著者と論文の関連度(Research)、著者と論文の関連度(Research)、関連度(Research)の3つから構成されている。

- [JPG - Jointly Learn to Align: Automated Disease Prediction and Radiology Report Generation](https://aclanthology.org/2022.coling-1.523)
  - Jingyi You, Dongyuan Li, Manabu Okumura, Kenji Suzuki
  - **TLDR**: 放射線診断の画像診断モデルのJoint Learningを、画像とテキストのペアで学習した研究。画像とテキストのペアは、画像の画像とテキストの画像をペアにし、画像とテキストのペアを入力とする。画像とテキストのペアは、画像とテキストのペアを入力とする。画像は、画像とテキストのペアを入力とする。

- [Automatic Nominalization of Clauses](https://aclanthology.org/2022.coling-1.524)
  - John S. Y. Lee, Ho Hung Lim, Carol Webster, Anton Melser
  - **TLDR**: 文のノルムを表現する手法の提案。ノルムは単語の位置を表す単語(=単語の位置)を、単語の位置を表す単語(=単語の位置)を表す単語(=単語の位置)を表す単語(=単語位置)を表す単語(=位置)を表す単語(=位置)を表す単語(=位置)に置き換える形で表現する。

- [Benchmarking Compositionality with Formal Languages](https://aclanthology.org/2022.coling-1.525)
  - Josef Valvoda, Naomi Saphra, Jonathan Rawski, Adina Williams, Ryan Cotterell
  - **TLDR**: 自然言語処理で、各言語モデルの特性を調べた研究。各言語モデルの特性を調べた結果、学習率が高いモデルは学習率が低いモデルと比較して学習率が低いモデルと比較して学習率が低いモデルと比較して、学習率が高いモデルは学習率が低いモデルと比較して学習率が高いモデルと比較して学習率が高いモデルを比較している。

- [Source-summary Entity Aggregation in Abstractive Summarization](https://aclanthology.org/2022.coling-1.526)
  - José Ángel González, Annie Louis, Jackie Chi Kit Cheung
  - **TLDR**: 文書中の単語を、その意味を含まない形で表現する手法の提案。単語は、単語の意味を含まない文書に置き換えられる。単語は、単語の意味を含まない文書に置き換えられる。単語は、単語の意味を含まない文書に置き換えられる。単語は、単語の意味を含まない文書に置き換えられる。単語の意味を含まない文書は、単語の意味を含まない文書に置き換えられる。

- [How to Find Strong Summary Coherence Measures? A Toolbox and a Comparative Study for Summary Coherence Measure Evaluation](https://aclanthology.org/2022.coling-1.527)
  - Julius Steen, Katja Markert
  - **TLDR**: 事前学習済みモデルの事前学習済みモデルの事前学習済みモデルの事前学習済みモデルの事前学習済みモデルの事前学習済みモデルの事前学習済みモデルの事前学習済みモデルの事前学習済みモデルの事前学習済みモデルの事前学習済みモデルの事前学習済みモデルの事前学習済みモデルの事前学習済みモデルの事前学習済みモデルの事前学習済みモデルの事前学習済みモデルの事前学習済みモデルの事前学習済みモデルの事前学習済みモデルの事前学習済みモデルの事前学習済みモデルの事前学習済みモデルの事前学習済みモデルの事前学習済みモデルの事前学習済みモデルの事前学習済みモデルの事前学習済みモデルの事前学習済みモデルの事前学習

- [Summarizing Dialogues with Negative Cues](https://aclanthology.org/2022.coling-1.528)
  - Junpeng Liu, Yanyan Zou, Yuxuan Xi, Shengjie Li, Mian Ma, Zhuoye Ding
  - **TLDR**: 対話システムの要約を、事前学習済みモデルの学習に組み込む手法。事前学習済みモデルは、要約の生成時に要約の要約箇所を消す(要約箇所は要約の要約箇所に置き換える)ことで、要約箇所を消す(要約箇所は要約箇所に置き換える)ことで、要約の要約精度を向上させる。

- [ALEXSIS-PT: A New Resource for Portuguese Lexical Simplification](https://aclanthology.org/2022.coling-1.529)
  - Kai North, Marcos Zampieri, Tharindu Ranasinghe
  - **TLDR**: マルチベクターの単語を自動生成する研究。マルチベクターの単語を生成するモデルは、通常のマルチベクターのモデルと異なり、マルチベクターの単語を生成するモデルを併用する。マルチベクターの単語を生成するモデルは、マルチベクターの単語を生成するモデルと、マルチベクターの単語を生成するモデルを併用する。

- [APPDIA: A Discourse-aware Transformer-based Style Transfer Model for Offensive Social Media Conversations](https://aclanthology.org/2022.coling-1.530)
  - Katherine Atwell, Sabit Hassan, Malihe Alikhani
  - **TLDR**: 投稿に対する不審なコメントを、既存の対話システムで転移する研究。既存の対話システムは、投稿に対する不審なコメントを転移する際の文脈を予測するが、この文脈を予測するモデルは、文脈の予測が難しい場合に転移する。そのため、文脈を予測するモデルを導入している。

- [View Dialogue in 2D: A Two-stream Model in Time-speaker Perspective for Dialogue Summarization and beyond](https://aclanthology.org/2022.coling-1.531)
  - Keli Xie, Dongchen He, Jiaxin Zhuang, Siyuan Lu, Zhongfeng Wang
  - **TLDR**: 対話システムの2つのフレームを組み合わせたモデルの提案。2つのフレームを組み合わせたモデルは、文の要約や文書の翻訳など様々なタスクで有効。文の要約は、文の要約と文の翻訳を同時に行うことで、文の要約を同時に行うことができる。

- [Denoising Large-Scale Image Captioning from Alt-text Data Using Content Selection Models](https://aclanthology.org/2022.coling-1.532)
  - Khyathi Raghavi Chandu, Piyush Sharma, Soravit Changpinyo, Ashish V. Thapliyal, Radu Soricut
  - **TLDR**: 画像のタイトルを予測するタスクを、シンプルな手法で解く研究。タイトルの予測は、タイトルの単語を予測する形で行う。タイトルの単語は、タイトルの画像を生成する際の単語として使用される。タイトルの単語は、タイトルの画像を生成する際の単語として使用される。

- [Meta-CQG: A Meta-Learning Framework for Complex Question Generation over Knowledge Bases](https://aclanthology.org/2022.coling-1.533)
  - Kun Zhang, Yunqi Qiu, Yuanzhuo Wang, Long Bai, Wei Li, Xuhui Jiang, Huawei Shen, Xueqi Cheng
  - **TLDR**: 複雑な質問を生成する手法の提案。質問の種類や文構造、また文構造を学習する際の学習方法(学習済みモデルの学習)を学習する。学習は、質問の文構造、文構造、文構造を学習する。学習は、文構造を学習するモデルの学習と、文構造を学習するモデルの学習を交互に行う。

- [Graph-to-Text Generation with Dynamic Structure Pruning](https://aclanthology.org/2022.coling-1.534)
  - Liang Li, Ruiying Geng, Bowen Li, Can Ma, Yinliang Yue, Binhua Li, Yongbin Li
  - **TLDR**: 文書構造をグラフ構造に置き換える研究。文書構造は、文書構造の各ノードの位置関係を考慮する。各ノードは、文書構造の位置関係を考慮するノードとして認識する。文書構造は、文書構造の位置関係を考慮するノードを追加することで、文書構造を認識する。

- [Multi-Perspective Document Revision](https://aclanthology.org/2022.coling-1.535)
  - Mana Ihori, Hiroshi Sato, Tomohiro Tanaka, Ryo Masumura
  - **TLDR**: 文書を更新する際、文書の各フレーズを更新する際のタスクをマルチスペクトで学習する研究。文書の更新は文書全体の更新ではなく、文書の各フレーズを更新するタスクを想定している。文書更新は文書全体の更新ではなく、文書全体の更新を想定している。文書更新は文書全体の更新と同等の精度を達成できる。

- [A Survey of Automatic Text Summarization Using Graph Neural Networks](https://aclanthology.org/2022.coling-1.536)
  - Marco Ferdinand Salchner, Adam Jatowt
  - **TLDR**: 自動文要約の研究。文書の要約をグラフ表現で作成し、文書の要約をグラフ表現に入力する。文書の要約は文書の分類/分類結果から生成するが、文書分類結果から要約を生成する際は要約の要約を生成する。文書分類結果から要約を生成する際は要約の要約を生成するモデルを使用。

- [Phrase-Level Localization of Inconsistency Errors in Summarization by Weak Supervision](https://aclanthology.org/2022.coling-1.537)
  - Masato Takatsuka, Tetsunori Kobayashi, Yoshihiko Hayashi
  - **TLDR**: 要約の誤りを検出する研究。要約文を生成する際、文の重みを逆算する(文の重みは文長の単語を予測する)、文の重みを逆算する(文長の単語を予測する)、文長の単語を予測する(文長の単語を予測する)、文長の単語を予測する(文長の単語を予測する)、文長の単語を予測する(文長の単語を予測する)、文長の単語を予測する(文長の単語を予測する)、文長の単語を予測する(文長の単語を予測する)、文長

- [PoliSe: Reinforcing Politeness Using User Sentiment for Customer Care Response Generation](https://aclanthology.org/2022.coling-1.538)
  - Mauajama Firdaus, Asif Ekbal, Pushpak Bhattacharyya
  - **TLDR**: 対話システムで、ユーザーの感情を入力に変換する研究。入力は、ユーザーの感情を入力とするTransformerベースのEncoderDecoderで、入力は、ユーザーの感情を入力とするTransformerベースのEncoderDecoderで行う。

- [Focus-Driven Contrastive Learning for Medical Question Summarization](https://aclanthology.org/2022.coling-1.539)
  - Ming Zhang, Shuai Dou, Ziyang Wang, Yunfang Wu
  - **TLDR**: 医療従事者に対する質問に対する事前学習を行う研究。質問の要約は、質問の要約と同等の要約の2つに分類される。要約は、要約の文を「質問」と「要約」の要約から抽出する。要約は、要約の要約文を「要約」と「要約」の要約文から抽出する。要約は、要約文の要約文を「要約」と「要約」の要約文から抽出する。

- [ArgLegalSumm: Improving Abstractive Summarization of Legal Documents with Argument Mining](https://aclanthology.org/2022.coling-1.540)
  - Mohamed Elaraby, Diane Litman
  - **TLDR**: 文書の要約を、文書構造を重視したモデルで作成する研究。文書構造を重視するモデルは、文書の要約を文書構造と見なし、文書構造を重視するモデルをベースにしている。文書構造を重視するモデルは、文書構造を重視するモデルと比較して、文書構造を重視するモデルを比較している。

- [Semantic Overlap Summarization among Multiple Alternative Narratives: An Exploratory Study](https://aclanthology.org/2022.coling-1.541)
  - Naman Bansal, Mousumi Akter, Shubhra Kanti Karmaker
  - **TLDR**: 文単位のOverlapを生成するタスクの提案。文単位のOverlapは、文の意味を含まない単語を含まない単語に置き換える(Overlapは、文単位の単語を含まない単語に置き換える)ことで、文単位のOverlapを生成する。文単位のOverlapは、文単位のOverlapと同等の精度を達成。

- [Analyzing the Dialect Diversity in Multi-document Summaries](https://aclanthology.org/2022.coling-1.542)
  - Olubusayo Olabisi, Aaron Hudson, Antonie Jetter, Ameeta Agrawal
  - **TLDR**: ニュースから、異なる視点からの情報を得るための手法を検証した研究。ニュースから、Twitterから、ニュースから、ニュースから、ニュースから、ニュースから、ニュースから、ニュースから、ニュースから、ニュースから、ニュースから、ニュースから、ニュースから、ニュースから、ニュースから、ニュースから、ニュースから、ニュースから、ニュースから、ニュースから、ニュースから、ニュースから、ニュースから、ニュースから、ニュースから、ニュースから、ニュースから、ニュースから、ニュースから、ニュースから、ニュースから、ニュースから、ニュースから、ニュースから、ニュースから、ニュースから、ニュースから、ニュースから、ニュースから、ニュースから、ニュースから、ニュースから、ニュースから、ニュースから、

- [Multi-Document Scientific Summarization from a Knowledge Graph-Centric View](https://aclanthology.org/2022.coling-1.543)
  - Pancheng Wang, Shasha Li, Kunyuan Pang, Liangliang He, Dong Li, Jintao Tang, Ting Wang
  - **TLDR**: 論文の概要を文書から抽出する研究。文書の概要は文書全体の構成を表すものとして、文書のタイトル/本文/論文の概要/論文の概要/論文の概要/文書の概要/文書の概要/文書の概要/文書の概要/文書の概要/文書の概要/文書の概要を抽出する。文書の概要は文書全体の構成を表すものとして文書全体の構成を表すものとして文書全体の構成を表すものとして文書全体の構成を表すものとして文書全体の構成を表すものとして文書全体の構成を表すものとして文書

- [Generation of Patient After-Visit Summaries to Support Physicians](https://aclanthology.org/2022.coling-1.544)
  - Pengshan Cai, Fei Liu, Adarsha Bajracharya, Joe Sills, Alok Kapoor, Weisong Liu, Dan Berlowitz, David Levy, Richeek Pradhan, Hong Yu
  - **TLDR**: 事前訪問文書の自動生成について、事前訪問文書の生成に適した手法を調査した研究。事前訪問文書は文書の概要をまとめるための重要な文書で、文書の概要は文書の分類器で抽出する。文書の分類器は、文書の分類器の分類結果から、文書分類器の分類結果から、文書分類器の分類結果から、文書分類器の分類結果から、文書分類器の分類結果から、文書分類器の分類結果から、文書分類器の分類結果から、文書分類器の分類結果から、文書分類器の分類結果から、文書分類器の分類結果から、文書分類器の分類結果から、

- [HeterGraphLongSum: Heterogeneous Graph Neural Network with Passage Aggregation for Extractive Long Document Summarization](https://aclanthology.org/2022.coling-1.545)
  - Tuan-Anh Phan, Ngoc-Dung Ngoc Nguyen, Khac-Hoai Nam Bui
  - **TLDR**: 文書から文書を抽出する手法の提案。文書の単語、文、文書の順番で抽出を行う。文書の順番は文書全体の表現に依存するが、文書全体の表現は文書全体の表現に依存しない。文書全体の表現を抽出する際、文書全体の表現を抽出する文書構造を追加することで、文書全体の表現を抽出する。

- [GRETEL: Graph Contrastive Topic Enhanced Language Model for Long Document Extractive Summarization](https://aclanthology.org/2022.coling-1.546)
  - Qianqian Xie, Jimin Huang, Tulika Saha, Sophia Ananiadou
  - **TLDR**: 事前学習済み言語モデル(SOTA)と、事前学習済みモデル(GRETEL)を組み合わせた研究。事前学習済みモデルは、事前学習済み言語モデルと同等の表現を学習するが、事前学習済みモデルは文書全体の表現を学習する。文書全体の表現を学習する際は、文書全体の表現を学習する。文書全体の表現を学習する際は、文書全体の表現を学習する。

- [PINEAPPLE: Personifying INanimate Entities by Acquiring Parallel Personification Data for Learning Enhanced Generation](https://aclanthology.org/2022.coling-1.547)
  - Sedrick Scott Keh, Kevin Lu, Varun Gangal, Steven Y. Feng, Harsh Jhamtani, Malihe Alikhani, Eduard Hovy
  - **TLDR**: 自然言語処理で、自然言語処理のモデルを学習する研究。入力に入力される単語を、自然言語処理のモデルで生成する。生成した単語を、自然言語処理のモデルで学習する。学習は、入力の単語を入力に入力する単語ベクトルから行う。入力は、入力の単語ベクトルを入力に入力する形で行う。

- [Mind the Gap! Injecting Commonsense Knowledge for Abstractive Dialogue Summarization](https://aclanthology.org/2022.coling-1.548)
  - Seungone Kim, Se June Joo, Hyungjoo Chae, Chaehyeong Kim, Seung-won Hwang, Jinyoung Yeo
  - **TLDR**: 対話中の質問に対し、質問に対する共通点を推論する研究。質問に対する共通点は、質問に対する質問に対する共通点の推論結果と同等か、それ以上の質問に対する共通点の推論結果と同等かを推定する。質問に対する共通点の推論結果は、質問に対する共通点の推論結果と同等か否かを推定する。

- [Type-dependent Prompt CycleQAG : Cycle Consistency for Multi-hop Question Generation](https://aclanthology.org/2022.coling-1.549)
  - Seungyeon Lee, Minho Lee
  - **TLDR**: マルチホップのQAモデルの提案。質問文を生成する際、質問文の単語を複数選択する(複数選択は単語分散表現の生成に使用する)。複数選択は、質問文の単語分散表現を複数選択する(複数選択は単語分散表現の生成に使用する)ことで、マルチホップのQAモデルをより精度の高いものにしている。

- [UPER: Boosting Multi-Document Summarization with an Unsupervised Prompt-based Extractor](https://aclanthology.org/2022.coling-1.550)
  - Shangqing Tu, Jifan Yu, Fangwei Zhu, Juanzi Li, Lei Hou, Jian-Yun Nie
  - **TLDR**: 事前学習済み言語モデルを、事前学習済みモデルの学習済みモデルに入力し、事前学習済みモデルの学習済みモデルから文書のPageRankを予測する研究。文書のPageRankは、文書のPageRankと文書のPageRankの2つに分け、文書のPageRankは文書のPageRankとPageRankの2つに分け、PageRankは文書のPageRankと文書のPageRankの2つに分け、PageRankは文書のPageRankと文書のPageRankの2つに分け、PageRankは文書のPageRankと文書のPageRankの2つに分け、Page

- [DISK: Domain-constrained Instance Sketch for Math Word Problem Generation](https://aclanthology.org/2022.coling-1.551)
  - Tianyang Cao, Shuang Zeng, Xiaodan Xu, Mairgup Mansur, Baobao Chang
  - **TLDR**: 自然言語処理で、モデルの生成を行う際に、モデルの生成結果をドメイン知識に基づいて予測する手法の提案。ドメイン知識は、モデルの生成結果を予測するモデルの枠組みで、予測結果を予測するモデルの枠組みで予測する。予測結果は、モデルの生成結果と異なる場合に予測する。

- [Context-Tuning: Learning Contextualized Prompts for Natural Language Generation](https://aclanthology.org/2022.coling-1.552)
  - Tianyi Tang, Junyi Li, Wayne Xin Zhao, Ji-Rong Wen
  - **TLDR**: 自然言語処理で、モデルの学習を効率化する手法の提案。モデルは、モデルの学習済み言語モデルと、モデルの学習済み言語モデルの学習済み言語モデルの2つを統合し、学習済み言語モデルの学習済み言語モデルを学習する。学習済み言語モデルは、学習済み言語モデルの学習済み言語モデルを学習する。

- [PSP: Pre-trained Soft Prompts for Few-Shot Abstractive Summarization](https://aclanthology.org/2022.coling-1.553)
  - Xiaochen Liu, Yang Gao, Yu Bai, Jiawei Li, Yinan Hu, Heyan Huang, Boxing Chen
  - **TLDR**: 自然言語処理で、事前学習済みモデルをモデルの重みに組み込んだ研究。事前学習済みモデルは、文書の文脈を入力に入力するpromptを生成する。promptは文書の文脈を入力に入力するpromptと、文書の文脈を入力に入力するpromptの2つを生成する。promptは文書の文脈を入力に入力するpromptを生成する。

- [Continuous Decomposition of Granularity for Neural Paraphrase Generation](https://aclanthology.org/2022.coling-1.554)
  - Xiaodong Gu, Zhaowei Zhang, Sang-Woo Lee, Kang Min Yoo, Jung-Woo Ha
  - **TLDR**: 文生成において、文の構造を個別に予測する手法の提案。文の構造を個別に予測するのではなく、文の構造を個別に予測する。文構造は、文の長さ(長さ=長さ)、文長は文長の長さ(長さ=長さ)、文長は文長の長さ(長さ=長さ)、文長は長さ(長さ=長さ)、長さ=長さ(長さ)、長さ=長さ(長さ)、長さ=長さ(長さ)、長さ=長さ(長さ)、長さ=長さ(長さ)、長

- [Paraphrase Generation as Unsupervised Machine Translation](https://aclanthology.org/2022.coling-1.555)
  - Xiaofei Sun, Yufei Tian, Yuxian Meng, Nanyun Peng, Fei Wu, Jiwei Li, Chun Fan
  - **TLDR**: 教師なし翻訳で、文の文長が同じ文長の単語を複数生成する研究。文長が同じ文長の場合、文長の文長を予測するモデルを用い、文長の文長を予測するモデルを用い、文長の文長を予測するモデルを用い、文長の文長を予測するモデルを用い、文長の文長を予測するモデルを用い、文長の文長を予測するモデルを用い、文長の文長を予測するモデルを用い、文長の文長を予測するモデルを用い、文長の文長を予測するモデルを用い、文

- [Summarize, Outline, and Elaborate: Long-Text Generation via Hierarchical Supervision from Extractive Summaries](https://aclanthology.org/2022.coling-1.556)
  - Xiaofei Sun, Zijun Sun, Yuxian Meng, Jiwei Li, Chun Fan
  - **TLDR**: 文書生成において、文書の構造を生成する手法の提案。文書のタイトルは文書のタイトルと同等か否かを予測するが、文書のタイトルは文書のタイトルと同等か否かを予測する。文書のタイトルは文書のタイトルと同等か否かを予測するが、文書の文書構造は文書の文書構造と同等か否かを予測する。文書の文書構造は文書構造と同等か否かを予測する。

- [CoCGAN: Contrastive Learning for Adversarial Category Text Generation](https://aclanthology.org/2022.coling-1.557)
  - Xin Sheng, Linli Xu, Yinlong Xu, Changcun Bao, Huang Chen, Bo Ren
  - **TLDR**: 自然言語処理で、クラス分類のテキスト生成を行う際に、クラス分類の表現を「クラス分類」と「クラス分類」の2つに分割し、それらを「クラス分類」と「クラス分類」の2つに分割する手法の提案。分類の表現を「クラス分類」と「クラス分類」の2つに分割し、それらを「クラス分類」と「クラス分類」の2つに分割する。

- [An Efficient Coarse-to-Fine Facet-Aware Unsupervised Summarization Framework Based on Semantic Blocks](https://aclanthology.org/2022.coling-1.558)
  - Xinnian Liang, Jing Li, Shuangzhi Wu, Jiali Zeng, Yufan Jiang, Mu Li, Zhoujun Li
  - **TLDR**: 文書から、事前学習済み言語モデルの表現を抽出する研究。文書のタイトル/本文/文書中の単語/文を抽出する。文書のタイトル/本文/文書中の単語/文を抽出する。文書のタイトル/文章/単語/文を抽出する文書を、文書の文書分類器で分類し、文書分類器の分類結果から抽出する。文書分類器の分類結果から抽出する文書を、文書分類器の分類結果から抽出する。

- [CHAE: Fine-Grained Controllable Story Generation with Characters, Actions and Emotions](https://aclanthology.org/2022.coling-1.559)
  - Xinpeng Wang, Han Jiang, Zhihua Wei, Shanlin Zhou
  - **TLDR**: 自然言語処理で、文の生成を行う際に、文の構造をどうコントロールするかを検証した研究。文の構造は、文のタイトル/単語/画像/テキストの3つからなる。文の構造は、文のタイトル/画像/画像/テキストの3つからなる。文の生成は、文のタイトル/画像/テキストの3つからなる。

- [Chinese Couplet Generation with Syntactic Information](https://aclanthology.org/2022.coling-1.560)
  - Yan Song
  - **TLDR**: 文の生成を行う際に、文の構造を考慮する研究。文の構造は、単語の位置関係(位置=>位置=>位置=>位置=>位置=>位置=>位置=>位置=>位置=>位置=>位置=>位置=>位置=>位置=>位置=>位置=>位置=>位置=>位置=>位置=>位置=>位置=>位置=>位置=>位置=>位置=>位置=>位置=>位置=>位置=>位置=>位置=>位置=>位置=>位置=>位置=>位置=>位置=>位置=>位置=>位置=>

- [Noise-injected Consistency Training and Entropy-constrained Pseudo Labeling for Semi-supervised Extractive Summarization](https://aclanthology.org/2022.coling-1.561)
  - Yiming Wang, Qianren Mao, Junnan Liu, Weifeng Jiang, Hongdong Zhu, Jianxin Li
  - **TLDR**: 事前学習済みモデルの予測を、事前学習済みモデルの予測と比較して低クラスの予測に置き換えた研究。事前学習済みモデルの予測は、事前学習済みモデルの予測と同等のノイズを発生させる。これにより、事前学習済みモデルの予測と同等のノイズを発生させることができる。

- [Question Generation Based on Grammar Knowledge and Fine-grained Classification](https://aclanthology.org/2022.coling-1.562)
  - Yuan Sun, Sisi Liu, Zhengcuo Dan, Xiaobing Zhao
  - **TLDR**: 質問生成を行う際に、質問分類器を用いた研究。質問分類器は質問分類の分類器と同等の分類器を用いているが、分類器は質問分類器と同等の分類器を用いている。分類器は質問分類器と同等の分類器を用いているが、分類器は質問分類器と同等の分類器を用いている。

- [CM-Gen: A Neural Framework for Chinese Metaphor Generation with Explicit Context Modelling](https://aclanthology.org/2022.coling-1.563)
  - Yucheng Li, Chenghua Lin, Frank Guerin
  - **TLDR**: 自然言語処理で、自然言語処理のタスクをマルチタスクにまとめた研究。タスクは、自然言語処理のタスク(生成)、タスクの分類(分類)、タスクの分類(分類)、タスクの分類(分類)、タスクの分類(分類)、タスクの分類(分類)、タスクの分類(分類)、タスクの分類(分類)、タスクの分類(分類)、タスクの分類(分類)、タスクの分類(分類)、タスクの分類(分類)、タスクの分類(分類)、タスクの分類(分類)、タスクの分類(分類)、

- [Psychology-guided Controllable Story Generation](https://aclanthology.org/2022.coling-1.564)
  - Yuqiang Xie, Yue Hu, Yunpeng Li, Guanqun Bi, Luxi Xing, Wei Peng
  - **TLDR**: 自然言語処理で、人間をコントロールする研究。心理学のモデルを用いて、人間がどんな状態なのかを予測する。人間は行動を変えたり、変化させたりする傾向がある。そのため、行動を予測するモデルを用い、行動予測のモデルを用い、行動予測のモデルを用い、行動予測のモデルを用い、行動予測のモデルを用い、行動予測のモデルを用い、行動予測のモデルを用い、行動予測のモデルを用い、行動予測のモデルを用い、行動予測のモデルを用い、行動予測のモデルを用い、行動予測のモデルを用い、行動予測のモデルを用い

- [Few-shot Table-to-text Generation with Prefix-Controlled Generator](https://aclanthology.org/2022.coling-1.565)
  - Yutao Luo, Menghua Lu, Gongshen Liu, Shilin Wang
  - **TLDR**: 事前学習済み言語モデルを、事前学習済みモデルの生成に応用した研究。事前学習済みモデルは、事前学習済みモデルの生成に使用するパラメーターを予測する。パラメーターは、事前学習済みモデルの生成に使用するパラメーターと、事前学習済みモデルの生成に使用するパラメーターを予測する。事前学習済みモデルは、事前学習済みモデルの生成に使用するパラメーターを予測する。

- [Text Simplification of College Admissions Instructions: A Professionally Simplified and Verified Corpus](https://aclanthology.org/2022.coling-1.566)
  - Zachary W. Taylor, Maximus H. Chu, Junyi Jessy Li
  - **TLDR**: 高校への進学を支援する文書のスケールを、教師なしでスケールを落とした研究。教師なしでスケールを落とした文書は、教師なしでスケールを落とした文書と同等の精度が得られる。教師なし文書は、教師なし文書のスケールを上回る精度を達成。

- [On the Role of Pre-trained Language Models in Word Ordering: A Case Study with BART](https://aclanthology.org/2022.coling-1.567)
  - Zebin Ou, Meishan Zhang, Yue Zhang
  - **TLDR**: 自然言語処理で、文順序を予測するモデルの提案。単語の順序は、文関係の知識がないと予測できない。そのため、文関係の知識をベースに、文関係の予測を行なっている。文関係の知識は、文関係の予測に有効なモデルの役割を果たしている。

- [Visual Information Guided Zero-Shot Paraphrase Generation](https://aclanthology.org/2022.coling-1.568)
  - Zhe Lin, Xiaojun Wan
  - **TLDR**: 画像から、事前学習済みモデルの事前学習済みモデルを生成するViPGを提案。事前学習済みモデルは画像から事前学習済みモデルの事前学習済みモデルを生成する。事前学習済みモデルは事前学習済みモデルの事前学習済みモデルと、事前学習済みモデルの事前学習済みモデルの事前学習済みモデルを組み合わせる形で生成を行う。事前学習済みモデルは事前学習済みモデルの事前学習済みモデルと同等の精度を達成。

- [Improving Abstractive Dialogue Summarization with Speaker-Aware Supervised Contrastive Learning](https://aclanthology.org/2022.coling-1.569)
  - Zhichao Geng, Ming Zhong, Zhangyue Yin, Xipeng Qiu, Xuanjing Huang
  - **TLDR**: 対話システムで、質問文を生成する際のモデルが、質問文の文長が異なる2つの単語を予測する問題を解決した研究。単語レベルのSCL、グローバルレベルのSCL、文長が異なる2つのSCLを組み合わせて生成を行う。

- [Diversifying Neural Text Generation with Part-of-Speech Guided Softmax and Sampling](https://aclanthology.org/2022.coling-1.570)
  - Zhixian Yang, Pengxuan Xu, Xiaojun Wan
  - **TLDR**: 文生成モデルで、文の分類を学習する際の手法を提案。文分類は文の文構造を予測するが、文の分類は文の文構造を予測する。文の分類は文の文構造を予測するが、文の文構造を予測する際は文の文構造を予測する。文の文構造を予測する際は、文の文構造を予測する文を生成する。

- [Enhancing Pre-trained Models with Text Structure Knowledge for Question Generation](https://aclanthology.org/2022.coling-1.571)
  - Zichen Wu, Xin Jia, Fanyi Qu, Yunfang Wu
  - **TLDR**: 事前学習済み言語モデルで、質問文の構造をモデル化する手法の提案。質問文の構造は、質問文の文構造と、質問文の文構造を結合した形でモデル化する。質問文の構造は、質問文の文構造と、質問文の文構造を結合した形でモデル化する。

- [LFKQG: A Controlled Generation Framework with Local Fine-tuning for Question Generation over Knowledge Bases](https://aclanthology.org/2022.coling-1.572)
  - Zichu Fei, Xin Zhou, Tao Gui, Qi Zhang, Xuanjing Huang
  - **TLDR**: 自然言語処理で質問生成を行う際に、事前学習済みモデルを学習させる手法の提案。事前学習済みモデルは、質問文のEntityを予測するモデルで、Entityの予測は質問文のEntityを予測するモデルで行う。事前学習済みモデルは、Entityの予測が困難な場合、予測結果を予測するモデルに置き換える。

- [Demystifying Neural Fake News via Linguistic Feature-Based Interpretation](https://aclanthology.org/2022.coling-1.573)
  - Ankit Aich, Souvik Bhattacharya, Natalie Parde
  - **TLDR**: ニューラルネットの生成を行う際に、単語の意味を考慮する手法について調査した研究。単語の意味は、ニュースの更新頻度やニュースの更新頻度といった基本要素から、ニュースの更新頻度から、ニュースの更新頻度から、更新頻度から、更新頻度から、更新頻度から、更新頻度から、更新頻度から、更新頻度から、更新頻度から、更新頻度から、更新頻度から、更新頻度から、更新頻度から、更新頻度から、更新頻度から、更新頻度から、更新頻度から、更新頻度から、更新頻度から、更新頻度から、更新

- [Measuring Geographic Performance Disparities of Offensive Language Classifiers](https://aclanthology.org/2022.coling-1.574)
  - Brandon Lwowski, Paul Rad, Anthony Rios
  - **TLDR**: 自然言語処理のモデルで、地理的特徴が異なる地域で対応する研究。地理的特徴は、地理的特徴と異なる地域で対応するモデルの性能を左右する。そのため、地理的特徴が異なる地域で対応するモデルを適用する際は、地理的特徴を考慮したモデルを適用する必要がある。

- [Offensive Content Detection via Synthetic Code-Switched Text](https://aclanthology.org/2022.coling-1.575)
  - Cesa Salaam, Franck Dernoncourt, Trung Bui, Danda Rawat, Seunghyun Yoon
  - **TLDR**: 自然言語処理で、不法行為を扱うテキストを分類する研究。テキスト分類は、テキスト分類器の分類結果を基に行う。テキスト分類器は、テキスト分類器の分類結果をベースに、テキスト分類器の分類結果をベースに分類結果を生成する。

- [A Survey on Multimodal Disinformation Detection](https://aclanthology.org/2022.coling-1.576)
  - Firoj Alam, Stefano Cresci, Tanmoy Chakraborty, Fabrizio Silvestri, Dimiter Dimitrov, Giovanni Da San Martino, Shaden Shaar, Hamed Firooz, Preslav Nakov
  - **TLDR**: マルチモーダルなニュースや動画を検出する手法についてまとめたサーベイ。ニュース/動画/画像/SNSの3つを対象としており、ニュース/動画/SNSの3つを検証している。また、ニュース/動画/SNSの3つを検証している。

- [Why Is It Hate Speech? Masked Rationale Prediction for Explainable Hate Speech Detection](https://aclanthology.org/2022.coling-1.577)
  - Jiyun Kim, Byounghan Lee, Kyung-Ah Sohn
  - **TLDR**: 不信なニュースを検知する研究。不信なニュースは、不信な単語を含まない文書から予測する。不信な文書は、単語の意味が不明瞭な文書から予測する。不信な文書は、単語の意味を含まない文書から予測する。不信な文書は、単語の意味を含まない文書から予測する。

- [Domain Classification-based Source-specific Term Penalization for Domain Adaptation in Hate-speech Detection](https://aclanthology.org/2022.coling-1.578)
  - Tulika Bose, Nikolaos Aletras, Irina Illina, Dominique Fohr
  - **TLDR**: 不信スピーチに対する分類器の自動調整で、不信スピーチの分類器を学習させる研究。不信スピーチの分類器は、不信スピーチの分類結果を予測する際、不信スピーチの分類結果を予測するドメインを学習する。不信スピーチの分類器は、不信スピーチの分類結果を予測するドメインを学習する。

- [Generalizable Implicit Hate Speech Detection Using Contrastive Learning](https://aclanthology.org/2022.coling-1.579)
  - Youngwook Kim, Shinwoo Park, Yo-Sub Han
  - **TLDR**: 不信投稿に対する不信表現を抽出する手法の提案。不信投稿に対する不信表現を抽出する際、不信投稿の投稿文を入力とする文空間を不自然な表現に置き換える(不信表現は入力空間に近い表現に置き換える)。不信投稿に対する不信表現を入力とする文空間を不自然な表現に置き換える(不自然な表現は入力空間に近い表現に置き換える)。

- [Social Bot-Aware Graph Neural Network for Early Rumor Detection](https://aclanthology.org/2022.coling-1.580)
  - Zhen Huang, Zhilong Lv, Xiaoyun Han, Binyang Li, Menglong Lu, Dongsheng Li
  - **TLDR**: ツリー拡散を防ぐために、事前学習済みモデルを用いた研究。ツリー拡散の初期段階で、ツリーの更新頻度を予測するネットワークを構築し、ツリーの更新頻度を予測するネットワークを構築する。ツリー拡散の初期段階で、ツリーの更新頻度を予測するネットワークを構築する。ツリー拡散の初期段階で、ツリーの更新頻度を予測するネットワークを構築する。

- [A Contrastive Cross-Channel Data Augmentation Framework for Aspect-Based Sentiment Analysis](https://aclanthology.org/2022.coling-1.581)
  - Bing Wang, Liang Ding, Qihuang Zhong, Ximing Li, Dacheng Tao
  - **TLDR**: 多角度の感情分析を行う際に、複数aspectのモデルを組み合わせて学習する研究。各aspectのモデルは、各aspectのモデルの学習率を上げるための手法を提案している。各aspectのモデルは、各aspectの学習率を上げるための手法を提案している。

- [Sentiment Interpretable Logic Tensor Network for Aspect-Term Sentiment Analysis](https://aclanthology.org/2022.coling-1.582)
  - Bowen Zhang, Xu Huang, Zhichao Huang, Hu Huang, Baoquan Zhang, Xianghua Fu, Liwen Jing
  - **TLDR**: 感情の理解を強化する研究。感情の理解は、感情の表現を表現する単語を認識する(単語の意味は、単語の意味と異なる単語を認識する)、単語の意味を認識する単語を認識する単語を認識する単語の認識を学習する。

- [Detecting Minority Arguments for Mutual Understanding: A Moderation Tool for the Online Climate Change Debate](https://aclanthology.org/2022.coling-1.583)
  - Cedric Waterschoot, Ernst van den Hemel, Antal van den Bosch
  - **TLDR**: 環境問題に関する投稿に対し、適切なコメントを生成する研究。投稿のコメントは、投稿者と投稿者双方の意見を反映させるために生成される。投稿者側は、投稿者と投稿者双方の意見を反映させるために、投稿者と投稿者双方の意見を個別に分類する。この分類器は、投稿者と投稿者双方の意見を個別に分類する。

- [A Multi-turn Machine Reading Comprehension Framework with Rethink Mechanism for Emotion-Cause Pair Extraction](https://aclanthology.org/2022.coling-1.584)
  - Changzhi Zhou, Dandan Song, Jing Xu, Zhijing Wu
  - **TLDR**: 感情因果推論の手法を、文書レベルの機械翻訳で解く研究。文書のタイトル/単語/文中の単語を入力として、文書のタイトル/単語/文中の単語を入力として、文書のタイトル/単語/文中の単語を入力として、文書のタイトル/単語/文中の単語を入力として、文書のタイトル/単語/文中の単語を入力として、文書のタイトル/単語/文中の単語を入力として、文書のタイトル/単語/文中の単語を入力として、文書のタイトル/単語/文中の単語を入力として、文書のタイトル/文中の単語を入力として、文書のタイトル/文中の単語を入力として、文書の

- [Structural Bias for Aspect Sentiment Triplet Extraction](https://aclanthology.org/2022.coling-1.585)
  - Chen Zhang, Lei Ren, Fang Ma, Jingang Wang, Wei Wu, Dawei Song
  - **TLDR**: 自然言語処理において、構造を組み込んだ研究。構造は単語の位置関係を表す単語の位置関係を、単語の位置関係を表現する単語の位置関係を表現する単語の位置関係を表現する単語の位置関係を表現する単語の位置関係を表現する単語の位置関係を表現する単語の位置関係を表現する単語の位置関係を表現する単語の位置関係を表現する単語の位置関係を表現する単語の位置関係を表現する単語の位置関係を表現する単語の位置関係を表現する単語の位置関係を表現する単語の位置関係を表現する単語の位置関係を表現する単語の位置関係を表現する単語の

- [Unsupervised Data Augmentation for Aspect Based Sentiment Analysis](https://aclanthology.org/2022.coling-1.586)
  - David Z. Chen, Adam Faulkner, Sahil Badyal
  - **TLDR**: セクシューベースの感情分類を、事前学習済みモデルで行う研究。事前学習済みモデルは、Agent/Agentの単語をAgent/Agentの単語ベクトルに変換する(Agentは単語ベクトルの重みを計算する)。この重みを、Agent/Agentの単語ベクトルに変換する形で学習する。

- [A Sentiment and Emotion Aware Multimodal Multiparty Humor Recognition in Multilingual Conversational Setting](https://aclanthology.org/2022.coling-1.587)
  - Dushyant Singh Chauhan, Gopendra Vikram Singh, Aseem Arora, Asif Ekbal, Pushpak Bhattacharyya
  - **TLDR**: 多言語の画像認識モデルで、感情と画像を統合する研究。画像の分類は、画像の表現を分類するクラスに分類する。画像の分類は、画像の表現を分類するクラスに分類する。画像の分類は、画像の表現を分類するクラスに分類する。画像の分類は、画像の分類を学習するモデルで行う。

- [TSAM: A Two-Stream Attention Model for Causal Emotion Entailment](https://aclanthology.org/2022.coling-1.588)
  - Duzhen Zhang, Zhen Yang, Fandong Meng, Xiuyi Chen, Jie Zhou
  - **TLDR**: 対話中の感情を、事前学習済みモデルで捉える研究。事前学習済みモデルは、事前学習済みモデルの学習済みモデルと同等の学習済みモデルを組み合わせて学習する。事前学習済みモデルは、事前学習済みモデルの学習済みモデルと同等の学習済みモデルを組み合わせて学習する。事前学習済みモデルは、事前学習済みモデルと同等の学習済みモデルを組み合わせて学習する。

- [Entity-Level Sentiment Analysis (ELSA): An Exploratory Task Survey](https://aclanthology.org/2022.coling-1.589)
  - Egil Rønningstad, Erik Velldal, Lilja Øvrelid
  - **TLDR**: 文書/文/ターゲットのエンティティに対する評価を調べる研究。文書/文/ターゲットのエンティティに対する評価は、文書/文/ターゲットの評価と同等の評価を行なっている。文書/文/ターゲットの評価は、文書/文/ターゲットの評価と同等の評価を行なっている。

- [Learning from Adjective-Noun Pairs: A Knowledge-enhanced Framework for Target-Oriented Multimodal Sentiment Classification](https://aclanthology.org/2022.coling-1.590)
  - Fei Zhao, Zhen Wu, Siyu Long, Xinyu Dai, Shujian Huang, Jiajun Chen
  - **TLDR**: 画像から、感情の分布を推定する研究。画像から、感情分布を推定するAdjective-Nounを抽出する。Adjective-Nounは、画像の画像特徴を捉えるための重要な要素である。そのため、画像特徴を捉えるためのAdjective-Nounを抽出する研究。

- [Towards Exploiting Sticker for Multimodal Sentiment Analysis in Social Media: A New Dataset and Baseline](https://aclanthology.org/2022.coling-1.591)
  - Feng Ge, Weizhao Li, Haopeng Ren, Yi Cai
  - **TLDR**: マルチモーダルなステッカーを用いた、SNSにおける、マルチモーダルな感情分析の研究。ステッカーは通常の画像を扱うことが多いが、画像を扱うことで、ステッカーの表現を変えられる。ステッカーは通常の画像を扱うことが多いが、ステッカーはステッカーの画像を扱うため、画像を認識する際はステッカーを認識する。

- [Natural Language Inference Prompts for Zero-shot Emotion Classification in Text across Corpora](https://aclanthology.org/2022.coling-1.592)
  - Flor Miriam Plaza-del-Arco, María-Teresa Martín-Valdivia, Roman Klinger
  - **TLDR**: 自然言語処理モデルで、感情分類を行う際に、どの文が適切なのかを判断する際のチェックポイントをまとめた研究。文のタイトル/単語/文中の単語/単語の順で、文のタイトル/単語/単語順の順で、文のタイトル/単語/単語順の順で、文の単語/単語順の順で、文の単語/単語順の順で、文の単語/単語順の順で、文の単語/単語順の順で、文の単語/単語順の順で、文の単語/単語順の順で、文の単語/単語順の順で、文の単語/単語順の順で、

- [CommunityLM: Probing Partisan Worldviews from Language Models](https://aclanthology.org/2022.coling-1.593)
  - Hang Jiang, Doug Beeferman, Brandon Roy, Deb Roy
  - **TLDR**: 政治とTwitterの対話について、異なる言語モデルで検証した研究。Twitterの投稿から、Twitterの投稿に対する評価を抽出する。評価は、投稿に対する評価と、投稿に対する評価の2つに分けて行う。評価は、投稿に対する評価と、投稿に対する評価の2つに分けて行う。

- [Composition-based Heterogeneous Graph Multi-channel Attention Network for Multi-aspect Multi-sentiment Classification](https://aclanthology.org/2022.coling-1.594)
  - Hao Niu, Yun Xiong, Jian Gao, Zhongchen Miao, Xiaosu Wang, Hongrun Ren, Yao Zhang, Yangyong Zhu
  - **TLDR**: 文中のaspect termを、文全体の感情分布に変換する研究。各aspect termの表現を、文全体の感情分布に変換する。各aspect termの表現を、文全体の感情分布に変換する。この時、各aspect termの表現を、文全体の感情分布に変換する。

- [CoNTACT: A Dutch COVID-19 Adapted BERT for Vaccine Hesitancy and Argumentation Detection](https://aclanthology.org/2022.coling-1.595)
  - Jens Lemmens, Jens Van Nooten, Tim Kreutz, Walter Daelemans
  - **TLDR**: 自然言語処理で、COVID-19に関するツリーの翻訳モデルを学習した研究。ツリーの翻訳モデルは、ツリーの翻訳モデルと同等の精度を維持しつつ、ツリーの翻訳モデルを学習する。ツリー翻訳モデルは、ツリーの翻訳モデルと同等の精度を維持しつつ、ツリー翻訳モデルの学習を行なっている。

- [SSR: Utilizing Simplified Stance Reasoning Process for Robust Stance Detection](https://aclanthology.org/2022.coling-1.596)
  - Jianhua Yuan, Yanyan Zhao, Yanyue Lu, Bing Qin
  - **TLDR**: stance detectionのタスクで、タスクの説明がタスク知識に依存しないか検証した研究。タスク説明はタスク知識でなく、タスクの説明をタスク知識に置き換えたもの。タスク説明はタスク知識でなくタスク説明をタスク知識に置き換えたもの。タスク説明はタスク知識で置き換えたもの。タスク説明はタスク知識で置き換えたもの。

- [Transferring Confluent Knowledge to Argument Mining](https://aclanthology.org/2022.coling-1.597)
  - João António Rodrigues, António Branco
  - **TLDR**: 文書から、論理モデルを転移学習する研究。文書から、文書の構造を推定するモデルを学習する。文書の構造は、文書の文脈から推定するが文書の文脈から推定する文書の構造を推定するモデルを学習する。文書の構造を推定するモデルは、文書の構造を推定するモデルと同等のモデルを用いている。

- [When to Laugh and How Hard? A Multimodal Approach to Detecting Humor and Its Intensity](https://aclanthology.org/2022.coling-1.598)
  - Khalid Alnajjar, Mika Hämäläinen, Jörg Tiedemann, Jorma Laaksonen, Mikko Kurimo
  - **TLDR**: フレーズの発話に対する、事前学習済みモデルの予測精度を上げる研究。発話は、発話の長さ(長さ=長さ)と、発話の長さ(長さ=長さ)を入力として、長さ=長さの長さを予測する。長さは、長さ=長さの長さを予測する。長さは、長さ=長さの長さを予測する。

- [Modeling Aspect Correlation for Aspect-based Sentiment Analysis via Recurrent Inverse Learning Guidance](https://aclanthology.org/2022.coling-1.599)
  - Longfeng Li, Haifeng Sun, Qi Qi, Jingyu Wang, Jing Wang, Jianxin Liao
  - **TLDR**: 文中のaspectを、逆強化学習で学習する研究。aspectの重みを逆強化学習で学習する。重みはaspectの重みと、aspectの重みはaspectの重みを重みとして学習する。重みはaspectの重みを重みとして学習する。重みはaspectの重みを重みとして重みを計算する。重みはaspectの重みを重みとして重みを計算する。

- [Analyzing Persuasion Strategies of Debaters on Social Media](https://aclanthology.org/2022.coling-1.600)
  - Matti Wiegmann, Khalid Al Khatib, Vishal Khanna, Benno Stein
  - **TLDR**: オンライン対話システムにおける、対話者の効果的な行動を調査した研究。対話システムのパフォーマンスを評価する指標として、対話者の行動を評価する指標(評価指標は、評価結果が良かった/悪い、評価結果が良かった/評価結果が良かったなど)、対話者の行動を評価する指標(評価指標は、評価結果が良かった/悪いなど)、対話者の行動を評価する指標(評価指標は、評価結果が良かった/悪いなど)を挙げている。

- [KC-ISA: An Implicit Sentiment Analysis Model Combining Knowledge Enhancement and Context Features](https://aclanthology.org/2022.coling-1.601)
  - Minghao Xu, Daling Wang, Shi Feng, Zhenfei Yang, Yifei Zhang
  - **TLDR**: 自然言語処理で、自然言語処理の文書に対する感情表現を抽出する研究。文書の文脈を推定する手法を、文脈の知識と文脈のコンテキストを組み合わせた手法とし、文脈のコンテキストを抽出する手法を提案している。文脈のコンテキストは、文脈の知識と文脈のコンテキストを組み合わせたものに置き換えられている。

- [Domain Generalization for Text Classification with Memory-Based Supervised Contrastive Learning](https://aclanthology.org/2022.coling-1.602)
  - Qingyu Tan, Ruidan He, Lidong Bing, Hwee Tou Ng
  - **TLDR**: ドメイン転移を行う際に、複数ドメインの文書を同時に学習する手法の提案。文書の分類結果を、同じクラスの文書と比較し、文書の分類結果を同じクラスの文書と比較する。文書の分類結果は、文書の分類結果と比較して、文書の分類結果が近い文書を優先的に学習する。

- [A Zero-Shot Claim Detection Framework Using Question Answering](https://aclanthology.org/2022.coling-1.603)
  - Revanth Gangi Reddy, Sai Chetan Chinthakindi, Yi R. Fung, Kevin Small, Heng Ji
  - **TLDR**: マルチタスクで、事前学習済みモデルを用いた事前学習済みモデルの提案。事前学習済みモデルは、事前学習済みモデルのタスクを学習する際、事前学習済みモデルのタスクを学習する。事前学習済みモデルは、事前学習済みモデルのタスクを学習する際、タスクのタスクを個別に学習する。事前学習済みモデルは、タスクを個別に学習するタスクを想定していない。

- [Asymmetric Mutual Learning for Multi-source Unsupervised Sentiment Adaptation with Dynamic Feature Network](https://aclanthology.org/2022.coling-1.604)
  - Rui Li, Cheng Liu, Dazhi Jiang
  - **TLDR**: 事前学習済み言語モデルを、マルチタスクで学習する研究。事前学習済み言語モデルは、事前学習済みの言語モデルの学習済みモデルと同等の精度を維持できるが、マルチタスクで学習する際は、事前学習済み言語モデルの学習済みモデルと同等の精度を維持する必要がある。

- [Target Really Matters: Target-aware Contrastive Learning and Consistency Regularization for Few-shot Stance Detection](https://aclanthology.org/2022.coling-1.605)
  - Rui Liu, Zheng Lin, Huishan Ji, Jiangnan Li, Peng Fu, Weiping Wang
  - **TLDR**: 少数サンプルから、少数サンプルの行動を予測する手法の提案。少数サンプルの行動を予測する際、少数サンプルの行動を予測するモデルを用いることで、少数サンプルの行動を予測するモデルの精度を上げている。

- [Joint Alignment of Multi-Task Feature and Label Spaces for Emotion Cause Pair Extraction](https://aclanthology.org/2022.coling-1.606)
  - Shunjie Chen, Xiaochuan Shi, Jingye Li, Shengqiong Wu, Hao Fei, Fei Li, Donghong Ji
  - **TLDR**: 感情因果推論のタスクで、事前学習済みモデルと同等のタスクを組み合わせて学習する手法の提案。事前学習済みモデルと同等のタスクを組み合わせて学習する。事前学習済みモデルは、事前学習済みモデルと同等のタスクを学習する。事前学習済みモデルは、事前学習済みモデルと同等のタスクを学習する。

- [Causal Intervention Improves Implicit Sentiment Analysis](https://aclanthology.org/2022.coling-1.607)
  - Siyin Wang, Jie Zhou, Changzhi Sun, Junjie Ye, Tao Gui, Qi Zhang, Xuanjing Huang
  - **TLDR**: 自然言語処理で、文中の感情表現を分析する研究。文中の感情表現は、文中の単語と同等かそれ以上の意味を持つかを検証する。文中の感情表現は、文中の単語と同等かそれ以上の意味を持つかを検証する。文中の感情表現は、文中の単語と同等かそれ以上の意味を持つかを検証する。

- [COMMA-DEER: COmmon-sense Aware Multimodal Multitask Approach for Detection of Emotion and Emotional Reasoning in Conversations](https://aclanthology.org/2022.coling-1.608)
  - Soumitra Ghosh, Gopendra Vikram Singh, Asif Ekbal, Pushpak Bhattacharyya
  - **TLDR**: ストレスや感情を発する質問に対し、事前学習済みモデルを用いた研究。質問に対する回答は、質問に対する回答と同等の文書で作成される。質問に対する回答は、質問に対する回答と同等の文書で作成される。質問に対する回答は、質問に対する回答と同等の文書で作成される。

- [EmoMent: An Emotion Annotated Mental Health Corpus from Two South Asian Countries](https://aclanthology.org/2022.coling-1.609)
  - Thushari Atapattu, Mahen Herath, Charitha Elvitigala, Piyanjali de Zoysa, Kasun Gunawardana, Menasha Thilakaratne, Kasun de Zoysa, Katrina Falkner
  - **TLDR**: ストレスや抑うつといった感情を表現するニュースを、自然言語処理で自動生成する研究。ニュースのタイトルを、ニュースの投稿から、ニュースの投稿から、ニュースの投稿から、ニュースの投稿から、ニュースの投稿から、ニュースの投稿から、ニュースの投稿から、ニュースの投稿から、ニュースの投稿から、ニュースの投稿から、ニュースの投稿から、ニュースの投稿から、ニュースの投稿から、ニュースの投稿から、ニュースの投稿から、ニュースの投稿から、ニュースの投稿から、ニュースの投稿から、ニュースの投稿から、ニュースの投稿から、ニュースの投稿から、ニュースの投稿から、ニュースの投稿から、ニュースの投稿から、

- [LEGO-ABSA: A Prompt-based Task Assemblable Unified Generative Framework for Multi-task Aspect-based Sentiment Analysis](https://aclanthology.org/2022.coling-1.610)
  - Tianhao Gao, Jun Fang, Hanyu Liu, Zhiyuan Liu, Chao Liu, Pengzhang Liu, Yongjun Bao, Weipeng Yan
  - **TLDR**: マルチタスクの手法を、複数タスクで解く手法の提案。タスクは、タスクの生成結果から生成したテキストをテキストベースのアルゴリズムで分類し、そのテキストを入力とする。入力は、テキストベースのアルゴリズムで生成したテキストを入力とする。

- [A Hierarchical Interactive Network for Joint Span-based Aspect-Sentiment Analysis](https://aclanthology.org/2022.coling-1.611)
  - Wei Chen, Jinglong Du, Zhao Zhang, Fuzhen Zhuang, Zhongshi He
  - **TLDR**: 画像分類とspan-basedのタスクを統合する研究。画像分類は、画像の表面的な特徴と、画像の内部特徴を統合する。画像の表面的な特徴は、画像の内部特徴と、画像の内部特徴を統合する。画像の内部特徴は、画像の内部特徴と、画像の内部特徴を統合する。

- [MuCDN: Mutual Conversational Detachment Network for Emotion Recognition in Multi-Party Conversations](https://aclanthology.org/2022.coling-1.612)
  - Weixiang Zhao, Yanyan Zhao, Bing Qin
  - **TLDR**: マルチスコアの対話において、対話中の感情を認識する研究。対話中の感情を認識するネットワークを構築し、そのネットワークをマルチスコアの対話に適用する。対話中の感情を認識するネットワークは、対話中の感情を認識するネットワークと同等の精度を達成。

- [UECA-Prompt: Universal Prompt for Emotion Cause Analysis](https://aclanthology.org/2022.coling-1.613)
  - Xiaopeng Zheng, Zhiyue Liu, Zizhen Zhang, Zhaoyang Wang, Jiahai Wang
  - **TLDR**: 感情因果推論のタスクを、タスクごとに個別に学習する研究。タスクごとに個別に学習するのではなく、タスクごとに学習する。タスクごとに学習する際は、タスクの重みを個別に学習する。タスクごとに学習する重みは、タスクごとに個別に学習する重みをベースにしている。

- [One-Teacher and Multiple-Student Knowledge Distillation on Sentiment Classification](https://aclanthology.org/2022.coling-1.614)
  - Xiaoqin Chang, Sophia Yat Mei Lee, Suyang Zhu, Shoushan Li, Guodong Zhou
  - **TLDR**: 教師モデルを蒸留する研究。蒸留は蒸留器の重みを蒸留するモデルの重みを蒸留する。蒸留器の重みは蒸留器の重みと同等になるよう、蒸留器の重みを蒸留するモデルの重みを蒸留するモデルの重みを蒸留するモデルの重みを蒸留するモデルの重みを蒸留するモデルの重みを蒸留するモデルの重みを蒸留するモデルの重みを蒸留するモデルの重みを蒸留するモデルの重みを蒸留するモデルの重みを蒸留するモデルの重みを蒸留する

- [Making Parameter-efficient Tuning More Efficient: A Unified Framework for Classification Tasks](https://aclanthology.org/2022.coling-1.615)
  - Xin Zhou, Ruotian Ma, Yicheng Zou, Xuanting Chen, Tao Gui, Qi Zhang, Xuanjing Huang, Rui Xie, Wei Wu
  - **TLDR**: 事前学習済み言語モデル(PLM)を、事前学習済み言語モデルの挙動に合わせるための研究。事前学習済み言語モデルは、事前学習済み言語モデルの挙動を予測するタスクを想定しており、事前学習済み言語モデルの挙動を予測するタスクを追加する。事前学習済み言語モデルの挙動を予測するタスクを追加することで、事前学習済み言語モデルの挙動を予測する。

- [A Multi-Task Dual-Tree Network for Aspect Sentiment Triplet Extraction](https://aclanthology.org/2022.coling-1.616)
  - Yichun Zhao, Kui Meng, Gongshen Liu, Jintao Du, Huijia Zhu
  - **TLDR**: 多言語のApartmentとopinionを抽出する研究。Apartmentは、opinion(opinion)とopinion(opinion)の2つから抽出する。opinionはopinionの語彙を入力として、opinionはopinionの語彙を入力として、opinionはopinionの語彙を入力として、opinionはopinionの語彙を入力として、opinionはopinionの語彙を入力として、opinionはopinionの語彙を入力として、opinionはopinionの語彙を入力として、opinionはopinionの語彙を入力として、opinionはopinionの語彙を入力として、opinionはopinionの語彙を入力として、opinionはopinionの語彙を入力として、opinion

- [Exploiting Unlabeled Data for Target-Oriented Opinion Words Extraction](https://aclanthology.org/2022.coling-1.617)
  - Yidong Wang, Hao Wu, Ao Liu, Wenxin Hou, Zhen Wu, Jindong Wang, Takahiro Shinozaki, Manabu Okumura, Yue Zhang
  - **TLDR**: 文中のopinion wordを抽出するタスクの提案。文中のopinion wordを抽出するタスクは、文中のopinion wordを抽出するモデルが一般的だが、このタスクでは、文中のopinion wordを抽出するモデルが限られている。そこで、文中のopinion wordを抽出するモデルを提案。

- [Learnable Dependency-based Double Graph Structure for Aspect-based Sentiment Analysis](https://aclanthology.org/2022.coling-1.618)
  - Yinglong Ma, Yunhe Pang
  - **TLDR**: 文の構造、関係、言語といった要素を、事前学習済みモデルで統合して学習する研究。事前学習済みモデルは、Bi-directional AttentionとMask Languageを組み合わせたモデルで、事前学習済みモデルは、文の構造、関係、言語といった要素を統合して学習する。

- [A Structure-Aware Argument Encoder for Literature Discourse Analysis](https://aclanthology.org/2022.coling-1.619)
  - Yinzi Li, Wei Chen, Zhongyu Wei, Yujun Huang, Chujun Wang, Siyuan Wang, Qi Zhang, Xuanjing Huang, Libo Wu
  - **TLDR**: 文中の単語を、文構造を認識するモデルを提案。単語の位置情報(position)を、文構造を認識するモデルに入力し、文構造を認識するモデルを生成する。文構造を認識するモデルは、文構造を認識するモデルと同等精度を達成。

- [Mere Contrastive Learning for Cross-Domain Sentiment Analysis](https://aclanthology.org/2022.coling-1.620)
  - Yun Luo, Fang Guo, Zihan Liu, Yue Zhang
  - **TLDR**: マルチドメインの文に対する感情分析で、文のクラスを比較する手法の提案。クラスの比較は、クラスの異なる文を比較する(クラスの異なる文を比較する)ことで、クラスの異なる文を比較する。クラスの比較は、クラスの異なる文を比較する(クラスの異なる文を比較する)ことで行う。

- [Exploiting Sentiment and Common Sense for Zero-shot Stance Detection](https://aclanthology.org/2022.coling-1.621)
  - Yun Luo, Zihan Liu, Yuefeng Shi, Stan Z. Li, Yue Zhang
  - **TLDR**: 文書分類タスクで、文書分類のタスクを強化した研究。文書分類タスクは文書分類のタスクで、文書分類のタスクは文書分類タスクと同等の扱いが難しい。文書分類タスクは文書分類タスクと同等の扱いが難しいため、文書分類タスクを併用する。文書分類タスクは文書分類タスクと同等の扱いが難しいため、文書分類タスクを併用する

- [Modeling Intra- and Inter-Modal Relations: Hierarchical Graph Contrastive Learning for Multimodal Sentiment Analysis](https://aclanthology.org/2022.coling-1.622)
  - Zijie Lin, Bin Liang, Yunfei Long, Yixue Dang, Min Yang, Min Zhang, Ruifeng Xu
  - **TLDR**: マルチモーダルで感情分析を行う際に、各モーダルの特徴をグラフ構造で表現する研究。各モーダルの特徴をグラフ構造で表現し、グラフ構造をグラフ構造とし、グラフ構造をグラフ構造ととらえている。これにより、マルチモーダルで感情分析を行う際に、グラフ構造をグラフ構造ととらえている。

- [AMOA: Global Acoustic Feature Enhanced Modal-Order-Aware Network for Multimodal Sentiment Analysis](https://aclanthology.org/2022.coling-1.623)
  - Ziming Li, Yan Zhou, Weibo Zhang, Yaxin Liu, Chuanpeng Yang, Zheng Lian, Songlin Hu
  - **TLDR**: マルチモーダルな動画を分析する研究。マルチモーダルな動画は、各モーダル特徴を個別に扱えるが、個別に扱えると全体の動画が小さくなる。そこで、各モーダル特徴を個別に扱えるようにする手法を提案。マルチモーダルな動画を個別に扱えるようにする。

- [Keyphrase Prediction from Video Transcripts: New Dataset and Directions](https://aclanthology.org/2022.coling-1.624)
  - Amir Pouran Ben Veyseh, Quan Hung Tran, Seunghyun Yoon, Varun Manjunatha, Hanieh Deilamsalehy, Rajiv Jain, Trung Bui, Walter W. Chang, Franck Dernoncourt, Thien Huu Nguyen
  - **TLDR**: 動画から、Keyphrase Predictionを行う際に、事前学習済みモデルの挙動を改善した研究。事前学習済みモデルは、Keyphrase Predictionのタスクで使われる単語を予測するが、事前学習済みモデルはKeyphraseを予測する単語を予測する単語に置き換える。この結果、事前学習済みモデルの挙動を改善した。

- [Event Extraction in Video Transcripts](https://aclanthology.org/2022.coling-1.625)
  - Amir Pouran Ben Veyseh, Viet Dac Lai, Franck Dernoncourt, Thien Huu Nguyen
  - **TLDR**: 動画から、ニュース記事や論文などから、イベントの情報を抽出する研究。ニュース記事や論文などから、イベントの参加者を特定するデータセットを収集し、それを基にモデルを作成する。モデルは、ニュース記事や論文などから、イベントの参加者を特定するデータセットを収集し、モデルの学習を行なっている。

- [Recycle Your Wav2Vec2 Codebook: A Speech Perceiver for Keyword Spotting](https://aclanthology.org/2022.coling-1.626)
  - Guillermo Cámbara, Jordi Luque, Mireia Farrús
  - **TLDR**: 事前学習済みモデルを、学習済み言語モデルに転移する手法の提案。学習済み言語モデルは、事前学習済み言語モデルの言語知識を学習データに転移する。学習済み言語モデルは、学習済み言語モデルの言語知識を学習データに転移する。学習データは、学習済み言語モデルの言語知識を学習データに転移する。

- [Improving Code-switched ASR with Linguistic Information](https://aclanthology.org/2022.coling-1.627)
  - Jie Chi, Peter Bell
  - **TLDR**: 自然言語処理でコードチェンジを行う際に、言語モデルの精度を上げるために必要な手法をまとめた研究。言語モデルの精度を上げるために、言語モデルの言語モデルを言語モデルの言語モデルに近い言語モデルに置き換える手法を提案している。

- [Language-specific Effects on Automatic Speech Recognition Errors for World Englishes](https://aclanthology.org/2022.coling-1.628)
  - June Choe, Yiran Chen, May Pik Yu Chan, Aini Li, Xin Gao, Nicole Holliday
  - **TLDR**: 自動対話システムで、言語モデルのパフォーマンスを検証した研究。言語モデルは、単語の意味を予測する単語分類器(単語分類器は単語分類器の分類器で、単語分類器は単語分類器の分類器で、単語分類器は単語分類器の分類器で)をベースに、単語分類器は単語分類器の分類器で、単語分類器は単語分類器の分類器で学習する。

- [A Transformer-based Threshold-Free Framework for Multi-Intent NLU](https://aclanthology.org/2022.coling-1.629)
  - Lisung Chen, Nuo Chen, Yuexian Zou, Yong Wang, Xinzhong Sun
  - **TLDR**: マルチエージェントの学習を、転移学習で行う研究。転移学習は転移先の言語モデルを学習するが、転移先の言語モデルは転移先の言語モデルの学習率を上回る。転移先の言語モデルは転移先の言語モデルの学習率を上回る。転移先の言語モデルは転移先の言語モデルを学習する。

- [Unsupervised Multi-scale Expressive Speaking Style Modeling with Hierarchical Context Information for Audiobook Speech Synthesis](https://aclanthology.org/2022.coling-1.630)
  - Xueyuan Chen, Shun Lei, Zhiyong Wu, Dong Xu, Weifeng Zhao, Helen Meng
  - **TLDR**: 音声合成において、グローバル・局所特徴をマルチスケールで表現する研究。グローバル・局所特徴をマルチスケールで表現するEncoderを、局所特徴を個別に予測するEncoderに置き換える。局所特徴は、局所特徴の予測結果から予測する。局所特徴は、局所特徴の予測結果から予測する。

- [Incorporating Instructional Prompts into a Unified Generative Framework for Joint Multiple Intent Detection and Slot Filling](https://aclanthology.org/2022.coling-1.631)
  - Yangjun Wu, Han Wang, Dongxiang Zhang, Gang Chen, Hao Zhang
  - **TLDR**: マルチインスタンスに対する事前学習済みモデルの提案。事前学習済みモデルは、事前学習済みモデルの質問をベースに、質問に対する事前学習済みモデルの回答をベースにしている。事前学習済みモデルは、事前学習済みモデルの質問をベースに、質問に対する事前学習済みモデルの回答をベースにしている。事前学習済みモデルは、事前学習済みモデルの質問をベースにしている。

- [Adaptive Unsupervised Self-training for Disfluency Detection](https://aclanthology.org/2022.coling-1.632)
  - Zhongyuan Wang, Yixuan Wang, Shaolei Wang, Wanxiang Che
  - **TLDR**: 事前学習済みモデルを、教師なしで学習する研究。事前学習済みモデルは学習データの質が低いため、学習データの質を上げるために重みを上げている。重みは、学習データの質と予測精度を近似する形で調整している。
