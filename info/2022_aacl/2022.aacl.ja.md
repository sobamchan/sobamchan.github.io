---
title: AACL 2022
updated: 2022-11-20
layout: tldr
---

# AACL 2022 in Japanese

## TLDRs

- [Chasing the Tail with Domain Generalization: A Case Study on Frequency-Enriched Datasets](https://aclanthology.org/2022.aacl-main.1)
  - Manoj Kumar, Anna Rumshisky, Rahul Gupta
  - **TLDR**: 自然言語処理で、頻繁に使われる単語を予測する研究。単語分類は通常のモデルで行い、単語の分類は通常のモデルで行い、単語分類は通常のモデルで行い、単語分類は通常のモデルで行い、単語分類は通常のモデルで行い、単語分類は通常のモデルで行い、単語分類は通常のモデルで行い、単語分類は通常のモデルで行い、単語分類は通常のモデルで行い、単語分類は通常のモデルで行い、という形で学習を行っている。

- [Double Trouble: How to not Explain a Text Classifier’s Decisions Using Counterfactuals Synthesized by Masked Language Models?](https://aclanthology.org/2022.aacl-main.2)
  - Thang Pham, Trung Bui, Long Mai, Anh Nguyen
  - **TLDR**: 事前学習済みモデルの精度を上げるための手法の提案。事前学習済みモデルは、事前学習済みモデルの予測値を基に予測する。事前学習済みモデルは、事前学習済みモデルの予測値を基に予測を行う。事前学習済みモデルは、事前学習済みモデルの予測値を基に予測を行う。事前学習済みモデルは、事前学習済みモデルの予測値を基に予測を行う。

- [An Empirical Study on Cross-X Transfer for Legal Judgment Prediction](https://aclanthology.org/2022.aacl-main.3)
  - Joel Niklaus, Matthias Stürmer, Ilias Chalkidis
  - **TLDR**: 法廷判決に対する転移学習について、転移学習の効果と検証結果をまとめた研究。転移学習は、翻訳モデルの学習率が低い場合に有効だが、翻訳モデルの学習率が高い場合に有効。翻訳モデルの学習率は、翻訳モデルの学習率と同等の結果が得られることを確認。

- [CNN for Modeling Sanskrit Originated Bengali and Hindi Language](https://aclanthology.org/2022.aacl-main.4)
  - Chowdhury Rahman, MD. Hasibur Rahman, Mohammad Rafsan, Mohammed Eunus Ali, Samiha Zakir, Rafsanjani Muhammod
  - **TLDR**: 低リソース言語のモデルを、CNNで学習する研究。CNNは、単語/文の重みを学習するモデルで、単語/文の重みはTransformerで学習する。重みはTransformerで学習する。重みは、Transformerで学習するモデルで、Transformerで学習する重みは、Transformerで学習する重みを上回る。

- [Leveraging Key Information Modeling to Improve Less-Data Constrained News Headline Generation via Duality Fine-Tuning](https://aclanthology.org/2022.aacl-main.5)
  - Zhuoxuan Jiang, Lingfeng Qiao, Di Yin, Shanshan Feng, Bo Ren
  - **TLDR**: ニュースのタイトルを予測するタスクについて、事前学習済みモデルと事前学習済みモデルの2つを組み合わせた手法を提案。事前学習済みモデルは、事前学習済みモデルの予測精度を上げるための条件を付与する。事前学習済みモデルは、事前学習済みモデルの予測精度を上げるための条件を付与する。事前学習済みモデルは、事前学習済みモデルの予測精度を上げるための条件を付与する。

- [Systematic Evaluation of Predictive Fairness](https://aclanthology.org/2022.aacl-main.6)
  - Xudong Han, Aili Shen, Trevor Cohn, Timothy Baldwin, Lea Frermann
  - **TLDR**: 教師なし学習で、学習データの劣化を防ぐ手法のサーベイ。学習データの劣化は、教師なし学習の結果と同等の結果が出るか検証している。教師なし学習では、学習データの劣化は教師なし学習の結果と同等の結果が出るか検証している。

- [Graph-augmented Learning to Rank for Querying Large-scale Knowledge Graph](https://aclanthology.org/2022.aacl-main.7)
  - Hanning Gao, Lingfei Wu, Po Hu, Zhihua Wei, Fangli Xu, Bo Long
  - **TLDR**: 質問回答を行う際に、質問文を解くためのグラフを追加する研究。質問文を解くためのグラフを追加する(=質問文の重みを減らす)ことで、質問文の重みを減らす。これにより、質問文の重みを減らすモデルを構築する。

- [An Embarrassingly Simple Approach for Intellectual Property Rights Protection on Recurrent Neural Networks](https://aclanthology.org/2022.aacl-main.8)
  - Zhi Qin Tan, Hao Shan Wong, Chee Seng Chan
  - **TLDR**: 自然言語処理モデルに対する防衛策の提案。既存の防衛策は、モデルの挙動を監視する仕組みがなく、モデルの挙動を監視する仕組みを導入している。この仕組みは、モデルの挙動を監視する仕組みを導入している。

- [WAX: A New Dataset for Word Association eXplanations](https://aclanthology.org/2022.aacl-main.9)
  - Chunhua Liu, Trevor Cohn, Simon De Deyne, Lea Frermann
  - **TLDR**: 自然言語処理で、意味の理解を検証した研究。単語の意味を説明するモデルは、単語の意味を説明するモデルと同等精度を獲得できるか検証している。単語の意味を説明するモデルは、説明が意味の理解に寄与するかを検証する。

- [Missing Modality meets Meta Sampling (M3S): An Efficient Universal Approach for Multimodal Sentiment Analysis with Missing Modality](https://aclanthology.org/2022.aacl-main.10)
  - Haozhe Chi, Minghua Yang, Junhao Zhu, Guanhong Wang, Gaoang Wang
  - **TLDR**: マルチモーダルな感情分析で、マルチモーダルな手法を組み合わせる手法の提案。マルチモーダルな手法は、マルチモーダルな手法を組み合わせる際に、マルチモーダルな手法を組み合わせる際に、マルチモーダルな手法を組み合わせる際に、マルチモーダルな手法を組み合わせる際に、マルチモーダルな手法を組み合わせる際に、マルチモーダルな手法を組み合わせる際に、マルチモーダルな手法を組み合わせる際に、マルチモーダルな手法を組み合わせる際に、マルチモーダルな手法を組み合わせる際に、マルチモーダルな

- [SPARQL-to-Text Question Generation for Knowledge-Based Conversational Applications](https://aclanthology.org/2022.aacl-main.11)
  - Gwénolé Lecorvé, Morgan Veyret, Quentin Brabant, Lina M. Rojas Barahona
  - **TLDR**: 自然言語処理のモデルで、質問に対する対応がどう変化しているのかを調べた研究。質問に対する対応は、質問に対する回答の精度(質問に対する回答の精度)と、質問に対する回答の精度(質問に対する回答の精度)の2つに分けられている。質問に対する回答の精度は、質問に対する回答の精度と同等の結果が得られた。

- [S+PAGE: A Speaker and Position-Aware Graph Neural Network Model for Emotion Recognition in Conversation](https://aclanthology.org/2022.aacl-main.12)
  - Chen Liang, Jing Xu, Yangkun Lin, Chong Yang, Yongliang Wang
  - **TLDR**: 対話システムにおける、位置情報の認識について、既存のモデルを改良した研究。位置情報の表現を、位置情報の表現と位置情報の表現の表現の2つに分割し、位置情報の表現を表現の表現に近い表現に変換する。位置情報の表現は、位置情報の表現と位置情報の表現の2つに分割し、表現の表現を表現の表現に近い表現に変換する。

- [Grammatical Error Correction Systems for Automated Assessment: Are They Susceptible to Universal Adversarial Attacks?](https://aclanthology.org/2022.aacl-main.13)
  - Vyas Raina, Yiting Lu, Mark Gales
  - **TLDR**: 自然言語処理で、文の文法誤差を訂正する手法のAdversarial Attackについて、具体的には文法誤差を訂正する手法を学習させる手法と、学習済み文を学習させる手法を比較した研究。学習済み文を学習させる際に、文法誤差を訂正する手法を学習させる手法と、学習済み文を学習させる手法を比較している。

- [This Patient Looks Like That Patient: Prototypical Networks for Interpretable Diagnosis Prediction from Clinical Text](https://aclanthology.org/2022.aacl-main.14)
  - Betty van Aken, Jens-Michalis Papaioannou, Marcel Naik, Georgios Eleftheriadis, Wolfgang Nejdl, Felix Gers, Alexander Loeser
  - **TLDR**: 臨床文書から診断の推定を行う研究。患者の画像を入力とし、患者の画像を入力とし、患者の画像を入力とし、患者の画像を入力とし、患者の画像を入力とし、患者の画像を入力とし、患者の画像を入力とし、患者の画像を入力とし、患者の画像を入力とし、患者の画像を入力とし、患者の画像を入力とし、患者の画像を入力とし、患者の画像を入力とし、患者の画像を入力とし、患者の画像を入力とし、患者の画像を入力とし、患者の画像を入力とし、患者

- [Cross-lingual Similarity of Multilingual Representations Revisited](https://aclanthology.org/2022.aacl-main.15)
  - Maksym Del, Mark Fishel
  - **TLDR**: マルチ言語の分散表現を扱う指数(CNN)と、マルチ言語の分散表現を扱う指数(CCA)の比較を検証した研究。CNNは分散表現の精度が低いため、学習率が高い言語モデル(MLM)で学習する。一方、CNNは学習率が高い言語モデル(CNN)で学習する。

- [Arabic Dialect Identification with a Few Labeled Examples Using Generative Adversarial Networks](https://aclanthology.org/2022.aacl-main.16)
  - Mahmoud Yusuf, Marwan Torki, Nagwa El-Makky
  - **TLDR**: アルカニズム分類タスクで、少数サンプルのみで対応するTransformerベースのモデルを提案。BERT/ARBET/MARBERTの3モデルをベースに、事前学習済みモデルをベースに、事前学習済みモデルの精度を上げるための事前学習済みモデルの提案。事前学習済みモデルは、事前学習済みモデルの精度を上げるための事前学習済みモデルの提案を行なっている。

- [Semantic Shift Stability: Efficient Way to Detect Performance Degradation of Word Embeddings and Pre-trained Language Models](https://aclanthology.org/2022.aacl-main.17)
  - Shotaro Ishihara, Hiromu Takahashi, Hono Shirai
  - **TLDR**: 事前学習済み言語モデルのパフォーマンスを評価する研究。事前学習済みモデルのパフォーマンスは徐々に低下していくが、学習済みモデルのパフォーマンスは徐々に低下していく。そのため、事前学習済みモデルのパフォーマンスを評価する手法を提案している。事前学習済みモデルのパフォーマンスを評価する指標として、事前学習済みモデルのパフォーマンスを計測する指標を提案している。

- [Neural Text Sanitization with Explicit Measures of Privacy Risk](https://aclanthology.org/2022.aacl-main.18)
  - Anthi Papadopoulou, Yunhao Yu, Pierre Lison, Lilja Øvrelid
  - **TLDR**: 文書中のプライバシー情報を消す手法の提案。文書中のプライバシー情報を消すには、文書中の単語を消す(プライバシーの保護)、文書中の単語を消す(プライバシーの保護)、文書中の単語を消す(プライバシーの保護)、文書中の単語を消す(プライバシーの保護)、文書中の単語を消す(プライバシーの保護)、文書中の単語を消す(プライバシーの保護)、文書中の単語を消す(プライバシーの保護)、文書中の単語を消す(プライバシーの保護)、文書中の単語を消す(プライバシーの保護)、文書中の単語を消す(プライバシーの保護)、文書中の単語を消す(プライバシーの

- [AGRank: Augmented Graph-based Unsupervised Keyphrase Extraction](https://aclanthology.org/2022.aacl-main.19)
  - Haoran Ding, Xiao Luo
  - **TLDR**: 文書中のKeyphraseを抽出する研究。文書中のKeyphraseは、文書のタイトル/要約/タイトルの順番に並べられている。文書中のKeyphraseは、文書のタイトル/要約/要約の順番に並べられている。文書中のKeyphraseは、文書のタイトル/要約/要約の順番に並べられている。

- [Towards Unified Representations of Knowledge Graph and Expert Rules for Machine Learning and Reasoning](https://aclanthology.org/2022.aacl-main.20)
  - Zhepei Wei, Yue Wang, Jinnan Li, Zhining Liu, Erxin Yu, Yuan Tian, Xin Wang, Yi Chang
  - **TLDR**: 知識グラフと、知識グラフの表現を統合した研究。知識グラフは、知識の表現と知識の表現を統合したグラフで、知識グラフは知識グラフと知識グラフの表現を統合したグラフで表現する。知識グラフは、知識グラフと知識グラフの表現を統合したグラフで表現する。

- [Who did what to Whom? Language models and humans respond diversely to features affecting argument hierarchy construction](https://aclanthology.org/2022.aacl-main.21)
  - Xiaonan Xu, Haoshuo Chen
  - **TLDR**: 言語モデルと人間が、文構造と文表現を統合する研究。文構造は、文表現の表現力(表現の意味)と、文表現の表現力(表現の意味)の2つに分けられている。文表現は、文表現の意味を表現する単語をベースに、文表現の意味を表現する単語をベースにしている。

- [CrowdChecked: Detecting Previously Fact-Checked Claims in Social Media](https://aclanthology.org/2022.aacl-main.22)
  - Momchil Hardalov, Anton Chernyavskiy, Ivan Koychev, Dmitry Ilvovsky, Preslav Nakov
  - **TLDR**: 自動事実チェックを行うための研究。ツリーニュースを生成する際、ツリーニュースの投稿から事実関係を推定する。ツリーニュースはツリーニュースの投稿から生成されるので、ツリーニュースの投稿から生成したツリーニュースをツリーニュースの投稿に置き換える。ツリーニュースはツリーニュースの投稿から生成されるので、ツリーニュースの投稿から生成したツリーニュースをツリーニュースの投稿に置き換える。ツリーニュースはツリーニュースの投稿から生成されるツリーニュースのツリーを生成する。

- [Hate Speech and Offensive Language Detection in Bengali](https://aclanthology.org/2022.aacl-main.23)
  - Mithun Das, Somnath Banerjee, Punyajoy Saha, Animesh Mukherjee
  - **TLDR**: ニュースに対する不信発言を検出する研究。ニュースに対する不信発言は、ニュースの投稿から特定する。ニュースは、ニュースの投稿から不信発言を検出する。ニュースは、ニュースの投稿から不信発言を検出する。不信発言は、ニュースの投稿から不信発言を検出する。

- [Learning Interpretable Latent Dialogue Actions With Less Supervision](https://aclanthology.org/2022.aacl-main.24)
  - Vojtěch Hudeček, Ondřej Dušek
  - **TLDR**: タスクを説明するモデルの提案。入力は入力と入力の双方向で、入力は入力と入力の双方向で入力する。入力は入力と入力双方向で入力する。入力は入力と入力双方向で入力する。入力は入力と入力双方向で入力する。入力は入力と入力双方向で入力する。入力は入力と入力双方向で入力する。

- [Named Entity Recognition in Twitter: A Dataset and Analysis on Short-Term Temporal Shifts](https://aclanthology.org/2022.aacl-main.25)
  - Asahi Ushio, Francesco Barbieri, Vitor Sousa, Leonardo Neves, Jose Camacho-Collados
  - **TLDR**: 自然言語処理で、Twitterで11,382のツリーサイズの文書を収集した研究。ツリーサイズの文書は、文書の分類に使用される単語を含まない。ツリーサイズの文書は、文書分類の精度が落ちる可能性がある。そのため、文書分類の精度を上げるために、文書分類のモデルを更新する手法を提案している。

- [PInKS: Preconditioned Commonsense Inference with Minimal Supervision](https://aclanthology.org/2022.aacl-main.26)
  - Ehsan Qasemi, Piyush Khanna, Qiang Ning, Muhao Chen
  - **TLDR**: 事前学習済み言語モデルで、事前学習済み言語モデルの学習方法を改善した研究。事前学習済み言語モデルは、事前学習済み言語モデルの学習率を上げるための条件を満たしている。事前学習済み言語モデルは、事前学習済み言語モデルの学習率を上げるための条件を満たしている。事前学習済み言語モデルは、事前学習済み言語モデルの学習率を上げるための条件を満たしている。

- [Cross-Lingual Open-Domain Question Answering with Answer Sentence Generation](https://aclanthology.org/2022.aacl-main.27)
  - Benjamin Muller, Luca Soldaini, Rik Koncel-Kedziorski, Eric Lind, Alessandro Moschitti
  - **TLDR**: 多言語の質問回答を、多言語のモデルで行う研究。多言語のモデルは、文書レベルの検索と文書レベルの生成を組み合わせている。文書レベルの検索は、文書の文特徴を入力に入力する形で行う。文書特徴を入力に入力する際、文書特徴を入力に入力する形で行う。文書特徴を入力に入力する際、文書特徴を入力に入力する形で行う。

- [Discourse Parsing Enhanced by Discourse Dependence Perception](https://aclanthology.org/2022.aacl-main.28)
  - Yuqing Xing, Longyin Zhang, Fang Kong, Guodong Zhou
  - **TLDR**: テキストレベルの文を、階層構造の枠組みで学習する研究。階層構造は、文の文関係(=文依存)と文関係(=文依存)の2つで、文関係の学習は、階層構造の枠組みで行う。階層構造は、文関係の依存と関係を学習する。

- [Prediction of People’s Emotional Response towards Multi-modal News](https://aclanthology.org/2022.aacl-main.29)
  - Ge Gao, Sejin Paik, Carley Reardon, Yanling Zhao, Lei Guo, Prakash Ishwar, Margrit Betke, Derry Tanti Wijaya
  - **TLDR**: ニュースに対する感情反応を予測する研究。ニュースのタイトル/ニュース画像を予測するモデルを、ニュースのタイトル/画像画像を予測するモデルと、ニュース画像を予測するモデルの2つに分け、各モデルの予測結果を比較し、各モデルがどのニュース画像を予測するのかを検証している。

- [AugCSE: Contrastive Sentence Embedding with Diverse Augmentations](https://aclanthology.org/2022.aacl-main.30)
  - Zilu Tang, Muhammed Yusuf Kocyigit, Derry Tanti Wijaya
  - **TLDR**: 文分散表現の強化手法の提案。文分散表現の強化は、文分散表現の表現を変えたり、文分散表現の表現を変えたりする手法を組み合わせている。文分散表現の強化は、文分散表現の表現を変えたり、文分散表現の表現を変えたりする手法を組み合わせている。

- [Seamlessly Integrating Factual Information and Social Content with Persuasive Dialogue](https://aclanthology.org/2022.aacl-main.31)
  - Maximillian Chen, Weiyan Shi, Feifan Yan, Ryan Hou, Jingwen Zhang, Saurav Sahay, Zhou Yu
  - **TLDR**: 対話システムの提案。対話システムは、ユーザーのフィードバックを基にモデルを構築する。フィードバックは、ユーザーの行動や行動の変化を反映する。フィードバックは、ユーザーの行動や行動の変化を反映する。フィードバックは、ユーザーの行動や行動の変化を反映する。

- [Dual-Encoder Transformers with Cross-modal Alignment for Multimodal Aspect-based Sentiment Analysis](https://aclanthology.org/2022.aacl-main.32)
  - Zhewen Yu, Jin Wang, Liang-Chih Yu, Xuejie Zhang
  - **TLDR**: 画像から、テキストと画像を結合する研究。画像とテキストを結合するTransformerを、画像/テキスト双方向に変換するTransformerと、テキスト/画像双方向に変換するTransformerの2つに分割し、TransformerとTransformer双方向に変換するTransformerを併用する。Transformerは、画像/テキスト双方向のEncoderを入力とするEncoder/Decoderの2つに分割する。

- [AVAST: Attentive Variational State Tracker in a Reinforced Navigator](https://aclanthology.org/2022.aacl-main.33)
  - Je-Wei Jang, Mahdin Rohmatillah, Jen-Tzung Chien
  - **TLDR**: 強化学習で、強化学習の学習を行わずに強化学習のモデルを構築する研究。強化学習のモデルは、強化学習のモデルと同等のモデルを構築するが、モデルの学習はモデルの学習率を上げるため、モデルの学習率を上げるために強化学習のモデルを学習させる。

- [Phylogeny-Inspired Adaptation of Multilingual Models to New Languages](https://aclanthology.org/2022.aacl-main.34)
  - Fahim Faisal, Antonios Anastasopoulos
  - **TLDR**: マルチ言語の学習を行う際に、自然言語の情報を活用する研究。自然言語の単語を入力に置き換える形で、単語の意味を予測するモデルを構築する。単語の意味予測は、単語の意味を予測するモデルの学習時に行う。単語の意味予測は、単語の意味を予測するモデルの学習時に行う。

- [Transferring Knowledge via Neighborhood-Aware Optimal Transport for Low-Resource Hate Speech Detection](https://aclanthology.org/2022.aacl-main.35)
  - Tulika Bose, Irina Illina, Dominique Fohr
  - **TLDR**: 自然言語処理で、学習済みのネットワークから転移学習を行う研究。転移学習は、転移先のネットワークの分布を予測するネットワークの分布と、転移先のネットワークの分布を予測するネットワークの分布を組み合わせる形で行う。転移学習は、転移先のネットワークの分布を予測するネットワークの分布と、転移先のネットワークの分布を予測するネットワークの分布を組み合わせる形で行う。

- [Bag-of-Vectors Autoencoders for Unsupervised Conditional Text Generation](https://aclanthology.org/2022.aacl-main.36)
  - Florian Mai, James Henderson
  - **TLDR**: 事前学習済みモデルを、事前学習済みモデルに組み込む研究。事前学習済みモデルは、事前学習済みモデルの入力と入力の入力を結合する形で学習する。入力と入力の入力を結合する際、入力と入力の入力を結合する形で学習する。入力と入力の入力を結合する際、入力と入力の入力を結合する形で学習する。

- [RecInDial: A Unified Framework for Conversational Recommendation with Pretrained Language Models](https://aclanthology.org/2022.aacl-main.37)
  - Lingzhi Wang, Huang Hu, Lei Sha, Can Xu, Daxin Jiang, Kam-Fai Wong
  - **TLDR**: 対話システムで、モデルの学習時にモデルの学習済み言語モデルとモデルの学習済み言語モデルを組み合わせて、モデルの学習済み言語モデルを学習する研究。学習済み言語モデルは、モデルの学習済み言語モデルと同等の学習済み言語モデルを組み合わせて学習する。学習済み言語モデルは、学習済み言語モデルの学習済み言語モデルと同等の学習済み言語モデルを組み合わせて学習する。

- [SummVD : An efficient approach for unsupervised topic-based text summarization](https://aclanthology.org/2022.aacl-main.38)
  - Gabriel Shenouda, Aurélien Bossard, Oussama Ayoub, Christophe Rodrigues
  - **TLDR**: 文書から抽出文を生成する手法の提案。文書の単語分散表現を、文書全体の単語分散表現に置き換える形で生成する。文書全体の単語分散表現を、文書全体の単語分散表現に置き換える形で生成する。文書全体の単語分散表現を、文書全体の単語分散表現に置き換える形で生成する。

- [Director: Generator-Classifiers For Supervised Language Modeling](https://aclanthology.org/2022.aacl-main.39)
  - Kushal Arora, Kurt Shuster, Sainbayar Sukhbaatar, Jason Weston
  - **TLDR**: 自然言語処理で生成したモデルを、分類器で分類する研究。分類器は通常の言語モデルと同等のモデルで、分類器は分類器の生成結果をベースに生成を行う。分類器は生成結果をベースに、分類器は生成結果をベースに生成を行う。生成結果は、生成結果の予測結果と同等のモデルで生成を行う。

- [VLStereoSet: A Study of Stereotypical Bias in Pre-trained Vision-Language Models](https://aclanthology.org/2022.aacl-main.40)
  - Kankan Zhou, Eason Lai, Jing Jiang
  - **TLDR**: 画像を学習するモデルで、画像に対するバイアスを計測する研究。バイアスは画像の表現に近いものから、画像の表現に近いものから、画像の表現に近いものから計測する。バイアスは画像の表現に近いものから計測する。バイアスは画像の表現に近いものから計測する。

- [Dynamic Context Extraction for Citation Classification](https://aclanthology.org/2022.aacl-main.41)
  - Suchetha Nambanoor Kunnath, David Pride, Petr Knoth
  - **TLDR**: 文書表現と文書構造を組み合わせた文書分類モデルの提案。文書表現は文書構造をベースに、文書構造はTransformerベースの文書表現と、文書構造は文書構造をベースにしている。文書構造は文書構造と文書構造の2つをベースにしており、文書構造は文書構造と文書構造の2つをベースにしている。

- [Affective Retrofitted Word Embeddings](https://aclanthology.org/2022.aacl-main.42)
  - Sapan Shah, Sreedhar Reddy, Pushpak Bhattacharyya
  - **TLDR**: 感情を表すベクトルを、事前学習済みベクトルに置き換える研究。既存のベクトルは、感情を表すベクトルの位置を推定する関数を学習する。この関数を、感情を表すベクトルの位置を推定する関数として学習する。

- [Is Encoder-Decoder Redundant for Neural Machine Translation?](https://aclanthology.org/2022.aacl-main.43)
  - Yingbo Gao, Christian Herold, Zijian Yang, Hermann Ney
  - **TLDR**: 機械翻訳において、Encoder Decoderを導入する研究。Transformerベースのモデルで、EncoderDecoderは必要ないが、Transformerベースのモデルは必要ない。Transformerは、EncoderDecoderの重みを入力とし、入力と入力を入力とし入力を入力とし入力と入力を入力とし入力と入力を入力とし入力と入力を入力とし入力と入力を入力とし入力と入力を入力とし入力と入力を入力とし入力と入力を入力とし入力と入力を入力とし入力と入力を入力とし入力と入力を入力とし入力と入力を入力とし入力

- [SAPGraph: Structure-aware Extractive Summarization for Scientific Papers with Heterogeneous Graph](https://aclanthology.org/2022.aacl-main.44)
  - Siya Qi, Lei Li, Yiyang Li, Jin Jiang, Dingxin Hu, Yuze Li, Yingqi Zhu, Yanquan Zhou, Marina Litvak, Natalia Vanetik
  - **TLDR**: 自然言語処理で、論文の要約を自動生成する手法の提案。要約は文書の構造を認識するノード(ノードは文書の構造を認識する)、ノードは文書のフレーズを認識するノード(ノードは文書のフレーズを認識する)、ノードは文書のフレーズを認識するノード(フレーズ認識のノード)、フレーズ認識のノードは文書のフレーズを認識するノード(フレーズ認識のノード)、フレーズ認識のノードは文書のフレーズを認識するノード(フレーズ認識のノード)の3

- [Toward Implicit Reference in Dialog: A Survey of Methods and Data](https://aclanthology.org/2022.aacl-main.45)
  - Lindsey Vanderlyn, Talita Anthonio, Daniel Ortega, Michael Roth, Ngoc Thang Vu
  - **TLDR**: 自然言語処理で、自然言語処理のモデルを自動生成する研究のサーベイ。モデルは、自然言語処理のモデルと、モデルの作成者とモデルの作成者双方で作成されている。モデルは、モデルの作成者と作成者双方で作成されている。

- [A Decade of Knowledge Graphs in Natural Language Processing: A Survey](https://aclanthology.org/2022.aacl-main.46)
  - Phillip Schneider, Tim Schopf, Juraj Vladika, Mikhail Galkin, Elena Simperl, Florian Matthes
  - **TLDR**: 自然言語処理における知識グラフの研究についてまとめたサーベイ。各サーベイの概要、評価指標、評価手法、評価手法の概要、評価手法の評価手法の概要、評価手法の評価手法の概要、評価手法の評価手法の概要、評価手法の評価手法の概要がまとめられている。

- [Multimodal Generation of Radiology Reports using Knowledge-Grounded Extraction of Entities and Relations](https://aclanthology.org/2022.aacl-main.47)
  - Francesco Dalla Serra, William Clackett, Hamish MacKinnon, Chaoyang Wang, Fani Deligianni, Jeff Dalton, Alison Q. O’Neil
  - **TLDR**: 放射線診断書の生成を行う際に、画像から3つの要素を抽出する手法を提案。画像から3つの要素を抽出する(entity1、 relation, entity2)、また3つの要素を入力として、生成した画像から3つの要素を抽出する(entity1、 relation, entity2)、という流れ。

- [SBERT studies Meaning Representations: Decomposing Sentence Embeddings into Explainable Semantic Features](https://aclanthology.org/2022.aacl-main.48)
  - Juri Opitz, Anette Frank
  - **TLDR**: 文の類似度を、文の構造と関係づける手法の提案。文構造は文の意味を表す単語(単語の意味を表す単語)と、単語の意味を表す単語(単語の意味を表す単語)の2つに分けて定義する。単語の意味を表す単語は、単語の意味を表す単語と、単語の意味を表す単語の意味を表す単語の2つに分けて定義する。

- [The Lifecycle of “Facts”: A Survey of Social Bias in Knowledge Graphs](https://aclanthology.org/2022.aacl-main.49)
  - Angelie Kraft, Ricardo Usbeck
  - **TLDR**: 知識グラフの潜在表現を、潜在表現の潜在表現に組み込んだ研究。潜在表現の潜在表現は、潜在表現の潜在表現に組み込んだ表現をベースにしている。潜在表現の潜在表現を、潜在表現の潜在表現に組み込んだ表現に置き換えた研究。潜在表現の潜在表現を、潜在表現の潜在表現に置き換えた表現に置き換えた研究。

- [Food Knowledge Representation Learning with Adversarial Substitution](https://aclanthology.org/2022.aacl-main.50)
  - Diya Li, Mohammed J Zaki
  - **TLDR**: 食の知識グラフを、事前学習済み言語モデルで学習する研究。事前学習済み言語モデルは、事前学習済みの知識グラフから、事前学習済み言語モデルの予測結果を予測する形で学習を行う。事前学習済み言語モデルは、事前学習済み言語モデルの予測結果を予測する形で学習を行う。事前学習済み言語モデルは、事前学習済み言語モデルの予測結果を予測する形で学習を行う。

- [Construction Repetition Reduces Information Rate in Dialogue](https://aclanthology.org/2022.aacl-main.51)
  - Mario Giulianelli, Arabella Sinclair, Raquel Fernandez
  - **TLDR**: 文の繰り返しは、情報量を減らすのに有効な手法として提案されている。文の繰り返しは、文の意味を理解するだけでなく、文の意味を理解する潜在表現を獲得する効果があることを示唆している。文の繰り返しは、文の意味を理解する潜在表現の獲得に有効な手法として提案されている。

- [Analogy-Guided Evolutionary Pretraining of Binary Word Embeddings](https://aclanthology.org/2022.aacl-main.52)
  - R. Alexander Knipper, Md. Mahadi Hassan, Mehdi Sadi, Shubhra Kanti Karmaker Santu
  - **TLDR**: 自然言語処理で、単語の分散表現を学習する研究。分散表現は、単語の類似度(類似度=単語の分散表現の分布)を学習する。分散表現は、単語の類似度(分布)を学習する際は、分布の分布を学習する。分布は、分布分布の分布を学習する際は、分布分布の分布を学習する。

- [Contrastive Video-Language Learning with Fine-grained Frame Sampling](https://aclanthology.org/2022.aacl-main.53)
  - Zixu Wang, Yujie Zhong, Yishu Miao, Lin Ma, Lucia Specia
  - **TLDR**: 動画とテキストのペアを学習する手法の提案。画像はテキストと同等のサイズで、テキストは画像サイズの2つを区別する。画像は画像サイズが大きいほど、テキストは画像サイズの2つを区別する。画像は画像サイズの2つを区別する。

- [Enhancing Tabular Reasoning with Pattern Exploiting Training](https://aclanthology.org/2022.aacl-main.54)
  - Abhilash Shankarampeta, Vivek Gupta, Shuo Zhang
  - **TLDR**: 事前学習済み言語モデルを、事前学習済みモデルの知識と判断力に近づける研究。事前学習済みモデルは、事前学習済みモデルの知識と判断力に近づけるための学習を行なっている。事前学習済みモデルは、事前学習済みモデルの知識と判断力に近づけるための学習を行なっている。

- [Re-contextualizing Fairness in NLP: The Case of India](https://aclanthology.org/2022.aacl-main.55)
  - Shaily Bhatt, Sunipa Dev, Partha Talukdar, Shachi Dave, Vinodkumar Prabhakaran
  - **TLDR**: 自然言語処理における公平性を評価する研究のサーベイ。公平性を評価する指標として、性別、国籍、宗教、性別適合といった3つの観点を挙げ、また、性別適合による差別についても検証している。

- [Low-Resource Multilingual and Zero-Shot Multispeaker TTS](https://aclanthology.org/2022.aacl-main.56)
  - Florian Lux, Julia Koch, Ngoc Thang Vu
  - **TLDR**: テキスト翻訳において、事前学習済み言語モデルを学習する手法の提案。事前学習済み言語モデルは、事前学習済み言語モデルの潜在表現を学習する。潜在表現は、通常のテキスト生成と同様に、事前学習済み言語モデルの潜在表現を学習する。事前学習済み言語モデルは、事前学習済み言語モデルの潜在表現を学習する。

- [Unsupervised Domain Adaptation for Sparse Retrieval by Filling Vocabulary and Word Frequency Gaps](https://aclanthology.org/2022.aacl-main.57)
  - Hiroki Iida, Naoaki Okazaki
  - **TLDR**: 言語モデルを学習する際、ドメイン変更による単語分布の変動を防ぎつつ、学習データの分散を防ぎつつ学習を行う手法の提案。学習データは、ドメイン変更による単語分布の変動を防ぎつつ、分散を防ぎつつ学習を行う。学習データは、ドメイン変更による単語分布の変動を防ぎつつ、分散を防ぎつつ学習を行う。

- [KESA: A Knowledge Enhanced Approach To Sentiment Analysis](https://aclanthology.org/2022.aacl-main.58)
  - Qinghua Zhao, Shuai Ma, Shuo Ren
  - **TLDR**: 事前学習済み言語モデルの事前学習済み単語を、事前学習済み言語モデルに組み込む手法の提案。事前学習済み言語モデルの事前学習済み単語を、事前学習済み言語モデルの事前学習済み単語に組み込む。事前学習済み言語モデルの事前学習済み単語を、事前学習済み言語モデルの事前学習済み単語に組み込む。事前学習済み言語モデルの事前学習済み単語を、事前学習済み言語モデルの事前学習済み単語に組み込む。

- [Cross-lingual Few-Shot Learning on Unseen Languages](https://aclanthology.org/2022.aacl-main.59)
  - Genta Winata, Shijie Wu, Mayank Kulkarni, Thamar Solorio, Daniel Preotiuc-Pietro
  - **TLDR**: 事前学習済み言語モデルで、マルチタスク学習を行う際に、どの言語が良いかを検証した研究。事前学習済み言語モデルは、学習済み言語の単語を予測するモデル(Encoder-Only)と、学習済み言語の単語を予測するモデル(Transformer)の2つを検証。Transformerは、学習済み言語の単語を予測するモデルで、Transformerは、学習済み言語の単語を予測するモデルで学習する。

- [Domain-aware Self-supervised Pre-training for Label-Efficient Meme Analysis](https://aclanthology.org/2022.aacl-main.60)
  - Shivam Sharma, Mohd Khizir Siddiqui, Md. Shad Akhtar, Tanmoy Chakraborty
  - **TLDR**: マルチモーダルでの学習を、事前学習済みモデルで行う手法の提案。事前学習済みモデルは、事前学習済みモデルの学習率が低い場合、学習率が高いモデルに学習させる。事前学習済みモデルは、事前学習済みモデルの学習率が低い場合、学習率が高いモデルに学習させる。事前学習済みモデルは、事前学習済みモデルの学習率が低い場合学習させる。

- [A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning](https://aclanthology.org/2022.aacl-main.61)
  - Hugo Berg, Siobhan Hall, Yash Bhalgat, Hannah Kirk, Aleksandar Shtedritski, Max Bain
  - **TLDR**: 言語モデルで、性別や偏見といった多様な表現を扱う際に、事前学習済みモデルと同等の手法を用い、性別や偏見を検出する研究。事前学習済みモデルは、性別や偏見を検出する際、性別の表現を入力とするテキストを生成する。これにより、性別の表現を検出する精度を向上させることができる。

- [Some Languages are More Equal than Others: Probing Deeper into the Linguistic Disparity in the NLP World](https://aclanthology.org/2022.aacl-main.62)
  - Surangika Ranathunga, Nisansa de Silva
  - **TLDR**: 自然言語処理における言語分類の不均衡について、言語分類の不均衡が原因なのかを検証した研究。言語分類は、言語の人口分布、言語分布の分布、言語分布分布の分布をベースにしている。また、言語分布の分布は、言語分布の分布と同等であるとしている。

- [Neural Readability Pairwise Ranking for Sentences in Italian Administrative Language](https://aclanthology.org/2022.aacl-main.63)
  - Martina Miliani, Serena Auriemma, Fernando Alva-Manchego, Alessandro Lenci
  - **TLDR**: 文構造の読解性能を評価する研究。文構造は、文の長さ、文の複雑性、文の長さ、文の長さ、文長の3つに分類される。文長は、文長の長さ(=文長の長さ)、文長の長さ(=長さ)、文長の長さ(=長さ)、文長の長さ(=長さ)、文長の長さ(=長さ)、文長の長さ(=長さ)、文長の長さ(=長さ)、文長の長さ(=長さ)、文長の長さ(=長

- [Delivering Fairness in Human Resources AI: Mutual Information to the Rescue](https://aclanthology.org/2022.aacl-main.64)
  - Leo Hemamou, William Coleman
  - **TLDR**: 自然言語処理で、プライバシーを考慮した表現を扱う手法の提案。プライバシーを考慮した表現を入力に入力する際、入力箇所を小さくする。入力箇所を大きくすると、プライバシーを考慮した表現が入力に入力される可能性がある。

- [Not another Negation Benchmark: The NaN-NLI Test Suite for Sub-clausal Negation](https://aclanthology.org/2022.aacl-main.65)
  - Thinh Hung Truong, Yulia Otmakhova, Timothy Baldwin, Trevor Cohn, Jey Han Lau, Karin Verspoor
  - **TLDR**: 自然言語処理のモデルで、自然言語処理の表現を学習する際の課題をまとめたサーベイ。単語の表現を、単語の表現を単語の表現に置き換える形で学習する。単語の表現は、単語の表現を単語の表現に置き換える形で学習する。単語の表現は、単語の表現を単語の表現に置き換える形で学習する。

- [HaRiM^+: Evaluating Summary Quality with Hallucination Risk](https://aclanthology.org/2022.aacl-main.66)
  - Seonil (Simon) Son, Junsoo Park, Jeong-in Hwang, Junghwa Lee, Hyungjong Noh, Yeonsoo Lee
  - **TLDR**: 事前学習済みモデルを、事前学習済みモデルの予測精度に近づける手法の提案。事前学習済みモデルは、事前学習済みモデルの予測精度と同等の精度を達成できるが、事前学習済みモデルは予測精度が低い。事前学習済みモデルは、事前学習済みモデルの予測精度と同等の精度を達成できるが、事前学習済みモデルは予測精度が低い。

- [The lack of theory is painful: Modeling Harshness in Peer Review Comments](https://aclanthology.org/2022.aacl-main.67)
  - Rajeev Verma, Rajarshi Roychoudhury, Tirthankar Ghosal
  - **TLDR**: 論文に対する評価を、事前学習済みモデルで行う研究。事前学習済みモデルは、評価の質を上げるために、評価の質を下げた文章を削除する。この文書を、事前学習済みモデルの文書に置き換えた文書に置き換えた文書に置き換える。文書の文書分類は、文書分類の分類器で行う。

- [Dual Mechanism Priming Effects in Hindi Word Order](https://aclanthology.org/2022.aacl-main.68)
  - Sidharth Ranjan, Marten van Schijndel, Sumeet Agarwal, Rajakrishnan Rajkumar
  - **TLDR**: 文生成時に、文の順序を決定する因子を調べた研究。文生成時に、文の順序は文の意味を表す単語の順で決定する。文の順序は、文の意味を表す単語の順で決定する。文の順序は、文の意味を表す単語の順で決定する。文の順序は、文の意味を表す単語の順で決定する。

- [Unsupervised Single Document Abstractive Summarization using Semantic Units](https://aclanthology.org/2022.aacl-main.69)
  - Jhen-Yi Wu, Ying-Jia Lin, Hung-Yu Kao
  - **TLDR**: 要約の要約を、文単位ではなく文単位で行う研究。文単位の要約は、文の文長が長くなるほど要約精度が高くなる。要約は文長が長くなるほど要約精度が高くなる。要約は文長が長くなるほど要約精度が高くなる。要約は文長が長くなるほど要約精度が高くなる。

- [Detecting Incongruent News Articles Using Multi-head Attention Dual Summarization](https://aclanthology.org/2022.aacl-main.70)
  - Sujit Kumar, Gaurav Kumar, Sanasam Ranbir Singh
  - **TLDR**: ニュース記事の類似度を調べる研究。ニュース記事のタイトルとタイトルを同じく、タイトルとタイトルは同じく、タイトルとタイトルは同じく、タイトルとタイトルは同じく、タイトルとタイトルは同じく、タイトルとタイトルは同じく、タイトルとタイトルは同じく、タイトルとタイトルは同じく、タイトルとタイトルは同じく、という2つのタスクを検証。

- [Meta-Learning based Deferred Optimisation for Sentiment and Emotion aware Multi-modal Dialogue Act Classification](https://aclanthology.org/2022.aacl-main.71)
  - Tulika Saha, Aditya Prakash Patra, Sriparna Saha, Pushpak Bhattacharyya
  - **TLDR**: 対話システムにおける、感情の表現を分類する手法の提案。感情の表現は、通常の文脈からではなく、文中の単語から抽出される。単語の意味を抽出する際、単語の意味を単語の意味に置き換える(単語の意味は単語の意味に置き換える)ことで、文脈の意味を抽出する。

- [Enhancing Financial Table and Text Question Answering with Tabular Graph and Numerical Reasoning](https://aclanthology.org/2022.aacl-main.72)
  - Rungsiman Nararatwong, Natthawut Kertkeidkachorn, Ryutaro Ichise
  - **TLDR**: 自然言語処理で、質問回答を行う際に、文書構造をグラフで表現する研究。文書構造は、文書の文書構造と同等かそれ以上の文書構造を表現する。文書構造は、文書の文書構造と同等かそれ以上の文書構造を表現する。文書構造は、文書構造と同等かそれ以上の文書構造を表現する。文書構造は、文書構造と同等かそれ以上の文書構造を表現する。

- [Fine-grained Contrastive Learning for Definition Generation](https://aclanthology.org/2022.aacl-main.73)
  - Hengyuan Zhang, Dawei Li, Shiping Yang, Yanran Li
  - **TLDR**: 事前学習済みモデルで、文の意味を抽出する手法の提案。文の意味を抽出する際、文の意味を予測するモデルを用い、文の意味を予測するモデルを用い文の意味を予測する。文の意味を予測するモデルは、文の意味を予測するモデルと同等の精度を維持できる。

- [Hengam: An Adversarially Trained Transformer for Persian Temporal Tagging](https://aclanthology.org/2022.aacl-main.74)
  - Sajad Mirzababaei, Amir Hossein Kargaran, Hinrich Schütze, Ehsaneddin Asgari
  - **TLDR**: 事前学習済みモデルを、事前学習済み言語モデルに適用する研究。事前学習済みモデルは、事前学習済み言語モデルのルールをベースに、事前学習済み言語モデルのルールをベースに学習する。事前学習済みモデルは、事前学習済み言語モデルのルールをベースに、事前学習済み言語モデルのルールをベースに学習する。事前学習済み言語モデルは、事前学習済み言語モデルのルールをベースに学習する。

- [What’s Different between Visual Question Answering for Machine “Understanding” Versus for Accessibility?](https://aclanthology.org/2022.aacl-main.75)
  - Yang Trista Cao, Kyle Seelman, Kyungjun Lee, Hal Daumé III
  - **TLDR**: 画像を扱うタスクで、画像を扱うモデルのベンチマークを検証した研究。ベンチマークは、画像を扱うモデルのベンチマークと、画像を扱うモデルのベンチマークのベンチマークが異なることを確認。ベンチマークは、画像を扱うモデルのベンチマークと、画像を扱うモデルのベンチマークのベンチマークが異なることを確認。

- [Persona or Context? Towards Building Context adaptive Personalized Persuasive Virtual Sales Assistant](https://aclanthology.org/2022.aacl-main.76)
  - Abhisek Tiwari, Sriparna Saha, Shubhashis Sengupta, Anutosh Maitra, Roshni Ramnani, Pushpak Bhattacharyya
  - **TLDR**: 対話システムで、ユーザーのゴールを達成するために、対話システムのゴールコントロールとゴールの提案者を統合した研究。提案者は、対話システムのゴールを達成するために、提案者と提案者双方の提案を統合し、提案者と提案者双方の提案を統合する。提案者は、提案者と提案者双方の提案を統合し、提案者と提案者双方の提案を統合する。

- [Legal Case Document Summarization: Extractive and Abstractive Methods and their Evaluation](https://aclanthology.org/2022.aacl-main.77)
  - Abhay Shukla, Paheli Bhattacharya, Soham Poddar, Rajdeep Mukherjee, Kripabandhu Ghosh, Pawan Goyal, Saptarshi Ghosh
  - **TLDR**: 裁判所判決文の要約を抽出する手法について、抽出型と要約型の2種類を検証した研究。抽出型は文書の長さに依存するが、要約型は文書の長さに依存しない。要約型は文書長が長いが要約は短いため、要約型は要約型より長い文書を要約する傾向がある。

- [FPC: Fine-tuning with Prompt Curriculum for Relation Extraction](https://aclanthology.org/2022.aacl-main.78)
  - Sicheng Yang, Dandan Song
  - **TLDR**: 事前学習済み言語モデルを、事前学習済みモデルに転移する手法の提案。事前学習済みモデルは、事前学習済みモデルの分類タスクを学習する。事前学習済みモデルは、事前学習済みモデルの分類タスクを学習する。事前学習済みモデルは、事前学習済みモデルの分類タスクを学習する。事前学習済みモデルは、事前学習済みモデルの分類タスクを学習する。

- [Dead or Murdered? Predicting Responsibility Perception in Femicide News Reports](https://aclanthology.org/2022.aacl-main.79)
  - Gosse Minnema, Sara Gemelli, Chiara Zanchi, Tommaso Caselli, Malvina Nissim
  - **TLDR**: 性別適合手術のニュースで、モデルがモデルの予測に影響を与えているか検証した研究。モデルは、モデルの予測結果と異なる箇所を予測する(=モデルの予測結果と異なる箇所は別々に予測する)ことで、モデルの予測結果と異なる箇所を予測する。モデルは、モデルの予測結果と異なる箇所を予測する(=モデルの予測結果と異なる箇所は別々に予測する)ことで、モデルの予測結果と異なる箇所を予測する

- [PESE: Event Structure Extraction using Pointer Network based Encoder-Decoder Architecture](https://aclanthology.org/2022.aacl-main.80)
  - Alapan Kuila, Sudeshna Sarkar
  - **TLDR**: テキスト中のイベントを抽出するタスクの提案。イベントのタイトル、イベントの実行時間、イベントの実行者、イベントの実行結果、イベントの実行者とイベントの実行者の関係を抽出する。イベントの実行者とイベントの実行者の関係を抽出するモデルを提案。

- [How do we get there? Evaluating transformer neural networks as cognitive models for English past tense inflection](https://aclanthology.org/2022.aacl-main.81)
  - Xiaomeng Ma, Lingyu Gao
  - **TLDR**: 自然言語処理で、通常の文を学習する研究。通常の文は通常の文と同等の学習が行えるが、通常の文は通常の文と同等の学習が行えない。そのため、通常の文を学習するTransformerモデルを学習する。通常のTransformerモデルは通常の文を学習するが、通常の文は通常の文の学習が行えない。

- [Characterizing and addressing the issue of oversmoothing in neural autoregressive sequence modeling](https://aclanthology.org/2022.aacl-main.82)
  - Ilia Kulikov, Maksim Eremeev, Kyunghyun Cho
  - **TLDR**: 翻訳モデルのOversmoothingを緩和する手法の提案。Oversmoothingは、翻訳モデルの学習率を上げるために有効な手法だが、学習率を上げるためにはOversmoothingの過剰な影響を抑える必要がある。Oversmoothingは、翻訳モデルの学習率を上げるために有効な手法だが、過剰な影響は過剰な影響を抑えるための過剰な影響をもつ。

- [Identifying Weaknesses in Machine Translation Metrics Through Minimum Bayes Risk Decoding: A Case Study for COMET](https://aclanthology.org/2022.aacl-main.83)
  - Chantal Amrhein, Rico Sennrich
  - **TLDR**: 機械翻訳モデルの評価指標と人間評価との相関を検証した研究。モデルは、翻訳の品質が低い場合に、翻訳の品質が低い場合に、翻訳の品質が低い場合に、翻訳の品質が低い場合に、翻訳の品質が低い場合に、翻訳の品質が低い場合に、翻訳の品質が低い場合に、翻訳の品質が低い場合に、翻訳の品質が低い場合に、翻訳の品質が低い場合に、翻訳の品質が低い場合に、翻訳の品質が低い場合に、翻訳の品質が低い場合に、翻訳の品質が低い場合に、翻訳の品質が低い場合に、翻訳の品質が低い場合に

- [Whodunit? Learning to Contrast for Authorship Attribution](https://aclanthology.org/2022.aacl-main.84)
  - Bo Ai, Yuchen Wang, Yugin Tan, Samson Tan
  - **TLDR**: 事前学習済み言語モデルを、事前学習済みのモデルに合わせる手法の提案。事前学習済み言語モデルは、各文書の文書特徴を予測するモデルで、文書特徴を予測するモデルは、文書特徴と文書特徴双方を予測するモデルで学習する。文書特徴を予測するモデルは、文書特徴と文書特徴双方を予測するモデルを併用する。

- [Higher-Order Dependency Parsing for Arc-Polynomial Score Functions via Gradient-Based Methods and Genetic Algorithm](https://aclanthology.org/2022.aacl-main.85)
  - Xudong Zhang, Joseph Le Roux, Thierry Charnois
  - **TLDR**: 強化学習で、各クラスのスコアを更新する手法の提案。スコアは、クラスの平均値と、クラスの平均値の差分を計算する。この差分を計算するモデルは、クラスの平均値を計算するモデルと同等精度で学習する。

- [Underspecification in Scene Description-to-Depiction Tasks](https://aclanthology.org/2022.aacl-main.86)
  - Ben Hutchinson, Jason Baldridge, Vinodkumar Prabhakaran
  - **TLDR**: マルチモーダル画像+テキストの生成について、マルチモーダル画像+テキストの生成について、マルチモーダル画像+テキストの生成について、マルチモーダル画像+テキストの生成について、マルチモーダル画像+テキストの生成について、マルチモーダル画像+テキストの生成について、マルチモーダル画像+テキストの生成について、マルチモーダル画像+テキストの生成について、マルチモーダル画像+テキストの生成について、マルチモーダル画像+テキストの生成について、マルチモーダル画像+テキストの生成について、マルチモーダル画像+テキストの生成について、マルチモー

- [COFAR: Commonsense and Factual Reasoning in Image Search](https://aclanthology.org/2022.aacl-main.87)
  - Prajwal Gatti, Abhirama Subramanyam Penamakuri, Revant Teotia, Anand Mishra, Shubhashis Sengupta, Roshni Ramnani
  - **TLDR**: 自然言語処理で、画像を検索する際のタスクをまとめた研究。画像を検索する際は、画像の表現(画像の人物など)と自然言語の検索結果を組み合わせて検索を行う。画像の表現は、自然言語の検索結果と画像の検索結果双方で表現される。
