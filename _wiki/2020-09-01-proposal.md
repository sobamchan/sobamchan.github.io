

## 前年 proposal

### RQs

- UP models から欠落している commonsense knowledge を profile できるか
- 欠落している commonsense knowledge の中から，visual datasets に入っているものはないか
- visual datasets から UP models にどう inject できるか

### Methods

- CSK の taxonomies の作成，アノテーション
  - モデルが知識として持っていない情報を炙り出すことができる
- MMM, unpaired
- MMM w/ Visual UP model
- Image Caption Matching, ICM
- caption to object
- object relation learning
  - image は使わずに，(word a - relation - word b) の triplet から masked lm をやる

#### Components

- Textual/Visual UP model (eg, RoBERTa)
- Matching

### 役立ちそうな論文等

- Parameter-Efficient Transfer Learning for NLP.
  - 概要: 新しいタスクがどんどん，stream で来る．タスクごとに adapter module を差し替えるだけで，効率的に前の学習結果を忘れずに適応できる．
  - csr は他のタスクから知識を獲得する必要がある．→ タスクの形式は似ている？
    - adapter module は task specific なので，それと同時に UP model に追加で，cross-task な module を用意することで他タスクからの知識の transfer を狙う．
  - csr taxonomies を作成できれば，category ごとに adapter を用意することもできるのか．必要とされる知識が違うのだとしたら有効に働きそう．
  - adapter module を差し替えて，bert 自体は froze しているので，forgetting の問題は task 間で無い
    - これは他でも使えそう

- Common Sense or World Knowledge? Investigating Adapter-Based Knowledge Injection into Pretrained Transformers
  - 概要: CSK injection w/ conceptnet を adapter module を使って実現．efficient に forgetting を回避
    2 つの konwledge base から文を構成して，bert w/ adapters を作成．各種 csr タスクで評価した．
  - ConceptNet, OMCS には commonsense question instances を得ための知識は包含されていなかった．(解けるようにならなかった)
    - ならどこにあるの？
  - タスク / インスタンス によって，有効な adapters が異なる
    - 複数 adapters で予測して，ensemble (eg, 多数決) できそう -> AdapterFusion

- AdapterHub: A Framework for Adapting Transformers
  - hugginface の interface にのっとって，adapters を共有しやすい web app の提供．
  - proposal を書くにあたっての用語を探すには良い論文かも

- Masking as an Efficient Alternative to Finetuning for Pretrained Language Models
  - adapter module と似たことの違うアプローチ，finetune 時に pretrained model のパラメータを binary mask するように学習．
  - パラメーター量は抑えつつ，finetune ができる．
  - 必要なパラメーターは 1 で通して，関係ないものは 0 で消す．というマスクを finetuning ときに学習する

- Toward General Scene Graph: Integration of Visual Semantic Knowledge with Entity Synset Alignment
  - この論文というよりも "scene graph datasets" が便利そう
    - Visual Genome 200
    - Visually Relevant Relationship (VrR-VG)

- Temporal Common Sense Acquisition with Minimal Supervision
  - duration, frequency of events に関しての cs
  - これらに関しての mention を利用するモデルの提案

- AdapterFusion: Non-Destructive Task Composition for Transfer Learning
  - 概要: multi-task 用の adapter module を使った学習方法

- BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning
  - AdapterFusion に似たもの．追加の parallel layer を transformer block に追加

- Intermediate-Task Transfer Learning with Pretrained Models for Natural Language Understanding: When and Why Does It Work?



### New Methodology

#### stacking adapter

- 目的
  - vision knowledge injection phase (p1), final finetuning phase (p2) それぞれで，catastrophic forgetting を回避する
- 概要
  - p1, p2 と段階的に，adapter modules (AM) を追加していく．
  - p1 でいくつか追加して，UP model は固定して，p1 を visual knowledge 用に最適化
  - p2 で新しくいくつか固定して，UP model & p1 adapter は固定して，downstream task 用に最適化
- memo
  - 元 proposal はどう inject するかの framework
    これは，inject する知識をどう保持するかのモデル提案
  - 段階的に AM を追加するのは，他の intermediate learning を行うモデルに適応することができる．

#### adapters ensembling

- 目的
  - knowledge base ごとに adapters を学習してみたときに，問題インスタンスごとに有効な adapter が変わりそう
- 概要
  - この問題を吸収するために，推論時は複数 adapters で予測を立てて多数決をとる
- memo
  - これは AdapterFusion と同じか．
    - → AdapterFusion を Multimodal or CSK 向けにさらに改造する手法を考える．



### アイデア

- visual knowledge を knowledge graph 的に保持する方法はないか？
  - visual info に match する knowledge sub-graph を抽出して利用
